{"namespace": "xmnlp.utils.parallel_handler", "type": "function", "project_path": "Text-Processing/xmnlp", "completion_path": "Text-Processing/xmnlp/xmnlp/utils/__init__.py", "signature_position": [90, 92], "body_position": [101, 107], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function is a parallel handler that takes a callback function and a list of texts as input. It then processes the texts using the callback function in parallel using a thread pool executor. If the input `texts` is not a list, raise a ValueError(\"You should pass a list of texts\").", "Arguments": ":param callback: Callable. The callback function to be applied to the list of texts.\n:param texts: List[str]. The list of texts to be processed.\n:param n_jobs: int. The pool size of threads. Defaults to 2.\n:param kwargs: Any additional keyword arguments to be passed to the callback function.\n:return: Generator[List[Any], None, None]. A generator that yields the results of applying the callback function to the texts in parallel."}, "tests": ["tests/test_xmnlp.py::test_pinyin_parallel", "tests/test_xmnlp.py::test_radical_parallel"], "indent": 4}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "type": "function", "project_path": "Text-Processing/pycorrector", "completion_path": "Text-Processing/pycorrector/pycorrector/proper_corrector.py", "signature_position": [31, 31], "body_position": [37, 52], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Load a dictionary from the given file path. It reads the file line by line and creates a dictionary with the key-value pairs from the file. If the file is not found, an empty dictionary is returned.", "Arguments": ":param path: String. The file path from which the dictionary is to be loaded.\n:return: Dictionary. The loaded dictionary from the file. If the file is not found, an empty dictionary is returned."}, "tests": ["tests/ner_error_test.py::test_common_error"], "indent": 4}
{"namespace": "pycorrector.utils.tokenizer.segment", "type": "function", "project_path": "Text-Processing/pycorrector", "completion_path": "Text-Processing/pycorrector/pycorrector/utils/tokenizer.py", "signature_position": [101, 101], "body_position": [109, 128], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function segments the input sentence into words or characters based on the given cut type. It also provides the option to enable POS tagging.", "Arguments": ":param sentence: String. The input sentence to be segmented.\n:param cut_type: String. The type of segmentation to be used. It defaults to 'word' if not specified.\n:param pos: Bool. Whether to enable POS tagging. It defaults to False if not specified.\n:return: List. The segmented words or characters along with their POS tags if enabled."}, "tests": ["tests/ner_error_test.py::test_ner"], "indent": 4}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/sum_basic.py", "signature_position": [49, 49], "body_position": [50, 53], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function computes the frequency of each word in the given list of words and returns a dictionary containing the word frequencies.", "Arguments": ":param list_of_words: List of strings. The list of words for which the frequency needs to be computed.\n:return: Dictionary. A dictionary containing the frequency of each word in the input list."}, "tests": ["tests/test_summarizers/test_sum_basic.py::test_compute_word_freq"], "indent": 8}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/sum_basic.py", "signature_position": [72, 72], "body_position": [73, 79], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function computes the average probability of words in a document based on the frequency of words in the document and the content words in a sentence.", "Arguments": ":param word_freq_in_doc: Dictionary. The frequency of words in the document.\n:param content_words_in_sentence: List. The content words in a sentence.\n:return: Float. The average probability of words in the document. If the content words count is 0, it returns 0."}, "tests": ["tests/test_summarizers/test_sum_basic.py::test_compute_average_probability_of_words"], "indent": 8}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/lex_rank.py", "signature_position": [78, 78], "body_position": [79, 88], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function computes the inverse document frequency (IDF) for each term in the sentences.", "Arguments": ":param sentences: List of strings. The sentences to compute IDF.\n:return: Dictionary. The IDF metrics for each term in the sentences."}, "tests": ["tests/test_summarizers/test_lex_rank.py::test_idf_metrics"], "indent": 8}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/lex_rank.py", "signature_position": [119, 119], "body_position": [142, 156], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Compute the cosine similarity between two sentences based on the TF*IDF metrics. It calculates the cosine similarity of two sentences represented as vectors A and B, computed as cos(x, y) = A . B / (|A| . |B|).", "Arguments": ":param sentence1: Iterable object. Every item represents a word of the 1st sentence.\n:param sentence2: Iterable object. Every item represents a word of the 2nd sentence.\n:param tf1: Dict. Term frequencies of words from the 1st sentence.\n:param tf2: Dict. Term frequencies of words from the 2nd sentence.\n:param idf_metrics: Dict. Inverted document metrics of the sentences. Every sentence is treated as a document for this algorithm.\n:return: Float. Returns -1.0 for opposite similarity, 1.0 for the same sentence, and zero for no similarity between sentences."}, "tests": ["tests/test_summarizers/test_lex_rank.py::test_cosine_similarity_for_the_same_sentence_with_duplicate_words_should_be_one", "tests/test_summarizers/test_lex_rank.py::test_cosine_similarity_sentences_with_no_common_word_should_be_zero"], "indent": 8}
{"namespace": "sumy.evaluation.rouge._split_into_words", "type": "function", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/evaluation/rouge.py", "signature_position": [18, 18], "body_position": [19, 24], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function splits the input sentences into words and returns a list of words. If there exists a element in the input sentences that is not a Sentence instance, it raises a ValueError(\"Object in collection must be of type Sentence\").", "Arguments": ":param sentences: List of Sentence instances. The input sentences to be split into words.\n:return: List of String. The list of words obtained after splitting the sentences."}, "tests": ["tests/test_evaluation/test_evaluation_rouge.py::test_split_into_words"], "indent": 4}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "type": "function", "project_path": "Internet/djangorestframework-simplejwt", "completion_path": "Internet/djangorestframework-simplejwt/rest_framework_simplejwt/utils.py", "signature_position": [18, 18], "body_position": [19, 22], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "The function converts a naive datetime object to a UTC-aware datetime object if the USE_TZ setting is enabled.", "Arguments": ":param dt: datetime. The datetime object to be converted to UTC.\n:return: datetime. The UTC-aware datetime object."}, "tests": ["tests/test_utils.py::TestMakeUtc::test_it_should_return_the_correct_values", "tests/test_utils.py::TestDatetimeFromEpoch::test_it_should_return_the_correct_values"], "indent": 4}
{"namespace": "datasette.filters.where_filters", "type": "function", "project_path": "Database/datasette", "completion_path": "Database/datasette/datasette/filters.py", "signature_position": [10, 11], "body_position": [12, 40], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "The function is used to handle the \"_where=\" parameter in a request. If the \"_where\" parameter is present in the request parameters, it checks if the user has permission to execute SQL. If no permission is granted, it raises a 403 error. If permission is granted, it adds the values of the \"_where\" parameter to the where_clauses list and generates a separate UI element for each value, which is added to the extra_wheres_for_ui list. Finally, it passes the where_clauses and extra_wheres_for_ui as arguments to the FilterArguments class and returns an inner function.", "Arguments": ":param request: The request object.\n:param database: The database object.\n:param datasette: The datasette object.\n:return: A nested function that processes the \"_where\" query parameter and returns the filter arguments."}, "tests": ["tests/test_filters.py::test_where_filters_from_request"], "indent": 4}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "type": "function", "project_path": "Database/mongo-doc-manager", "completion_path": "Database/mongo-doc-manager/mongo_connector/namespace_config.py", "signature_position": [546, 546], "body_position": [549, 552], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function checks if the source namespace matches the given regex. If it does, it returns the new mapped namespace by replacing the wildcard character '*' with the corresponding group from the match.", "Arguments": ":param regex: Regular expression. The regular expression pattern to match the source namespace.\n:param src_namespace: String. The source namespace to be matched.\n:param dest_namespace: String. The destination namespace to be replaced.\n:return: String. The new mapped namespace if the source namespace matches the regex, otherwise None."}, "tests": ["tests/test_namespace_config.py::TestNamespaceConfig::test_match_replace_regex"], "indent": 4}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "type": "function", "project_path": "Database/mongo-doc-manager", "completion_path": "Database/mongo-doc-manager/mongo_connector/namespace_config.py", "signature_position": [560, 560], "body_position": [562, 567], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Create a regular expression object from a wildcard namespace. It first splits the namespace into database and collection names. Then, it creates a regular expression for the database and collection names and compiles them into a single regular expression object.", "Arguments": ":param namespace: String. The wildcard namespace to be converted into a regular expression.\n:return: RegexObject. The compiled regular expression object."}, "tests": ["tests/test_namespace_config.py::TestNamespaceConfig::test_namespace_to_regex_escapes_metacharacters", "tests/test_namespace_config.py::TestNamespaceConfig::test_namespace_to_regex"], "indent": 4}
{"namespace": "mongo_connector.util.long_to_bson_ts", "type": "function", "project_path": "Database/mongo-doc-manager", "completion_path": "Database/mongo-doc-manager/mongo_connector/util.py", "signature_position": [60, 60], "body_position": [63, 66], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Converts an integer into a BSON timestamp. The resulting BSON timestamp is a combination of the time (in seconds) and an incrementor, both extracted from the 64-bit integer.", "Arguments": ":param val: Integer. The value to be converted into a BSON timestamp.\n:return: Timestamp. The BSON timestamp created from the input value."}, "tests": ["tests/test_util.py::TestUtil::test_bson_ts_to_long"], "indent": 4}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "type": "method", "project_path": "Database/mongo-doc-manager", "completion_path": "Database/mongo-doc-manager/mongo_connector/doc_managers/formatters.py", "signature_position": [150, 150], "body_position": [151, 170], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function flattens the given document and returns a dictionary with the flattened keys and values. It uses a recursive approach to flatten the document. For example, given a dictionary {\"a\": 2, \"b\": {\"c\": {\"d\": 5}}, \"e\": [6, 7, 8]}, it would output {\"a\": 2, \"b.c.d\": 5, \"e.0\": 6, \"e.1\": 7, \"e.2\": 8}.", "Arguments": ":param self: DocumentFlattener. An instance of the DocumentFlattener class.\n:param document: Dictionary. The document to be flattened.\n:return: Dictionary. The flattened document."}, "tests": ["tests/test_formatters.py::TestFormatters::test_flattener"], "indent": 8}
{"namespace": "hypertools.tools.df2mat.df2mat", "type": "function", "project_path": "Multimedia/hypertools", "completion_path": "Multimedia/hypertools/hypertools/tools/df2mat.py", "signature_position": [6, 6], "body_position": [32, 45], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function transforms a single-level Pandas DataFrame into a Numpy array with binarized text columns. It uses the Pandas.DataFrame.get_dummies function to transform text columns into binary vectors.", "Arguments": ":param data: Pandas DataFrame. The DataFrame that needs to be converted. It only works with single-level (not Multi-level indices).\n:param return_labels: Bool. Whether to return a list of column labels for the numpy array. Defaults to False.\n:return: Numpy array. A Numpy array where text columns are turned into binary vectors.\n:labels: List. A list of column labels for the numpy array. Only returned if return_labels is set to True."}, "tests": ["tests/test_pandas_to_matrix.py::test_df2mat"], "indent": 4}
{"namespace": "pyinfra.operations.server.shell", "type": "function", "project_path": "System/pyinfra", "completion_path": "System/pyinfra/pyinfra/operations/server.py", "signature_position": [133, 133], "body_position": [152, 156], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "It converts str to list by using [str] and yields each str in the list.", "Arguments": ":param commands: str or list. Command or list of commands to execute on the remote server.\n:return: Iterator[str]. It yields each command in the list."}, "tests": ["tests/test_api/test_api_operations.py::TestNestedOperationsApi::test_nested_op_api", "tests/test_api/test_api_operations.py::TestOperationOrdering::test_cli_op_line_numbers", "tests/test_api/test_api_operations.py::TestOperationsApi::test_run_once_serial_op", "tests/test_api/test_api_operations.py::TestOperationFailures::test_full_op_fail", "tests/test_api/test_api_operations.py::TestOperationFailures::test_ignore_errors_op_fail"], "indent": 4}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "type": "function", "project_path": "Security/msticpy", "completion_path": "Security/msticpy/msticpy/analysis/anomalous_sequence/utils/laplace_smooth.py", "signature_position": [12, 18], "body_position": [45, 56], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Apply laplace smoothing to the input counts for the cmds. It adds 1 to each of the counts, including the unk_token, to handle unseen commands. It's used for smoothing individual command counts (seq1_counts) and sequence command counts of length 2 (seq2_counts).", "Arguments": ":param seq1_counts: DefaultDict[str, int]. Individual command counts.\n:param seq2_counts: DefaultDict[str, DefaultDict[str, int]]. Sequence command (length 2) counts.\n:param start_token: str. Dummy command to signify the start of a session (e.g. \"##START##\").\n:param end_token: str. Dummy command to signify the end of a session (e.g. \"##END##\").\n:param unk_token: str. Dummy command to signify an unseen command (e.g. \"##UNK##\").\n:return: Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]. Laplace smoothed counts: individual command counts, sequence command (length 2) counts."}, "tests": ["tests/analysis/test_anom_seq_laplace_smooth.py::TestLaplaceSmooth::test_laplace_smooth_cmd_counts"], "indent": 4}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "type": "function", "project_path": "Security/msticpy", "completion_path": "Security/msticpy/msticpy/analysis/anomalous_sequence/utils/laplace_smooth.py", "signature_position": [59, 64], "body_position": [89, 99], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function applies Laplace smoothing to the input counts for the parameters. It adds 1 to each of the counts, including the unk_token, to handle unseen parameters. it's used for smoothing individual parameter counts (param_counts) and parameter conditional on command counts (cmd_param_counts).", "Arguments": ":param cmds: List[str]. List of all the possible commands (including the unk_token).\n:param param_counts: DefaultDict[str, int]. Individual parameter counts.\n:param cmd_param_counts: DefaultDict[str, DefaultDict[str, int]]. Parameter conditional on command counts.\n:param unk_token: str. Dummy command to signify an unseen command (e.g. \"##UNK##\").\n:return: Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]. Individual parameter probabilities, parameter conditional on command probabilities."}, "tests": ["tests/analysis/test_anom_seq_laplace_smooth.py::TestLaplaceSmooth::test_laplace_smooth_param_counts"], "indent": 4}
{"namespace": "ydata_profiling.compare_reports._compare_title", "type": "function", "project_path": "Software-Development/ydata-profiling", "completion_path": "Software-Development/ydata-profiling/src/ydata_profiling/compare_reports.py", "signature_position": [112, 112], "body_position": [113, 117], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Compare the titles in the list and return the result. If all titles are the same, it returns the title. Otherwise, it returns a string that compares the titles.", "Arguments": ":param titles: List of strings. The list of titles to be compared.\n:return: String. The result of the comparison."}, "tests": ["tests/unit/test_comparison.py::test_title"], "indent": 4}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "type": "function", "project_path": "Scientific-Engineering/TPOT", "completion_path": "Scientific-Engineering/TPOT/tpot/gp_deap.py", "signature_position": [154, 154], "body_position": [171, 174], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function initializes the stats dictionary for an individual. The statistics initialized are: 'generation', 'mutation_count', 'crossover_count', and 'predecessor'.", "Arguments": ":param individual: deap individual. The individual for which the stats dictionary is to be initialized.\n:return: No return values."}, "tests": ["tests/stats_test.py::test_dict_initialization"], "indent": 4}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "type": "function", "project_path": "Scientific-Engineering/bentoml", "completion_path": "Scientific-Engineering/bentoml/src/bentoml_cli/env_manager.py", "signature_position": [34, 34], "body_position": [38, 50], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function removes `--env <env_name>` or `--env=<env_name>` from the input list of command line arguments.", "Arguments": ":param cmd_args: List of string. The list of command line arguments.\n:return: List of string. The updated list of command line arguments after removing `--env <env_name>` or `--env=<env_name>`."}, "tests": ["tests/unit/bentoml_cli/test_env_manager.py::test_remove_env_arg"], "indent": 4}
{"namespace": "pymc.math.cartesian", "type": "function", "project_path": "Scientific-Engineering/pymc", "completion_path": "Scientific-Engineering/pymc/pymc/math.py", "signature_position": [187, 187], "body_position": [195, 202], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function makes the Cartesian product of arrays. It takes N-D arrays as input and returns the Cartesian product of the arrays.", "Arguments": ":param arrays: N-D array-like. N-D arrays where earlier arrays loop more slowly than later ones.\n:return: N-D array-like. The Cartesian product of the input arrays."}, "tests": ["tests/test_math.py::test_cartesian_2d", "tests/gp/test_cov.py::TestCovKron::test_multiops", "tests/test_math.py::test_cartesian", "tests/gp/test_cov.py::TestCovKron::test_symprod_cov"], "indent": 4}
{"namespace": "pymc.testing.select_by_precision", "type": "function", "project_path": "Scientific-Engineering/pymc", "completion_path": "Scientific-Engineering/pymc/pymc/testing.py", "signature_position": [225, 225], "body_position": [227, 228], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function is a helper function to choose reasonable decimal cutoffs for different floatX modes. It selects the decimal cutoff based on the floatX mode.", "Arguments": ":param float64: The decimal cutoff for float64 mode.\n:param float32: The decimal cutoff for float32 mode.\n:return: The decimal cutoff based on the floatX mode."}, "tests": ["tests/distributions/test_multivariate.py::TestMatchesScipy::test_mvnormal", "tests/distributions/test_multivariate.py::TestMatchesScipy::test_dirichlet_multinomial_matches_beta_binomial", "tests/distributions/test_multivariate.py::TestMatchesScipy::test_stickbreakingweights_logp", "tests/distributions/test_multivariate.py::TestMatchesScipy::test_lkjcorr", "tests/distributions/test_multivariate.py::TestMatchesScipy::test_matrixnormal"], "indent": 4}
{"namespace": "pymc.pytensorf.floatX", "type": "function", "project_path": "Scientific-Engineering/pymc", "completion_path": "Scientific-Engineering/pymc/pymc/pytensorf.py", "signature_position": [436, 436], "body_position": [440, 444], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function converts a PyTensor tensor or numpy array to pytensor.config.floatX type.", "Arguments": ":param X: PyTensor tensor or numpy array. The input tensor or array to be converted.\n:return: PyTensor tensor or numpy array. The converted tensor or array."}, "tests": ["tests/test_math.py::test_expand_packed_triangular", "tests/test_data.py::TestData::test_symbolic_coords", "tests/distributions/test_mixture.py::TestMixture::test_list_mvnormals_logp", "tests/distributions/test_mixture.py::TestMixture::test_list_poissons_sampling", "tests/distributions/test_mixture.py::TestMixture::test_single_poisson_sampling"], "indent": 4}
{"namespace": "pymc.logprob.scan.construct_scan", "type": "function", "project_path": "Scientific-Engineering/pymc", "completion_path": "Scientific-Engineering/pymc/pymc/logprob/scan.py", "signature_position": [295, 295], "body_position": [296, 299], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "It constructs a scan operation based on the given scan arguments and additional keyword arguments. It creates a scan operation and a node based on the input and output variables and returns the node outputs and updates.", "Arguments": ":param scan_args: ScanArgs. An instance of the ScanArgs class containing inner and outer inputs, outputs, and other information.\n:param kwargs: Additional keyword arguments.\n:return: Tuple. A tuple containing a list of TensorVariable and OrderedUpdates."}, "tests": ["tests/logprob/test_scan.py::test_convert_outer_out_to_in_sit_sot", "tests/logprob/test_scan.py::test_convert_outer_out_to_in_mit_sot"], "indent": 4}
{"namespace": "sacred.commands.help_for_command", "type": "function", "project_path": "Utilities/sacred", "completion_path": "Utilities/sacred/sacred/commands.py", "signature_position": [118, 118], "body_position": [120, 122], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function retrieves the help text, including the signature and docstring, for a given command (function). It then removes any backspaces from the help text before returning it.", "Arguments": ":param command: The command (function) for which to retrieve the help text.\n:return: String. The help text for the given command."}, "tests": ["tests/test_commands.py::test_help_for_command"], "indent": 4}
{"namespace": "cupy.random._generator.reset_states", "type": "function", "project_path": "Scientific-Engineering/cupy", "completion_path": "Scientific-Engineering/cupy/cupy/random/_generator.py", "signature_position": [1264, 1264], "body_position": [1265, 1266], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Reset the global variable _random_states to an empty dictionary.", "Arguments": ":param: No input parameters.\n:return: No return values."}, "tests": ["tests/cupy_tests/random_tests/test_generator.py::TestResetStates::test_reset_states"], "indent": 4}
{"namespace": "cupy.random._generator._check_and_get_dtype", "type": "function", "project_path": "Scientific-Engineering/cupy", "completion_path": "Scientific-Engineering/cupy/cupy/random/_generator.py", "signature_position": [1305, 1305], "body_position": [1306, 1309], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function checks if the input data type is supported by cupy.random and returns the data type if it is supported. If it is not supported by cupy.random, raise an error.", "Arguments": ":param dtype: data type. The input data type to be checked.\n:return: numpy.dtype. The checked and converted data type."}, "tests": ["tests/cupy_tests/random_tests/test_generator.py::TestCheckAndGetDtype::test_float16", "tests/cupy_tests/random_tests/test_generator.py::TestCheckAndGetDtype::test_int_type", "tests/cupy_tests/random_tests/test_generator.py::TestCheckAndGetDtype::test_float32_64_type"], "indent": 4}
{"namespace": "cupy_builder._command.filter_files_by_extension", "type": "function", "project_path": "Scientific-Engineering/cupy", "completion_path": "Scientific-Engineering/cupy/install/cupy_builder/_command.py", "signature_position": [17, 20], "body_position": [21, 28], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Filter the files in the given list based on the file extension. It separates the files with the given extension from the rest of the files.", "Arguments": ":param sources: List of strings. The list of file paths to be filtered.\n:param extension: String. The file extension to be used for filtering.\n:return: Two lists of strings. The first list contains the files with the given extension, and the second list contains the rest of the files."}, "tests": ["tests/install_tests/test_cupy_builder/test_command.py::test_filter_files_by_extension"], "indent": 4}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "type": "function", "project_path": "Scientific-Engineering/datasets", "completion_path": "Scientific-Engineering/datasets/src/datasets/table.py", "signature_position": [35, 35], "body_position": [36, 39], "dependency": {"intra_class": [], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function reads an Arrow file from the given filename and returns an in-memory Arrow table.", "Arguments": ":param filename: String. The name of the file to read the Arrow table from.\n:return: pa.Table. The in-memory Arrow table read from the file."}, "tests": ["tests/test_table.py::test_in_memory_arrow_table_from_file"], "indent": 4}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "type": "method", "project_path": "Database/mongoengine", "completion_path": "Database/mongoengine/mongoengine/base/datastructures.py", "signature_position": [56, 57], "body_position": [58, 61], "dependency": {"intra_class": ["mongoengine.base.datastructures.BaseDict.__getitem__"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function retrieves the value associated with the given key in the BaseDict instance. If the key is not found, it returns the default value instead. It overrides the default behavior.", "Arguments": ":param self: BaseDict. An instance of the BaseDict class.\n:param key: The key to retrieve the value for.\n:param default: The value to return if the key is not found. Defaults to None.\n:return: The value associated with the key, or the default value if the key is not found."}, "tests": ["tests/test_datastructures.py::TestBaseDict::test_get_default", "tests/test_datastructures.py::TestBaseDict::test_get_returns_the_same_as___getitem__", "tests/test_datastructures.py::TestBaseDict::test_get_sublist_gets_converted_to_BaseList_just_like__getitem__"], "indent": 8}
{"namespace": "sacred.experiment.Experiment.main", "type": "method", "project_path": "Utilities/sacred", "completion_path": "Utilities/sacred/sacred/experiment.py", "signature_position": [140, 140], "body_position": [149, 151], "dependency": {"intra_class": ["sacred.experiment.Experiment.default_command"], "intra_file": [], "cross_file": ["sacred.ingredient.Ingredient.command"]}, "requirement": {"Functionality": "This function is a decorator that is used to define the main function of an experiment. The main function is the default command that is executed when no command is specified or when calling the run() method. It captures the decorated function and sets it as the default command for the experiment.", "Arguments": ":param self: Experiment. An instance of the Experiment class.\n:param function: The function to be decorated and set as the main function.\n:return: The captured function that is set as the default command."}, "tests": ["tests/test_experiment.py::test_additional_cli_options_flag", "tests/test_modules.py::test_experiment_run_subingredient_function", "tests/test_modules.py::test_experiment_named_config_subingredient_overwrite", "tests/test_modules.py::test_double_nested_config", "tests/test_ingredients.py::test_config_docs_are_preserved"], "indent": 8}
{"namespace": "datasette.utils.asgi.Request.fake", "type": "method", "project_path": "Database/datasette", "completion_path": "Database/datasette/datasette/utils/asgi.py", "signature_position": [128, 128], "body_position": [130, 142], "dependency": {"intra_class": ["datasette.utils.asgi.Request.__init__"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function is a class method that creates a fake Request object for testing purposes. It takes in parameters such as the path with query string, method, scheme, and url variables, and constructs a Request object with the given values.", "Arguments": ":param cls: Class. The class itself.\n:param path_with_query_string: String. The path with query string for the Request object.\n:param method: String. The HTTP method for the Request object. Defaults to \"GET\" if not specified.\n:param scheme: String. The scheme for the Request object. Defaults to \"http\" if not specified.\n:param url_vars: Dictionary. The URL variables for the Request object. Defaults to None if not specified.\n:return: Request. The created Request object."}, "tests": ["tests/test_facets.py::test_column_facet_suggest_skip_if_enabled_by_metadata", "tests/test_filters.py::test_through_filters_from_request", "tests/test_facets.py::test_array_facet_results", "tests/test_facets.py::test_date_facet_results", "tests/test_utils.py::test_path_with_removed_args"], "indent": 8}
{"namespace": "sacred.utils.format_sacred_error", "type": "function", "project_path": "Utilities/sacred", "completion_path": "Utilities/sacred/sacred/utils.py", "signature_position": [592, 592], "body_position": [593, 600], "dependency": {"intra_class": [], "intra_file": ["sacred.utils.format_filtered_stacktrace", "sacred.utils.SacredError.print_usage", "sacred.utils.SacredError.print_traceback", "sacred.utils.SacredError.filter_traceback"], "cross_file": []}, "requirement": {"Functionality": "This function formats a SacredError object into a string representation. It creates a list of lines to be included in the final formatted error message. The lines include the short usage message and the filtered stacktrace (if specified) or the exception type and message (if not specified).", "Arguments": ":param e: SacredError. The SacredError object to be formatted.\n:param short_usage: String. The short usage message to be included in the formatted error message.\n:return: String. The formatted error message."}, "tests": ["tests/test_exceptions.py::test_format_sacred_error", "tests/test_exceptions.py::test_chained_error"], "indent": 4}
{"namespace": "datasette.facets.ColumnFacet.suggest", "type": "method", "project_path": "Database/datasette", "completion_path": "Database/datasette/datasette/facets.py", "signature_position": [159, 159], "body_position": [160, 208], "dependency": {"intra_class": [], "intra_file": ["datasette.facets.Facet.database", "datasette.facets.Facet.ds", "datasette.facets.Facet.get_columns", "datasette.facets.Facet.get_configs", "datasette.facets.Facet.get_facet_size", "datasette.facets.Facet.get_row_count", "datasette.facets.Facet.params", "datasette.facets.Facet.request", "datasette.facets.Facet.sql"], "cross_file": ["datasette.database.QueryInterrupted", "datasette.utils.escape_sqlite", "datasette.utils.path_with_added_args", "datasette.app.Datasette.absolute_url", "datasette.app.Datasette.execute", "datasette.app.Datasette.setting", "datasette.app.Datasette.urls", "datasette.url_builder.Urls.path"]}, "requirement": {"Functionality": "This function suggests facets for a given column in a dataset. It retrieves the row count and columns from the dataset, determines the facet size, and then iterates through each column. For each column, it constructs a SQL query to retrieve distinct values and their counts. If the number of distinct values is between 1 and the row count, and the number of distinct values is less than or equal to the facet size, and at least one distinct value has a count greater than 1, it adds the column as a suggested facet. Finally, it returns a list of suggested facets.", "Arguments": ":param self: ColumnFacet. An instance of the ColumnFacet class.\n:return: List of dictionaries. A list of dictionaries representing the suggested facets. Each dictionary contains the name of the column and a toggle URL for enabling the facet."}, "tests": ["tests/test_facets.py::test_column_facet_suggest", "tests/test_facets.py::test_column_facet_suggest_skip_if_already_selected", "tests/test_facets.py::test_column_facet_suggest_skip_if_enabled_by_metadata"], "indent": 8}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/text_rank.py", "signature_position": [83, 83], "body_position": [84, 85], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["sumy.summarizers._summarizer.AbstractSummarizer.normalize_word", "sumy.summarizers._summarizer.AbstractSummarizer.stem_word", "sumy.models.dom._sentence.Sentence.words"]}, "requirement": {"Functionality": "This function takes a sentence as input and converts it into a set of words. It normalizes each word in the sentence and removes any stop words. The function then returns the set of stemmed words.", "Arguments": ":param self: TextRankSummarizer. An instance of the TextRankSummarizer class.\n:param sentence: Sentence. The sentence to be converted into a set of words.\n:return: List. The set of stemmed words in the sentence after removing stop words."}, "tests": ["tests/test_summarizers/test_text_rank.py::test_stop_words_correctly_removed"], "indent": 8}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "type": "method", "project_path": "Security/barf", "completion_path": "Security/barf/barf/arch/translator.py", "signature_position": [104, 104], "body_position": [107, 114], "dependency": {"intra_class": ["barf.arch.translator.InstructionTranslator._log_translation_exception", "barf.arch.translator.InstructionTranslator._translate"], "intra_file": ["barf.arch.translator.TranslationError"], "cross_file": []}, "requirement": {"Functionality": "This function translates an instruction into REIL representation. If an exception occurs during the translation process, it logs the exception and raises a translation error with the message \"Unknown error\".", "Arguments": ":param self: InstructionTranslator. An instance of the InstructionTranslator class.\n:param instruction: The instruction to be translated.\n:return: The REIL representation of the instruction."}, "tests": ["tests/core/reil/emulator/test_emulator.py::ReilEmulatorTests::test_zero_division_error_1", "tests/core/reil/emulator/test_tainter.py::ReilEmulatorTaintTests::test_store_mem_2", "tests/core/reil/emulator/test_tainter.py::ReilEmulatorTaintTests::test_store_mem_1", "tests/core/reil/emulator/test_emulator.py::ReilEmulatorTests::test_add", "tests/core/reil/emulator/test_emulator.py::ReilEmulatorTests::test_mov"], "indent": 8}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/edmundson.py", "signature_position": [119, 119], "body_position": [120, 121], "dependency": {"intra_class": ["sumy.summarizers.edmundson.EdmundsonSummarizer._build_location_method_instance"], "intra_file": [], "cross_file": ["sumy.summarizers.edmundson_location.EdmundsonLocationMethod.__call__"]}, "requirement": {"Functionality": "This function applies the location-based method for text summarization. It creates an instance of the location-based method and uses it to summarize the given document based on the specified parameters.", "Arguments": ":param self: EdmundsonSummarizer. An instance of the EdmundsonSummarizer class.\n:param document: Document. The document to be summarized.\n:param sentences_count: Integer. The number of sentences to include in the summary.\n:param w_h: Integer. The weight for the frequency term in a sentence. Defaults to 1.\n:param w_p1: Integer. The weight for the first paragraph. Defaults to 1.\n:param w_p2: Integer. The weight for the last paragraph. Defaults to 1.\n:param w_s1: Integer. The weight for the first sentence. Defaults to 1.\n:param w_s2: Integer. The weight for the last sentence. Defaults to 1.\n:return: Tuple. The summary of the document using the location-based method."}, "tests": ["tests/test_summarizers/test_edmundson.py::test_location_method_1", "tests/test_summarizers/test_edmundson.py::test_location_method_without_null_words", "tests/test_summarizers/test_edmundson.py::test_location_method_with_empty_document", "tests/test_summarizers/test_edmundson.py::test_location_method_2"], "indent": 8}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "type": "method", "project_path": "Security/barf", "completion_path": "Security/barf/barf/arch/arm/parser.py", "signature_position": [394, 394], "body_position": [398, 416], "dependency": {"intra_class": ["barf.arch.arm.parser.ArmParser._cache"], "intra_file": ["barf.arch.arm.parser.instruction", "barf.arch.arm.parser.logger"], "cross_file": []}, "requirement": {"Functionality": "This function parses an ARM instruction. It takes an input instruction, converts it to lowercase, and checks if it is present in the cache. If not, it parses the instruction and stores it in the cache. It then returns a deep copy of the parsed instruction. If any exception occurs during parsing, it logs an error message and returns None.", "Arguments": ":param self: ArmParser. An instance of the ArmParser class.\n:param instr: String. The ARM instruction to be parsed.\n:return: The parsed ARM instruction, or None if parsing fails."}, "tests": ["tests/arch/arm/test_armparser.py::ArmParser32BitsTests::test_data_processing_instructions", "tests/arch/arm/test_armparser.py::ArmParser32BitsTests::test_load_store_instructions"], "indent": 8}
{"namespace": "sacred.ingredient.Ingredient.named_config", "type": "method", "project_path": "Utilities/sacred", "completion_path": "Utilities/sacred/sacred/ingredient.py", "signature_position": [165, 165], "body_position": [171, 173], "dependency": {"intra_class": ["sacred.ingredient.Ingredient._add_named_config"], "intra_file": [], "cross_file": ["sacred.config.config_scope.ConfigScope", "sacred.config.config_scope.ConfigScope.__init__"]}, "requirement": {"Functionality": "This function is a decorator that turns a function into a named configuration. It creates a ConfigScope instance based on the input function and adds it to the named configurations of the Ingredient instance.", "Arguments": ":param self: Ingredient. An instance of the Ingredient class.\n:param func: Function. The function to be turned into a named configuration.\n:return: ConfigScope. The created ConfigScope object."}, "tests": ["tests/test_ingredients.py::test_gather_named_configs", "tests/test_experiment.py::test_named_config_and_ingredient"], "indent": 8}
{"namespace": "sumy.evaluation.rouge._union_lcs", "type": "function", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/evaluation/rouge.py", "signature_position": [220, 220], "body_position": [237, 251], "dependency": {"intra_class": [], "intra_file": ["sumy.evaluation.rouge._recon_lcs", "sumy.evaluation.rouge._split_into_words"], "cross_file": []}, "requirement": {"Functionality": "This function calculates LCS_u(r_i, C), which is the LCS score of the union longest common subsequence between a reference sentence and a candidate summary. For example, if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1 is \u201cw1 w2\u201d and the longest common subsequence of r_i and c2 is \u201cw1 w3 w5\u201d. The union longest common subsequence of r_i, c1, and c2 is \u201cw1 w2 w3 w5\u201d, and the conbined lcs is \"w1 w2 w1 w3 w5\". So LCS_u(r_i, C) = 4/5.", "Arguments": ":param evaluated_sentences: List of Sentence. The sentences that have been picked by the summarizer.\n:param reference_sentence: Sentence. One of the sentences in the reference summaries.\n:return: float. The LCS_u(r_i, C) score."}, "tests": ["tests/test_evaluation/test_evaluation_rouge.py::test_union_lcs"], "indent": 4}
{"namespace": "datasette.app.Datasette.add_database", "type": "method", "project_path": "Database/datasette", "completion_path": "Database/datasette/datasette/app.py", "signature_position": [415, 415], "body_position": [416, 432], "dependency": {"intra_class": ["datasette.app.Datasette.databases"], "intra_file": [], "cross_file": ["datasette.database.Database.suggest_name"]}, "requirement": {"Functionality": "This function adds a new database to the Datasette instance. It first creates a copy of the existing databases, then assigns a unique name to the new database if no name is provided. If a name is provided, it checks if the name already exists and appends a number to make it unique. It then assigns the name and route to the new database, adds it to the copied databases dictionary, and assigns the copied dictionary back to the instance.", "Arguments": ":param self: Datasette. An instance of the Datasette class.\n:param db: The database to be added.\n:param name: String [optional]. The name to be assigned to the new database. If not provided, a unique name will be generated.\n:param route: String [optional]. The route to be assigned to the new database. If not provided, the name will be used as the route.\n:return: The added database."}, "tests": ["tests/test_facets.py::test_facet_size", "tests/test_facets.py::test_json_array_with_blanks_and_nulls", "tests/test_facets.py::test_array_facet_handle_duplicate_tags", "tests/test_internals_datasette.py::test_num_sql_threads_zero"], "indent": 8}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "type": "function", "project_path": "Software-Development/pandas-profiling", "completion_path": "Software-Development/pandas-profiling/src/ydata_profiling/utils/dataframe.py", "signature_position": [144, 144], "body_position": [158, 181], "dependency": {"intra_class": [], "intra_file": ["ydata_profiling.utils.dataframe.expand_mixed"], "cross_file": []}, "requirement": {"Functionality": "This function expands non-nested lists, dicts, and tuples in a DataFrame into separate columns with a prefix. It iterates over each column in the DataFrame and checks if the values in the column are of the specified types. If they are, it expands the values into separate columns with a prefix based on the original column name.", "Arguments": ":param df: pd.DataFrame. The DataFrame to be expanded.\n:param types: Any. A list of types to expand. It defaults to [list, dict, tuple] if not specified.\n:return: pd.DataFrame. The DataFrame with the expanded columns."}, "tests": ["tests/unit/test_utils.py::test_expand"], "indent": 4}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "type": "method", "project_path": "Security/barf", "completion_path": "Security/barf/barf/arch/x86/parser.py", "signature_position": [291, 291], "body_position": [294, 312], "dependency": {"intra_class": ["barf.arch.x86.parser.X86Parser._cache"], "intra_file": ["barf.arch.x86.parser.instruction", "barf.arch.x86.parser.logger"], "cross_file": []}, "requirement": {"Functionality": "This function parses an x86 instruction. It takes an input instruction, converts it to lowercase, and checks if it is present in the cache. If not, it parses the instruction and stores it in the cache. It then returns a deep copy of the parsed instruction. If any exception occurs during parsing, it returns None and logs an error message.", "Arguments": ":param self: X86Parser. An instance of the X86Parser class.\n:param instr: String. The x86 instruction to be parsed.\n:return: The parsed x86 instruction as an object, or None if parsing fails."}, "tests": ["tests/arch/x86/test_x86parser.py::X86Parser32BitsTests::test_misc_2", "tests/core/reil/emulator/test_emulator.py::ReilEmulatorTests::test_loop", "tests/core/reil/emulator/test_emulator.py::ReilEmulatorTests::test_zero_division_error_1", "tests/core/reil/emulator/test_tainter.py::ReilEmulatorTaintTests::test_store_mem_2", "tests/core/reil/emulator/test_emulator.py::ReilEmulatorTests::test_add"], "indent": 8}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "type": "method", "project_path": "Security/barf", "completion_path": "Security/barf/barf/analysis/gadgets/finder.py", "signature_position": [74, 74], "body_position": [77, 88], "dependency": {"intra_class": ["barf.analysis.gadgets.finder.GadgetFinder._architecture", "barf.analysis.gadgets.finder.GadgetFinder._find_arm_candidates", "barf.analysis.gadgets.finder.GadgetFinder._find_x86_candidates", "barf.analysis.gadgets.finder.GadgetFinder._instrs_depth", "barf.analysis.gadgets.finder.GadgetFinder._max_bytes"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function finds gadgets based on the given start and end addresses. It sets the maximum number of bytes and the depth of instructions to be considered. Then, it calls the appropriate method based on the architecture to find the candidates. Finally, it sorts the candidates based on their addresses and returns the sorted list.", "Arguments": ":param self: GadgetFinder. An instance of the GadgetFinder class.\n:param start_address: The starting address to search for gadgets.\n:param end_address: The ending address to search for gadgets.\n:param byte_depth: Integer. The maximum number of bytes to consider for each gadget. It defaults to 20 if not specified.\n:param instrs_depth: Integer. The depth of instructions to consider for each gadget. It defaults to 2 if not specified.\n:return: List of gadgets. The list of gadgets found, sorted by their addresses."}, "tests": ["tests/analysis/gadgets/test_gadget_x86.py::GadgetClassifierTests::test_arithmetic_load_add_1", "tests/analysis/gadgets/test_gadget_x86.py::GadgetClassifierTests::test_move_register_3", "tests/analysis/gadgets/test_gadget_x86.py::GadgetClassifierTests::test_move_register_4", "tests/analysis/gadgets/test_gadget_x86.py::GadgetClassifierTests::test_load_constant_2", "tests/analysis/gadgets/test_gadget_x86.py::GadgetClassifierTests::test_arithmetic_load_add_3"], "indent": 8}
{"namespace": "lux.action.default.register_default_actions", "type": "function", "project_path": "Scientific-Engineering/lux", "completion_path": "Scientific-Engineering/lux/lux/action/default.py", "signature_position": [1, 1], "body_position": [2, 29], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["lux.action.correlation.correlation", "lux.action.custom.custom", "lux.action.enhance.enhance", "lux.action.filter.add_filter", "lux.action.generalize.generalize", "lux.action.temporal.temporal", "lux.action.univariate.univariate", "lux._config.config", "lux._config.config.Config.register_action", "lux"]}, "requirement": {"Functionality": "This function registers default actions for the Lux library. It imports various action modules and defines display conditions for each action. Then, it globally registers each action with its corresponding display condition.", "Arguments": ":param: No input parameters.\n:return: No return values."}, "tests": ["tests/test_config.py::test_remove_default_actions"], "indent": 4}
{"namespace": "boltons.tbutils.print_exception", "type": "function", "project_path": "Utilities/boltons", "completion_path": "Utilities/boltons/boltons/tbutils.py", "signature_position": [651, 651], "body_position": [663, 670], "dependency": {"intra_class": [], "intra_file": ["boltons.tbutils.TracebackInfo", "boltons.tbutils.TracebackInfo.__str__", "boltons.tbutils.TracebackInfo.from_traceback", "boltons.tbutils.format_exception_only"], "cross_file": []}, "requirement": {"Functionality": "This function prints the exception information, including the stack trace and the type and value of the exception. It also handles special cases for SyntaxError, where it prints the line where the syntax error occurred with a caret indicating the approximate position of the error.", "Arguments": ":param etype: The type of the exception.\n:param value: The value of the exception.\n:param tb: The traceback object.\n:param limit: Optional. The maximum number of stack trace entries to print. Defaults to None.\n:param file: Optional. The file object to which the output is written. Defaults to sys.stderr.\n:return: No return values."}, "tests": ["tests/test_tbutils.py::test_exception_info"], "indent": 4}
{"namespace": "gunicorn.http.body.Body.read", "type": "method", "project_path": "Utilities/gunicorn", "completion_path": "Utilities/gunicorn/gunicorn/http/body.py", "signature_position": [202, 202], "body_position": [203, 224], "dependency": {"intra_class": ["gunicorn.http.body.Body.buf", "gunicorn.http.body.Body.getsize", "gunicorn.http.body.Body.reader", "gunicorn.http.body.LengthReader.read"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Read a specified number of bytes from the Body instance. First, the function get the size to read. If the size is 0, it returns an empty byte string b\"\" since there is nothing to read. If the size is less than the current position of the buffer, it means that the requested size has been reached or exceeded. In this case, it retrieves the data from the buffer, splits it into two parts - 'ret' and 'rest', and updates the buffer by writing the remaining data into a new BytesIO object. It then returns the 'ret' part. If the size is greater than the current position of the buffer, it means that the requested data is not currently available in the buffer. In this case, it reads data from the reader object in blocks of 1024 bytes and writes it to the buffer until either all the data has been read or the requested size has been reached. Finally, it retrieves the data from the buffer, splits it into two parts - 'ret' and 'rest', updates the buffer by writing the remaining data into a new BytesIO object, and returns the 'ret' part.", "Arguments": ":param self: Body. An instance of the Body class.\n:param size: Integer. The number of bytes to read from the Body instance. Defaults to None.\n:return: Bytes. The read data from the Body instance."}, "tests": ["tests/test_http.py::test_readline_buffer_loaded_with_size", "tests/test_http.py::test_readline_buffer_loaded"], "indent": 8}
{"namespace": "pyramid.config.Configurator.scan", "type": "method", "project_path": "Internet/pyramid", "completion_path": "Internet/pyramid/src/pyramid/config/__init__.py", "signature_position": [799, 806], "body_position": [868, 879], "dependency": {"intra_class": ["pyramid.config.Configurator.maybe_dotted", "pyramid.config.Configurator.venusian"], "intra_file": [], "cross_file": ["pyramid.path.caller_package"]}, "requirement": {"Functionality": "This function scans a Python package and its subpackages for objects marked with configuration decoration. It uses the Venusian library to perform the scanning and executes the corresponding decorator callbacks. The decorated objects found during the scan will influence the current configuration state.", "Arguments": ":param self: Configurator. An instance of the Configurator class.\n:param package: Optional. The Python package or module object to scan. If None, the package of the caller is used.\n:param categories: Tuple of strings. The Venusian 'scan categories' to use during scanning. Defaults to ('pyramid').\n:param onerror: Optional. Venusian 'onerror' callback function to influence error behavior during scanning.\n:param ignore: Optional. Venusian 'ignore' value to ignore specific modules, packages, or global objects during scanning.\n:param **kw: Additional keyword arguments to pass to the Venusian Scanner object's constructor.\n:return: No return values."}, "tests": ["tests/test_config/test_init.py::ConfiguratorTests::test_scan_integration_with_extra_kw", "tests/test_integration.py::WGSIAppPlusViewConfigTests::test_scanned", "tests/test_config/test_init.py::ConfiguratorTests::test_scan_integration", "tests/test_config/test_init.py::ConfiguratorTests::test_scan_integration_dottedname_package", "tests/test_config/test_init.py::ConfiguratorTests::test_scan_integration_with_onerror"], "indent": 8}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "type": "function", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/evaluation/rouge.py", "signature_position": [27, 27], "body_position": [28, 35], "dependency": {"intra_class": [], "intra_file": ["sumy.evaluation.rouge._get_ngrams", "sumy.evaluation.rouge._split_into_words"], "cross_file": []}, "requirement": {"Functionality": "This function takes in a positive value n and a non empty list of sentences as input. It iterates over each sentence in the sentences list and get the n-grams for each sentence. The resulting n-grams are added to a set. Finally, the function returns the set.", "Arguments": ":param n: Integer. The value of n for the n-grams.\n:param sentences: List of Sentence. The list of sentences to generate n-grams from.\n:return: Set of strings. The set of unique n-grams generated from the sentences."}, "tests": ["tests/test_evaluation/test_evaluation_rouge.py::test_get_word_ngrams", "tests/test_evaluation/test_evaluation_rouge.py::test_ngrams_for_more_sentences_should_not_return_words_at_boundaries"], "indent": 4}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "type": "method", "project_path": "Utilities/sacred", "completion_path": "Utilities/sacred/sacred/ingredient.py", "signature_position": [316, 318], "body_position": [328, 332], "dependency": {"intra_class": ["sacred.ingredient.Ingredient.post_process_name", "sacred.ingredient.Ingredient.traverse_ingredients", "sacred.ingredient.Ingredient.named_configs"], "intra_file": [], "cross_file": ["sacred.utils.join_paths"]}, "requirement": {"Functionality": "This function gathers all named configurations from the Ingredient instance and its sub-ingredients. It iterates through each ingredient and its named configurations to collect the configuration names and corresponding configurations.", "Arguments": ":param self: Ingredient. An instance of the Ingredient class.\n:return: Generator. A generator that yields tuples containing the full name of the named configuration and the corresponding configuration."}, "tests": ["tests/test_ingredients.py::test_gather_named_configs"], "indent": 8}
{"namespace": "sumy._compat.to_unicode", "type": "function", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/_compat.py", "signature_position": [60, 60], "body_position": [61, 67], "dependency": {"intra_class": [], "intra_file": ["sumy._compat.bytes", "sumy._compat.instance_to_unicode", "sumy._compat.unicode"], "cross_file": []}, "requirement": {"Functionality": "This function converts the input object to a Unicode string. It first checks if the object is already a Unicode string, and if so, returns it as is. If the object is a byte string, it decodes it using the \"utf-8\" encoding and returns the resulting Unicode string. If the object is neither a Unicode string nor a byte string, it calls a custom function to decode it to a Unicode string.", "Arguments": ":param object: Object. The object to be converted to a Unicode string.\n:return: Unicode string. The converted Unicode string."}, "tests": ["tests/test_summarizers/test_edmundson.py::test_title_method_1", "tests/test_summarizers/test_edmundson.py::test_location_method_with_empty_document", "tests/test_utils/test_compat.py::test_str_object_to_unicode", "tests/test_summarizers/test_edmundson.py::test_key_1", "tests/test_html_parser.py::test_annotated_text"], "indent": 4}
{"namespace": "ydata_profiling.controller.console.main", "type": "function", "project_path": "Software-Development/pandas-profiling", "completion_path": "Software-Development/pandas-profiling/src/ydata_profiling/controller/console.py", "signature_position": [99, 99], "body_position": [107, 126], "dependency": {"intra_class": [], "intra_file": ["ydata_profiling.controller.console.parse_args"], "cross_file": ["ydata_profiling.utils.dataframe.read_pandas", "ydata_profiling.profile_report.ProfileReport", "ydata_profiling.profile_report.ProfileReport.to_file"]}, "requirement": {"Functionality": "This function is the main entry point for running another corresponding package. It takes in arguments, parses them, and generates a profiling report based on the input data.", "Arguments": ":param args: Optional list of any type. Arguments for the program. Defaults to None.\n:return: None."}, "tests": ["tests/unit/test_console.py::test_console_explorative", "tests/unit/test_console.py::test_console_single_core", "tests/unit/test_console.py::test_double_config", "tests/issues/test_issue388.py::test_issue388", "tests/unit/test_console.py::test_console_minimal"], "indent": 4}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "type": "method", "project_path": "Security/barf", "completion_path": "Security/barf/barf/analysis/gadgets/classifier.py", "signature_position": [104, 104], "body_position": [107, 121], "dependency": {"intra_class": ["barf.analysis.gadgets.classifier.GadgetClassifier._classifiers", "barf.analysis.gadgets.classifier.GadgetClassifier._classify", "barf.analysis.gadgets.classifier.GadgetClassifier._emu_iters"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function classifies gadgets based on their types. It iterates through the classifiers and tries to classify the given gadget using each classifier. If an error occurs during classification, it prints the error message and traceback. Finally, it sorts the classified gadgets and returns them.", "Arguments": ":param self: GadgetClassifier. An instance of the GadgetClassifier class.\n:param gadget: The gadget to be classified.\n:return: List of classified gadgets, sorted by their string representation."}, "tests": ["tests/analysis/gadgets/test_gadget_x86.py::GadgetClassifierTests::test_arithmetic_load_add_1", "tests/analysis/gadgets/test_gadget_x86.py::GadgetClassifierTests::test_move_register_3", "tests/analysis/gadgets/test_gadget_x86.py::GadgetClassifierTests::test_move_register_4", "tests/analysis/gadgets/test_gadget_x86.py::GadgetClassifierTests::test_load_constant_2", "tests/analysis/gadgets/test_gadget_x86.py::GadgetClassifierTests::test_arithmetic_load_add_3"], "indent": 8}
{"namespace": "hl7.client.MLLPClient.send", "type": "method", "project_path": "Communications/hl7", "completion_path": "Communications/hl7/hl7/client.py", "signature_position": [82, 82], "body_position": [87, 89], "dependency": {"intra_class": ["hl7.client.MLLPClient.socket"], "intra_file": ["hl7.client.RECV_BUFFER"], "cross_file": []}, "requirement": {"Functionality": "This function sends data to the server using a low-level, direct access to the socket. It first sends the data to the server and then waits for the server to return a response.", "Arguments": ":param self: MLLPClient. An instance of the MLLPClient class.\n:param data: The data to be sent to the server. It should already be wrapped in an MLLP container.\n:return: The response received from the server."}, "tests": ["tests/test_client.py::MLLPClientTest::test_send", "tests/test_client.py::MLLPClientTest::test_context_manager"], "indent": 8}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "type": "method", "project_path": "Security/barf", "completion_path": "Security/barf/barf/core/reil/parser.py", "signature_position": [197, 197], "body_position": [200, 220], "dependency": {"intra_class": ["barf.core.reil.parser.ReilParser._cache"], "intra_file": ["barf.core.reil.parser.instruction", "barf.core.reil.parser.logger"], "cross_file": []}, "requirement": {"Functionality": "This function parses a list of IR instructions. It converts each instruction to lowercase and checks if it is already present in the cache. If not, it parses the instruction and adds it to the cache. It then retrieves the parsed instruction from the cache, clones it, and adds it to the list of parsed instructions. If an error occurs during parsing, an error message is logged.", "Arguments": ":param self: ReilParser. An instance of the ReilParser class.\n:param instrs: List of strings. The list of IR instructions to be parsed.\n:return: List of parsed instructions in REIL format."}, "tests": ["tests/core/reil/emulator/test_emulator.py::ReilEmulatorTests::test_zero_division_error_2", "tests/core/reil/emulator/test_cpu.py::ReilCpuTests::test_mod", "tests/core/reil/emulator/test_cpu.py::ReilCpuTests::test_stm", "tests/core/reil/emulator/test_cpu.py::ReilCpuTests::test_ldm", "tests/core/reil/emulator/test_cpu.py::ReilCpuTests::test_bisz"], "indent": 8}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/lsa.py", "signature_position": [55, 55], "body_position": [57, 60], "dependency": {"intra_class": ["sumy.summarizers.lsa.LsaSummarizer._stop_words"], "intra_file": [], "cross_file": ["sumy.summarizers._summarizer.AbstractSummarizer.normalize_word", "sumy.summarizers._summarizer.AbstractSummarizer.stem_word", "sumy.models.dom._document.ObjectDocumentModel.words"]}, "requirement": {"Functionality": "This function creates a dictionary that maps each unique word in the document to its corresponding row index. It first normalizes each word in the document and removes any stop words. Then, it creates a dictionary where the keys are the unique words and the values are their respective row indices.", "Arguments": ":param self: LsaSummarizer. An instance of the LsaSummarizer class.\n:param document: Object. The document for which the dictionary is created.\n:return: dict. A dictionary mapping unique words to their row indices."}, "tests": ["tests/test_summarizers/test_lsa.py::test_dictionary_without_stop_words"], "indent": 8}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "type": "function", "project_path": "Communications/ehforwarderbot", "completion_path": "Communications/ehforwarderbot/ehforwarderbot/utils.py", "signature_position": [113, 113], "body_position": [120, 123], "dependency": {"intra_class": [], "intra_file": ["ehforwarderbot.utils.get_base_path"], "cross_file": []}, "requirement": {"Functionality": "This function returns the path to the custom channels. It first gets the base path and appends \"modules\" to it to create the channel path. If the channel path does not exist, it creates it.", "Arguments": ":param: No input parameters.\n:return: Path. The path to the custom channels."}, "tests": ["tests/test_channel_loading.py::test_custom_path_module_loading"], "indent": 4}
{"namespace": "passpie.database.PasspieStorage.read", "type": "method", "project_path": "Security/passpie", "completion_path": "Security/passpie/passpie/database.py", "signature_position": [33, 33], "body_position": [34, 43], "dependency": {"intra_class": ["passpie.database.PasspieStorage.path"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Read data from files in a directory and return the data as a dictionary. It searches for files with a specific extension in the given directory and reads the contents of each file. The contents are then parsed as YAML and added to a list. Finally, the list is converted into a dictionary with numbered keys.", "Arguments": ":param self: PasspieStorage. An instance of the PasspieStorage class.\n:return: Dictionary. A dictionary containing the read data, with numbered keys."}, "tests": ["tests/test_database.py::StorageTests::test_read_returns_all_found_credentials_in_default_dict"], "indent": 8}
{"namespace": "barf.core.smt.smtfunction.ite", "type": "function", "project_path": "Security/barf", "completion_path": "Security/barf/barf/core/smt/smtfunction.py", "signature_position": [59, 59], "body_position": [60, 63], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["barf.core.smt.smtsymbol.BitVec", "barf.core.smt.smtsymbol.Bool"]}, "requirement": {"Functionality": "This function creates an if-then-else expression. It takes in a size, a condition, a true value, and a false value, validate the condition type, and returns a class expression representing the if-then-else expression.", "Arguments": ":param size: Integer. The size of the BitVec expression to be created.\n:param cond: Bool. The condition for the if-then-else expression.\n:param true: BitVec. The value to be returned if the condition is true.\n:param false: BitVec. The value to be returned if the condition is false.\n:return: BitVec. The if-then-else expression."}, "tests": ["tests/core/smt/test_smtfunction.py::SmtFunctionTests::test_ite"], "indent": 4}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/reduction.py", "signature_position": [30, 30], "body_position": [31, 39], "dependency": {"intra_class": ["sumy.summarizers.reduction.ReductionSummarizer._rate_sentences_edge"], "intra_file": [], "cross_file": ["sumy.models.dom._document.ObjectDocumentModel.sentences"]}, "requirement": {"Functionality": "This function rates the sentences in a document based on their similarity. It calculates the similarity between each pair of sentences and assigns a rating to each sentence based on the similarity with other sentences.", "Arguments": ":param self: ReductionSummarizer. An instance of the ReductionSummarizer class.\n:param document: Document. The document containing the sentences to be rated.\n:return: defaultdict. A dictionary containing the ratings for each sentence."}, "tests": ["tests/test_summarizers/test_reduction.py::test_sentences_rating"], "indent": 8}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "type": "method", "project_path": "Scientific-Engineering/lux", "completion_path": "Scientific-Engineering/lux/lux/vis/Vis.py", "signature_position": [142, 142], "body_position": [143, 149], "dependency": {"intra_class": ["lux.vis.Vis.Vis._inferred_intent"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function retrieves the attribute based on the given channel from the inferred intent list. It filters the list based on the channel and value attributes of each object in the list and returns the filtered list.", "Arguments": ":param self: Vis. An instance of the Vis class.\n:param channel: The channel to filter the inferred intent list.\n:return: List. The filtered list of objects from the inferred intent list."}, "tests": ["tests/test_columns.py::test_special_char", "tests/test_dates.py::test_refresh_inplace"], "indent": 8}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/sum_basic.py", "signature_position": [98, 98], "body_position": [99, 120], "dependency": {"intra_class": ["sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "sumy.summarizers.sum_basic.SumBasicSummarizer._find_index_of_best_sentence", "sumy.summarizers.sum_basic.SumBasicSummarizer._update_tf"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function computes the ratings of sentences based on their importance in the text. It takes a list of sentences as input and calculates the frequency of each word in the sentences. It then iteratively selects the most important sentence based on the word frequency and removes it from the list of sentences. The importance value of each sentence is the iteration in which it was removed multiplied by -1. The ratings of all sentences are returned as a dictionary.", "Arguments": ":param self: SumBasicSummarizer. An instance of the SumBasicSummarizer class.\n:param sentences: List of Sentence. The sentences in the text.\n:return: Dictionary. The ratings of sentences, where the key is the sentence and the value is its rating."}, "tests": ["tests/test_summarizers/test_sum_basic.py::test_compute_ratings"], "indent": 8}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "type": "method", "project_path": "Utilities/boltons", "completion_path": "Utilities/boltons/boltons/tbutils.py", "signature_position": [441, 441], "body_position": [447, 448], "dependency": {"intra_class": ["boltons.tbutils.ExceptionInfo.exc_msg", "boltons.tbutils.ExceptionInfo.exc_type", "boltons.tbutils.ExceptionInfo.tb_info", "boltons.tbutils.TracebackInfo.get_formatted"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns a formatted string that mimics the output of the traceback.format_exception function. It combines the formatted traceback information with the exception type and message.", "Arguments": ":param self: ExceptionInfo. An instance of the ExceptionInfo class.\n:return: str. The formatted string containing the traceback information, exception type, and exception message."}, "tests": ["tests/test_tbutils.py::test_contextual"], "indent": 8}
{"namespace": "barf.core.smt.smtfunction.concat", "type": "function", "project_path": "Security/barf", "completion_path": "Security/barf/barf/core/smt/smtfunction.py", "signature_position": [66, 66], "body_position": [67, 70], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["barf.core.smt.smtsymbol.BitVec"]}, "requirement": {"Functionality": "Concatenate multiple BitVec objects into a single BitVec object. If only one BitVec object is provided, it is returned as is.", "Arguments": ":param size: Integer. The size of the resulting BitVec object.\n:param *args: BitVec objects. Multiple BitVec objects to be concatenated.\n:return: BitVec. The concatenated BitVec object."}, "tests": ["tests/core/smt/test_smtfunction.py::SmtFunctionTests::test_concat"], "indent": 4}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "type": "method", "project_path": "Database/datasette", "completion_path": "Database/datasette/datasette/facets.py", "signature_position": [210, 210], "body_position": [211, 287], "dependency": {"intra_class": ["datasette.facets.ColumnFacet.type"], "intra_file": ["datasette.facets.Facet.database", "datasette.facets.Facet.ds", "datasette.facets.Facet.get_configs", "datasette.facets.Facet.get_facet_size", "datasette.facets.Facet.get_querystring_pairs", "datasette.facets.Facet.params", "datasette.facets.Facet.request", "datasette.facets.Facet.sql", "datasette.facets.Facet.table"], "cross_file": ["datasette.database.QueryInterrupted", "datasette.utils.escape_sqlite", "datasette.utils.path_with_added_args", "datasette.utils.path_with_removed_args", "datasette.app.Datasette.absolute_url", "datasette.app.Datasette.execute", "datasette.app.Datasette.expand_foreign_keys", "datasette.app.Datasette.setting", "datasette.app.Datasette.urls", "datasette.url_builder.Urls.path"]}, "requirement": {"Functionality": "This function retrieves facet results for a column facet. It executes a SQL query to get the facet values and their corresponding counts, and then formats the results into a list of dictionaries. Each dictionary represents a facet value and includes information such as the value itself, its label (if available), the count, and a toggle URL. The function also handles cases where the facet results exceed the specified facet size by truncating the results and setting a \"truncated\" flag.", "Arguments": ":param self: ColumnFacet. An instance of the ColumnFacet class.\n:return: Tuple[List[Dict[str, Any]], List[str]]. A tuple containing the facet results and a list of facets that timed out during execution. The facet results is a list of dictionaries, where each dictionary represents a facet value and includes information such as the value, label, count, toggle URL, and selected flag. The facets_timed_out list contains the names of facets that timed out during execution."}, "tests": ["tests/test_facets.py::test_column_facet_from_metadata_cannot_be_hidden", "tests/test_facets.py::test_column_facet_results", "tests/test_facets.py::test_column_facet_results_column_starts_with_underscore"], "indent": 8}
{"namespace": "hl7.client.MLLPClient.send_message", "type": "method", "project_path": "Communications/hl7", "completion_path": "Communications/hl7/hl7/client.py", "signature_position": [60, 60], "body_position": [69, 80], "dependency": {"intra_class": ["hl7.client.MLLPClient.encoding", "hl7.client.MLLPClient.send"], "intra_file": ["hl7.client.CR", "hl7.client.EB", "hl7.client.SB"], "cross_file": ["hl7.containers.Message"]}, "requirement": {"Functionality": "This function takes a message and wraps it in a MLLP (Minimum Lower Layer Protocol) container before sending it to the server.\nIt handles different types of input messages and encodes them accordingly.", "Arguments": ":param self: MLLPClient. An instance of the MLLPClient class.\n:param message: The message to be sent. It can be a byte string, unicode string, or hl7.Message object.\n:return: The response received after sending the message."}, "tests": ["tests/test_client.py::MLLPClientTest::test_send_message_bytestring", "tests/test_client.py::MLLPClientTest::test_send_message_hl7_message", "tests/test_client.py::MLLPClientTest::test_send_message_unicode"], "indent": 8}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/edmundson.py", "signature_position": [90, 90], "body_position": [91, 93], "dependency": {"intra_class": ["sumy.summarizers.edmundson.EdmundsonSummarizer._build_cue_method_instance"], "intra_file": [], "cross_file": ["sumy.summarizers.edmundson_cue.EdmundsonCueMethod.__call__"]}, "requirement": {"Functionality": "This function applies the cue method for text summarization. It creates an instance of the cue method and uses it to summarize the given document by selecting a specified number of sentences. The bonus_word_value and stigma_word_value parameters determine the weight of bonus and stigma words in the summarization process.", "Arguments": ":param self: EdmundsonSummarizer. An instance of the EdmundsonSummarizer class.\n:param document: Document. The document to be summarized.\n:param sentences_count: Integer. The number of sentences to be selected for the summary.\n:param bonus_word_value: Integer. The weight of bonus words in the summarization process. Defaults to 1.\n:param stigma_word_value: Integer. The weight of stigma words in the summarization process. Defaults to 1.\n:return: Tuple. The summarized text."}, "tests": ["tests/test_summarizers/test_edmundson.py::test_cue_1", "tests/test_summarizers/test_edmundson.py::test_cue_letters_case", "tests/test_summarizers/test_edmundson.py::test_cue_with_no_words", "tests/test_summarizers/test_edmundson.py::test_cue_3", "tests/test_summarizers/test_edmundson.py::test_cue_empty"], "indent": 8}
{"namespace": "barf.core.smt.smtfunction.extract", "type": "function", "project_path": "Security/barf", "completion_path": "Security/barf/barf/core/smt/smtfunction.py", "signature_position": [50, 50], "body_position": [51, 56], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["barf.core.smt.smtsymbol.BitVec", "barf.core.smt.smtsymbol.Constant"]}, "requirement": {"Functionality": "This function extracts a portion of a given object and returns a new object with the extracted portion. If the offset is 0 and the size is equal to the size of the input object, the function returns the input object itself.", "Arguments": ":param s: Constant or BitVec. The input object from which the portion needs to be extracted.\n:param offset: Integer. The starting index of the portion to be extracted.\n:param size: Integer. The size of the portion to be extracted.\n:return: BitVec. A new BitVec object with the extracted portion."}, "tests": ["tests/core/smt/test_smtfunction.py::SmtFunctionTests::test_extract"], "indent": 4}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/sum_basic.py", "signature_position": [33, 33], "body_position": [34, 37], "dependency": {"intra_class": ["sumy.summarizers.sum_basic.SumBasicSummarizer._filter_out_stop_words", "sumy.summarizers.sum_basic.SumBasicSummarizer._normalize_words", "sumy.summarizers.sum_basic.SumBasicSummarizer._stem_words"], "intra_file": [], "cross_file": ["sumy.models.dom._sentence.Sentence.words"]}, "requirement": {"Functionality": "This function takes a sentence as input and returns the content words in that sentence. It performs several operations on the sentence, including normalizing the words, filtering out stop words, and stemming the content words.", "Arguments": ":param self: SumBasicSummarizer. An instance of the SumBasicSummarizer class.\n:param sentence: The input sentence.\n:return: A list of content words in the sentence after performing normalization, stop word filtering, and stemming."}, "tests": ["tests/test_summarizers/test_sum_basic.py::test_stemmer"], "indent": 8}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/parsers/plaintext.py", "signature_position": [60, 60], "body_position": [61, 78], "dependency": {"intra_class": ["sumy.parsers.plaintext.PlaintextParser._text", "sumy.parsers.plaintext.PlaintextParser._to_sentences"], "intra_file": [], "cross_file": ["sumy.parsers.parser.DocumentParser._tokenizer", "sumy.models.dom._document.ObjectDocumentModel", "sumy.models.dom._paragraph.Paragraph", "sumy.models.dom._sentence.Sentence"]}, "requirement": {"Functionality": "This function parses the plaintext document saves in this instance and creates a document model object. It iterates through each line of the input text, identifies sentences and paragraphs, and creates corresponding objects. The final document model is returned.", "Arguments": ":param self: PlaintextParser. An instance of the PlaintextParser class.\n:return: ObjectDocumentModel. The created document model object."}, "tests": ["tests/test_evaluation/test_evaluation_rouge.py::test_rouge_l_sentence_level", "tests/test_evaluation/test_evaluation_rouge.py::test_ngrams_for_more_sentences_should_not_return_words_at_boundaries", "tests/test_plaintext_parser.py::test_parse_plaintext_long", "tests/test_plaintext_parser.py::test_parse_plaintext", "tests/test_evaluation/test_evaluation_rouge.py::test_rouge_l_summary_level"], "indent": 8}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "type": "method", "project_path": "Internet/pyramid", "completion_path": "Internet/pyramid/src/pyramid/config/__init__.py", "signature_position": [881, 881], "body_position": [888, 906], "dependency": {"intra_class": ["pyramid.config.Configurator.begin", "pyramid.config.Configurator.end", "pyramid.config.Configurator.registry"], "intra_file": ["pyramid.config.global_registries"], "cross_file": ["pyramid.events.ApplicationCreated", "pyramid.registry.Registry.notify", "pyramid.router.Router", "pyramid.config.actions.ActionConfiguratorMixin.commit", "pyramid.util.WeakOrderedSet.add"]}, "requirement": {"Functionality": "This function is a method of the Configurator class. It commits any pending configuration statements, sends an ApplicationCreated event to all listeners, adds this configuration's registry to global, and returns a Pyramid WSGI application representing the committed configuration state.", "Arguments": ":param self: Configurator. An instance of the Configurator class.\n:return: Router. The WSGI application representing the committed configuration state."}, "tests": ["tests/test_config/test_init.py::TestGlobalRegistriesIntegration::test_global_registries", "tests/test_config/test_init.py::ConfiguratorTests::test_make_wsgi_app", "tests/test_integration.py::TestConflictApp::test_overridden_route_view", "tests/test_integration.py::TestConflictApp::test_overridden_authorization_policy", "tests/test_integration.py::TestConflictApp::test_autoresolved_view"], "indent": 8}
{"namespace": "datasette.app.DatasetteClient.get", "type": "method", "project_path": "Database/datasette", "completion_path": "Database/datasette/datasette/app.py", "signature_position": [1556, 1556], "body_position": [1557, 1558], "dependency": {"intra_class": ["datasette.app.DatasetteClient._fix", "datasette.app.DatasetteClient.app"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function sends an HTTP GET request to the specified path using the DatasetteClient instance. It uses the httpx library to make the asynchronous request.", "Arguments": ":param self: DatasetteClient. An instance of the DatasetteClient class.\n:param path: String. The path to send the GET request to.\n:param kwargs: Additional keyword arguments that can be passed to the httpx client.\n:return: The response from the GET request."}, "tests": ["tests/test_facets.py::test_facet_size", "tests/test_internals_datasette.py::test_datasette_constructor", "tests/test_facets.py::test_json_array_with_blanks_and_nulls", "tests/test_internals_datasette.py::test_num_sql_threads_zero", "tests/test_api.py::test_hidden_sqlite_stat1_table"], "indent": 8}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/kl.py", "signature_position": [54, 54], "body_position": [60, 64], "dependency": {"intra_class": ["sumy.summarizers.kl.KLSummarizer._compute_word_freq", "sumy.summarizers.kl.KLSummarizer._get_all_content_words_in_doc"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function computes the normalized term frequency for a given list of sentences. It first extracts the content words from the sentences, then calculates the frequency of each content word. Finally, it normalizes the term frequency by dividing the frequency of each content word by the total number of content words in the document.", "Arguments": ":param self: KLSummarizer. An instance of the KLSummarizer class.\n:param sentences: List of Sentence objects. The sentences from which to compute the term frequency.\n:return: Dictionary. A dictionary mapping each content word to its normalized term frequency."}, "tests": ["tests/test_summarizers/test_kl.py::test_tf_idf_metric_should_be_real_number"], "indent": 8}
{"namespace": "passpie.database.PasspieStorage.delete", "type": "method", "project_path": "Security/passpie", "completion_path": "Security/passpie/passpie/database.py", "signature_position": [26, 26], "body_position": [27, 31], "dependency": {"intra_class": ["passpie.database.PasspieStorage.make_credpath"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Delete the credentials from the PasspieStorage instance. It iterates over the list of credentials and deletes the corresponding files from the storage. If the directory containing the file becomes empty after deletion, it is also removed.", "Arguments": ":param self: PasspieStorage. An instance of the PasspieStorage class.\n:param credentials: List of dictionaries. A list of credentials, where each credential is represented as a dictionary with \"name\" and \"login\" keys.\n:return: No return values."}, "tests": ["tests/test_database.py::StorageTests::test_delete_removes_empty_directories", "tests/test_database.py::StorageTests::test_delete_removes_credentials_files"], "indent": 8}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/reduction.py", "signature_position": [41, 41], "body_position": [42, 43], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["sumy.summarizers._summarizer.AbstractSummarizer.normalize_word", "sumy.summarizers._summarizer.AbstractSummarizer.stem_word", "sumy.models.dom._sentence.Sentence.words"]}, "requirement": {"Functionality": "This function takes a sentence as input and converts it into a set of words. It normalizes each word in the sentence and removes any stop words. The resulting set of words is returned.", "Arguments": ":param self: ReductionSummarizer. An instance of the ReductionSummarizer class.\n:param sentence: Sentence. The sentence to be converted into a set of words.\n:return: List. The set of words after normalization and removal of stop words."}, "tests": ["tests/test_summarizers/test_reduction.py::test_stop_words_correctly_removed"], "indent": 8}
{"namespace": "check_dummies.find_backend", "type": "function", "project_path": "Scientific-Engineering/diffusers", "completion_path": "Scientific-Engineering/diffusers/utils/check_dummies.py", "signature_position": [58, 58], "body_position": [60, 64], "dependency": {"intra_class": [], "intra_file": ["check_dummies._re_backend"], "cross_file": []}, "requirement": {"Functionality": "This function searches for one or multiple backends in a code line of the init and returns them as a string joined by \"_and_\" if found.", "Arguments": ":param line: str. The code line to search for backends.\n:return: str or None. The backends found in the code line joined by \"_and_\" if found, otherwise None."}, "tests": ["tests/others/test_check_dummies.py::CheckDummiesTester::test_find_backend"], "indent": 4}
{"namespace": "sumy.utils.get_stop_words", "type": "function", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/utils.py", "signature_position": [66, 66], "body_position": [67, 72], "dependency": {"intra_class": [], "intra_file": ["sumy.utils.normalize_language", "sumy.utils.parse_stop_words"], "cross_file": []}, "requirement": {"Functionality": "This function retrieves the stop words for a given language. The language name is normalized before retrieval. If the data is not available, it raises a LookupError. The data is converted before being returned.", "Arguments": ":param language: str. The language for which stop words are needed.\n:return: frozenset. The stop words for the given language."}, "tests": ["tests/test_utils/test_utils.py::test_missing_stop_words_language", "tests/test_utils/test_utils.py::test_ok_stop_words_language", "tests/test_summarizers/test_luhn.py::test_real_example"], "indent": 4}
{"namespace": "sacred.ingredient.Ingredient.config", "type": "method", "project_path": "Utilities/sacred", "completion_path": "Utilities/sacred/sacred/ingredient.py", "signature_position": [150, 150], "body_position": [162, 163], "dependency": {"intra_class": ["sacred.ingredient.Ingredient.configurations"], "intra_file": [], "cross_file": ["sacred.config.config_scope.ConfigScope", "sacred.config.config_scope.ConfigScope.__init__"]}, "requirement": {"Functionality": "This function is a decorator that adds a function to the configuration of the Experiment. The decorated function is turned into a ConfigScope instance and added to the Ingredient/Experiment. When the experiment is run, this function will also be executed and all json-serializable local variables inside it will end up as entries in the configuration of the experiment.", "Arguments": ":param self: Ingredient. An instance of the Ingredient class.\n:param function: The function to be added to the configuration of the Experiment.\n:return: The ConfigScope object that represents the added function."}, "tests": ["tests/test_experiment.py::test_named_config_and_ingredient"], "indent": 8}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/nlp/tokenizers.py", "signature_position": [186, 186], "body_position": [187, 192], "dependency": {"intra_class": ["sumy.nlp.tokenizers.Tokenizer.LANGUAGE_EXTRA_ABREVS", "sumy.nlp.tokenizers.Tokenizer._language", "sumy.nlp.tokenizers.Tokenizer._sentence_tokenizer"], "intra_file": ["sumy.nlp.tokenizers.ArabicSentencesTokenizer.tokenize"], "cross_file": ["sumy._compat.to_unicode", "sumy._compat.unicode"]}, "requirement": {"Functionality": "This function takes a paragraph as input and tokenizes it into sentences using a sentence tokenizer. It first updates the abbreviations used by the tokenizer based on the language. The function returns a tuple of the tokenized sentences.", "Arguments": ":param self: Tokenizer. An instance of the Tokenizer class.\n:param paragraph: String. The paragraph to be tokenized into sentences.\n:return: Tuple of strings. The tokenized sentences."}, "tests": ["tests/test_tokenizers.py::test_ensure_czech_tokenizer_available", "tests/test_tokenizers.py::test_slovak_alias_into_czech_tokenizer", "tests/test_tokenizers.py::test_tokenize_japanese_paragraph", "tests/test_tokenizers.py::test_tokenize_sentences_with_abbreviations", "tests/test_tokenizers.py::test_tokenize_paragraph"], "indent": 8}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "type": "function", "project_path": "Security/barf", "completion_path": "Security/barf/barf/core/smt/smtfunction.py", "signature_position": [32, 32], "body_position": [33, 38], "dependency": {"intra_class": [], "intra_file": [], "cross_file": ["barf.core.smt.smtsymbol.BitVec", "barf.core.smt.smtsymbol.Constant"]}, "requirement": {"Functionality": "This function extends a given value to a specified size by zero-padding. It checks the input value is of relevant type and if the size difference is non-negative. If the size is already equal to the value's size, it returns the value as is. Otherwise, it creates a new Class with the specified size and the zero-extend operation and returns it.", "Arguments": ":param s: Constant or BitVec. The value to be extended.\n:param size: Integer. The desired size to extend the value to.\n:return: BitVec. The value after zero-extension."}, "tests": ["tests/core/smt/test_smtfunction.py::SmtFunctionTests::test_zero_extend"], "indent": 4}
{"namespace": "check_dummies.create_dummy_object", "type": "function", "project_path": "Scientific-Engineering/diffusers", "completion_path": "Scientific-Engineering/diffusers/utils/check_dummies.py", "signature_position": [105, 105], "body_position": [107, 112], "dependency": {"intra_class": [], "intra_file": ["check_dummies.DUMMY_CLASS", "check_dummies.DUMMY_CONSTANT", "check_dummies.DUMMY_FUNCTION"], "cross_file": []}, "requirement": {"Functionality": "This function creates the code for a dummy object based on the given `name` and `backend_name`. It checks the case of the `name` and returns the corresponding code template with the `name` and `backend_name` filled in.", "Arguments": ":param name: str. The name of the object.\n:param backend_name: str. The name of the backend.\n:return: str. The code for the dummy object."}, "tests": ["tests/others/test_check_dummies.py::CheckDummiesTester::test_create_dummy_object"], "indent": 4}
{"namespace": "sacred.experiment.Experiment.run", "type": "method", "project_path": "Utilities/sacred", "completion_path": "Utilities/sacred/sacred/experiment.py", "signature_position": [237, 245], "body_position": [273, 277], "dependency": {"intra_class": ["sacred.experiment.Experiment._create_run"], "intra_file": [], "cross_file": ["sacred.run.Run"]}, "requirement": {"Functionality": "This function runs the main function of an experiment or a given command. It creates a run instance based on the input parameters and executes it.", "Arguments": ":param self: Experiment. An instance of the Experiment class.\n:param command_name: Optional string. The name of the command to be run. Defaults to the main function.\n:param config_updates: Optional dictionary. Changes to the configuration as a nested dictionary.\n:param named_configs: Sequence of strings. A list of names of named_configs to use.\n:param info: Optional dictionary. Additional information for this run.\n:param meta_info: Optional dictionary. Additional meta information for this run.\n:param options: Optional dictionary. Dictionary of options to use.\n:return: Run. The Run object corresponding to the finished run."}, "tests": ["tests/test_modules.py::test_experiment_run_subingredient_function", "tests/test_modules.py::test_experiment_named_config_subingredient_overwrite", "tests/test_modules.py::test_ingredient_command", "tests/test_modules.py::test_experiment_double_named_config", "tests/test_experiment.py::test_named_config_and_ingredient"], "indent": 8}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/sum_basic.py", "signature_position": [61, 61], "body_position": [65, 69], "dependency": {"intra_class": ["sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function computes the normalized term frequency of content words in a document. It first retrieves all the content words from the given sentences, then calculates the frequency of each content word. Finally, it normalizes the term frequency by dividing the frequency of each content word by the total count of content words in the document.", "Arguments": ":param self: SumBasicSummarizer. An instance of the SumBasicSummarizer class.\n:param sentences: List of Sentence. The sentences in the document.\n:return: Dict. A dictionary containing the normalized term frequency of each content word."}, "tests": ["tests/test_summarizers/test_sum_basic.py::test_compute_tf"], "indent": 8}
{"namespace": "viztracer.tracer._VizTracer.stop", "type": "method", "project_path": "System/viztracer", "completion_path": "System/viztracer/src/viztracer/tracer.py", "signature_position": [254, 254], "body_position": [255, 258], "dependency": {"intra_class": ["viztracer.tracer._VizTracer._tracer", "viztracer.tracer._VizTracer.enable", "viztracer.tracer._VizTracer.log_print", "viztracer.tracer._VizTracer.restore_print"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Stop the VizTracer instance. It disables the tracer, restores the print function if the log print is True, and stops the tracer.", "Arguments": ":param self: _VizTracer. An instance of the _VizTracer class.\n:return: No return values."}, "tests": ["tests/test_tracer.py::TestTracerFeature::test_log_gc", "tests/test_tracer.py::TestTracerFilter::test_ignore_c_function", "tests/test_tracer.py::TestTracerFeature::test_log_func_retval", "tests/test_tracer.py::TestTracerFilter::test_exclude_files"], "indent": 8}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "type": "method", "project_path": "Security/barf", "completion_path": "Security/barf/barf/core/smt/smtsymbol.py", "signature_position": [278, 278], "body_position": [279, 280], "dependency": {"intra_class": ["barf.core.smt.smtsymbol.BitVecArray.key_size", "barf.core.smt.smtsymbol.BitVecArray.name", "barf.core.smt.smtsymbol.BitVecArray.value_size"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function generates a declaration string for a BitVecArray instance. The declaration string specifies the name of the BitVecArray, as well as the sizes of the key and value BitVectors. The output format is \"(declare-fun {name} () (Array (_ BitVec {key size}) (_ BitVec {value size})))\".", "Arguments": ":param self: BitVecArray. An instance of the BitVecArray class.\n:return: str. The declaration string for the BitVecArray instance."}, "tests": ["tests/core/smt/test_smtsymbol.py::BitVecArrayTests::test_declaration"], "indent": 8}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/edmundson.py", "signature_position": [110, 110], "body_position": [111, 112], "dependency": {"intra_class": ["sumy.summarizers.edmundson.EdmundsonSummarizer._build_title_method_instance"], "intra_file": [], "cross_file": ["sumy.summarizers.edmundson_title.EdmundsonTitleMethod.__call__"]}, "requirement": {"Functionality": "This function applies the title method of summarization to a given document and returns the summarized text. It first creates an instance of the title method and then uses it to summarize the document.", "Arguments": ":param self: EdmundsonSummarizer. An instance of the EdmundsonSummarizer class.\n:param document: String. The document to be summarized.\n:param sentences_count: Integer. The number of sentences to include in the summary.\n:return: Tuple. The summarized text."}, "tests": ["tests/test_summarizers/test_edmundson.py::test_title_method_without_null_words", "tests/test_summarizers/test_edmundson.py::test_title_method_1", "tests/test_summarizers/test_edmundson.py::test_title_method_with_empty_document", "tests/test_summarizers/test_edmundson.py::test_title_method_2", "tests/test_summarizers/test_edmundson.py::test_title_method_3"], "indent": 8}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/sum_basic.py", "signature_position": [55, 55], "body_position": [56, 59], "dependency": {"intra_class": ["sumy.summarizers.sum_basic.SumBasicSummarizer._filter_out_stop_words", "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_words_in_doc", "sumy.summarizers.sum_basic.SumBasicSummarizer._normalize_words"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function takes a list of sentences as input and returns a list of normalized content words. It first gets all the words in the sentences, then filters out the stop words, and finally normalizes the remaining content words.", "Arguments": ":param self: SumBasicSummarizer. An instance of the SumBasicSummarizer class.\n:param sentences: List of Sentence. The sentences from which to extract the content words.\n:return: List of strings. The normalized content words extracted from the sentences."}, "tests": ["tests/test_summarizers/test_sum_basic.py::test_get_all_content_words_in_doc"], "indent": 8}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "type": "method", "project_path": "Security/barf", "completion_path": "Security/barf/barf/arch/emulator.py", "signature_position": [369, 369], "body_position": [370, 382], "dependency": {"intra_class": ["barf.arch.emulator.Emulator._load_binary_elf", "barf.arch.emulator.Emulator._load_binary_pe"], "intra_file": [], "cross_file": ["barf.core.binary.BinaryFile.filename"]}, "requirement": {"Functionality": "This function reads a binary file and determines its format based on the file signature. If it is b'\\x7fELF', it is an ELF file; if it is b'MZ', it is a PE file. It then calls the corresponding private method to further process the binary file. If there is error during reading, it raises an exception with the message \"Error loading file.\" If the file format is not recognized, it raises an exception with the message \"Unknown file format.\"", "Arguments": ":param self: Emulator. An instance of the Emulator class.\n:param binary: The binary file to load.\n:return: No return values."}, "tests": ["tests/arch/test_emulator.py::EmulatorTests::test_emulate_x86_64", "tests/arch/test_emulator.py::EmulatorTests::test_emulate_x86"], "indent": 8}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/summarizers/edmundson.py", "signature_position": [101, 101], "body_position": [102, 103], "dependency": {"intra_class": ["sumy.summarizers.edmundson.EdmundsonSummarizer._build_key_method_instance"], "intra_file": [], "cross_file": ["sumy.summarizers.edmundson_key.EdmundsonKeyMethod.__call__"]}, "requirement": {"Functionality": "This function applies the key method of summarization to a given document. It first builds an instance of the key method and then uses it to summarize the document by selecting a specified number of sentences based on their importance.", "Arguments": ":param self: EdmundsonSummarizer. An instance of the EdmundsonSummarizer class.\n:param document: Document. The document to be summarized.\n:param sentences_count: Integer. The number of sentences to be selected for the summary.\n:param weight: Float. The weight to be assigned to the key method. Defaults to 0.5.\n:return: Tuple. The summarized text."}, "tests": ["tests/test_summarizers/test_edmundson.py::test_key_without_bonus_words", "tests/test_summarizers/test_edmundson.py::test_key_empty", "tests/test_summarizers/test_edmundson.py::test_key_2", "tests/test_summarizers/test_edmundson.py::test_key_3", "tests/test_summarizers/test_edmundson.py::test_key_1"], "indent": 8}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "type": "method", "project_path": "Internet/sumy", "completion_path": "Internet/sumy/sumy/parsers/html.py", "signature_position": [30, 30], "body_position": [31, 32], "dependency": {"intra_class": ["sumy.parsers.html.HtmlParser.__init__"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function reads the contents of a file and creates an instance of the HtmlParser class using the file contents, tokenizer, and URL.", "Arguments": ":param cls: Class. The HtmlParser class.\n:param file_path: String. The path to the file to be read.\n:param url: String. The URL associated with the file.\n:param tokenizer: Object. The tokenizer to be used for parsing the HTML.\n:return: HtmlParser. An instance of the HtmlParser class."}, "tests": ["tests/test_html_parser.py::test_annotated_text"], "indent": 8}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "type": "method", "project_path": "Utilities/boltons", "completion_path": "Utilities/boltons/boltons/tbutils.py", "signature_position": [292, 292], "body_position": [309, 322], "dependency": {"intra_class": ["boltons.tbutils.TracebackInfo.__init__", "boltons.tbutils.TracebackInfo.callpoint_type"], "intra_file": ["boltons.tbutils.Callpoint.from_tb"], "cross_file": []}, "requirement": {"Functionality": "Create a new TracebackInfo instance based on the given traceback. It retrieves the traceback from the currently handled exception or from the input parameter. It then recursively goes up the stack a maximum of *limit* times and creates a list of callpoint items. Finally, it returns the TracebackInfo instance with the list of callpoint items.", "Arguments": ":param cls: type. The class itself.\n:param tb: TracebackType. The traceback object. If not provided, it fins the traceback from the currently handled exception. If no exception is being handled, it raises a ValueError: 'no tb set and no exception being handled'. Defaults to None.\n:param limit: int. The maximum number of parent frames to extract. It defaults to system traceback limit if not provided. If that is not available, it defaults to 1000. Defaults to None.\n:return: TracebackInfo. The created TracebackInfo instance."}, "tests": ["tests/test_tbutils.py::test_exception_info"], "indent": 8}
