{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    json_types = (type(None), bool, dict, float, int, list, str, tuple)\n    return isinstance(val, json_types)\n"}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    parts = list(urllib.parse.urlsplit(url))\n    try:\n        parts[1].encode('ascii')\n    except UnicodeEncodeError:\n        # the url needs to be converted to idn notation\n        host = parts[1].rsplit(':', 1)\n        newhost = []\n        port = ''\n        if len(host) == 2:\n            port = host.pop()\n        for h in host[0].split('.'):\n            newhost.append(h.encode('idna').decode('utf-8'))\n        parts[1] = '.'.join(newhost)\n        if port:\n            parts[1] += ':' + port\n        return urllib.parse.urlunsplit(parts)\n    else:\n        return url\n"}
{"namespace": "mistune.toc.add_toc_hook", "completion": "    if heading_id is None:\n        def heading_id(token, index):\n            return 'toc_' + str(index + 1)\n\n    def toc_hook(md, state):\n        headings = []\n\n        for tok in state.tokens:\n            if tok['type'] == 'heading':\n                level = tok['attrs']['level']\n                if min_level <= level <= max_level:\n                    headings.append(tok)\n\n        toc_items = []\n        for i, tok in enumerate(headings):\n            tok['attrs']['id'] = heading_id(tok, i)\n            toc_items.append(normalize_toc_item(md, tok))\n\n        # save items into state\n        state.env['toc_items'] = toc_items\n\n    md.before_render_hooks.append(toc_hook)\n"}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.insert_rule(md.block.block_quote_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.block_quote_rules, 'nptable', before='paragraph')\n"}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')\n"}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n    if kwargs:\n        callback = partial(callback, **kwargs)\n    with futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n        for ret in executor.map(callback, texts):\n            yield ret\n"}
{"namespace": "parsel.utils.shorten", "completion": "    if len(text) <= width:\n        return text\n    if width > len(suffix):\n        return text[: width - len(suffix)] + suffix\n    if width >= 0:\n        return suffix[len(suffix) - width :]\n    raise ValueError(\"width must be equal or greater than 0\")\n"}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    ns_fns = etree.FunctionNamespace(None)  # type: ignore[attr-defined]\n    if func is not None:\n        ns_fns[fname] = func\n    else:\n        del ns_fns[fname]\n"}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  context = [threading.current_thread()]\n  if greenlet:\n    context.append(greenlet.getcurrent())\n  return hash(tuple(context))\n"}
{"namespace": "dominate.util.system", "completion": "  import subprocess\n  s = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stdin=subprocess.PIPE)\n  out, err = s.communicate(data)\n  return out.decode('utf8')\n"}
{"namespace": "dominate.util.url_unescape", "completion": "  return re.sub('%([0-9a-fA-F]{2})',\n    lambda m: unichr(int(m.group(1), 16)), data)\n"}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n"}
{"namespace": "rows.fields.Field.serialize", "completion": "        if value is None:\n            value = \"\"\n        return value\n"}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n"}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.binary_type):\n        raise ValueError(\"Binary is not supported\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)\n"}
{"namespace": "rows.fields.get_items", "completion": "    return lambda obj: tuple(\n        obj[index] if len(obj) > index else None for index in indexes\n    )\n"}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    result = {}\n    if path:\n        if not os.path.exists(path):\n            logger.warning('file not found.%s' % path)\n            return result\n        else:\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    result[terms[0]] = terms[1]\n    return result\n"}
{"namespace": "natasha.span.envelop_spans", "completion": "    index = 0\n    for envelope in envelopes:\n        chunk = []\n        while index < len(spans):\n            span = spans[index]\n            index += 1\n            if span.start < envelope.start:\n                continue\n            elif span.stop <= envelope.stop:\n                chunk.append(span)\n            else:\n                index -= 1\n                break\n        yield chunk\n"}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    urlencoded_params = urllib.parse.parse_qs(content)\n    params = {}\n    for key, value in urlencoded_params.items():\n        if len(value) != 1:\n            msg = \"URL-encoded content contains a repeated value:\" \"%s -> %s\" % (\n                key,\n                \", \".join(value),\n            )\n            raise ValueError(msg)\n        params[key] = value[0]\n    return params\n"}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        async for item in t.cast(\"t.AsyncIterable[V]\", iterable):\n            yield item\n    else:\n        for item in t.cast(\"t.Iterable[V]\", iterable):\n            yield item\n"}
{"namespace": "jinja2.utils.consume", "completion": "    for _ in iterable:\n        pass\n"}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if pos:\n        if cut_type == 'word':\n            word_pos_seq = posseg.lcut(sentence)\n            word_seq, pos_seq = [], []\n            for w, p in word_pos_seq:\n                word_seq.append(w)\n                pos_seq.append(p)\n            return word_seq, pos_seq\n        elif cut_type == 'char':\n            word_seq = list(sentence)\n            pos_seq = []\n            for w in word_seq:\n                w_p = posseg.lcut(w)\n                pos_seq.append(w_p[0].flag)\n            return word_seq, pos_seq\n    else:\n        if cut_type == 'word':\n            return jieba.lcut(sentence)\n        elif cut_type == 'char':\n            return list(sentence)\n"}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is None:\n        return \"None\"\n    elif obj is Ellipsis:\n        return \"Ellipsis\"\n\n    cls = type(obj)\n\n    if cls.__module__ == \"builtins\":\n        return f\"{cls.__name__} object\"\n\n    return f\"{cls.__module__}.{cls.__name__} object\"\n"}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n"}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for w in list_of_words:\n            word_freq[w] = word_freq.get(w, 0) + 1\n        return word_freq\n"}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        content_words_count = len(content_words_in_sentence)\n        if content_words_count > 0:\n            word_freq_sum = sum([word_freq_in_doc[w] for w in content_words_in_sentence])\n            word_freq_avg = word_freq_sum / content_words_count\n            return word_freq_avg\n        else:\n            return 0\n"}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_metrics = {}\n        sentences_count = len(sentences)\n\n        for sentence in sentences:\n            for term in sentence:\n                if term not in idf_metrics:\n                    n_j = sum(1 for s in sentences if term in s)\n                    idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n\n        return idf_metrics\n"}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        unique_words1 = frozenset(sentence1)\n        unique_words2 = frozenset(sentence2)\n        common_words = unique_words1 & unique_words2\n\n        numerator = 0.0\n        for term in common_words:\n            numerator += tf1[term]*tf2[term] * idf_metrics[term]**2\n\n        denominator1 = sum((tf1[t]*idf_metrics[t])**2 for t in unique_words1)\n        denominator2 = sum((tf2[t]*idf_metrics[t])**2 for t in unique_words2)\n\n        if denominator1 > 0 and denominator2 > 0:\n            return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))\n        else:\n            return 0.0\n"}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    ngram_set = set()\n    text_length = len(text)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram_set.add(tuple(text[i:i + n]))\n    return ngram_set\n"}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    full_text_words = []\n    for s in sentences:\n        if not isinstance(s, Sentence):\n            raise (ValueError(\"Object in collection must be of type Sentence\"))\n        full_text_words.extend(s.words)\n    return full_text_words\n"}
{"namespace": "falcon.inspect.register_router", "completion": "    def wraps(fn):\n        if router_class in _supported_routers:\n            raise ValueError(\n                'Another function is already registered'\n                ' for the router {}'.format(router_class)\n            )\n        _supported_routers[router_class] = fn\n        return fn\n\n    return wraps\n"}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    def _traverse(roots, parent):\n        for root in roots:\n            path = parent + '/' + root.raw_segment\n            if root.resource is not None:\n                methods = []\n                if root.method_map:\n                    for method, func in root.method_map.items():\n                        if isinstance(func, partial):\n                            real_func = func.func\n                        else:\n                            real_func = func\n\n                        source_info = _get_source_info(real_func)\n                        internal = _is_internal(real_func)\n\n                        method_info = RouteMethodInfo(\n                            method, source_info, real_func.__name__, internal\n                        )\n                        methods.append(method_info)\n                source_info, class_name = _get_source_info_and_name(root.resource)\n\n                route_info = RouteInfo(path, class_name, source_info, methods)\n                routes.append(route_info)\n\n            if root.children:\n                _traverse(root.children, path)\n\n    routes = []  # type: List[RouteInfo]\n    _traverse(router._roots, '')\n    return routes\n"}
{"namespace": "falcon.inspect._is_internal", "completion": "    module = inspect.getmodule(obj)\n    if module:\n        return module.__name__.startswith('falcon.')\n    return False\n"}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    try:\n        module, instance = args.app_module.split(':', 1)\n    except ValueError:\n        parser.error(\n            'The app_module must include a colon between the module and instance'\n        )\n    try:\n        app = getattr(importlib.import_module(module), instance)\n    except AttributeError:\n        parser.error('{!r} not found in module {!r}'.format(instance, module))\n\n    if not isinstance(app, falcon.App):\n        if callable(app):\n            app = app()\n            if not isinstance(app, falcon.App):\n                parser.error(\n                    '{} did not return a falcon.App instance'.format(args.app_module)\n                )\n        else:\n            parser.error(\n                'The instance must be of falcon.App or be '\n                'a callable without args that returns falcon.App'\n            )\n    return app\n"}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(\n        description='Example: falcon-inspect-app myprogram:app'\n    )\n    parser.add_argument(\n        '-r',\n        '--route_only',\n        action='store_true',\n        help='Prints only the information regarding the routes',\n    )\n    parser.add_argument(\n        '-v',\n        '--verbose',\n        action='store_true',\n        help='More verbose output',\n    )\n    parser.add_argument(\n        '-i',\n        '--internal',\n        action='store_true',\n        help='Print also internal falcon route methods and error handlers',\n    )\n    parser.add_argument(\n        'app_module',\n        help='The module and app to inspect. Example: myapp.somemodule:api',\n    )\n    return parser\n"}
{"namespace": "falcon.util.uri.unquote_string", "completion": "    if len(quoted) < 2:\n        return quoted\n    elif quoted[0] != '\"' or quoted[-1] != '\"':\n        # return original one, prevent side-effect\n        return quoted\n\n    tmp_quoted = quoted[1:-1]\n\n    # PERF(philiptzou): Most header strings don't contain \"quoted-pair\" which\n    # defined by RFC 7320. We use this little trick (quick string search) to\n    # speed up string parsing by preventing unnecessary processes if possible.\n    if '\\\\' not in tmp_quoted:\n        return tmp_quoted\n    elif r'\\\\' not in tmp_quoted:\n        return tmp_quoted.replace('\\\\', '')\n    else:\n        return '\\\\'.join([q.replace('\\\\', '') for q in tmp_quoted.split(r'\\\\')])\n"}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    sig = inspect.signature(func)\n\n    args = [\n        param.name\n        for param in sig.parameters.values()\n        if param.kind\n        not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD)\n    ]\n\n    # NOTE(kgriffs): Depending on the version of Python, 'self' may or may not\n    # be present, so we normalize the results by removing 'self' as needed.\n    # Note that this behavior varies between 3.x versions.\n    if args and args[0] == 'self':\n        args = args[1:]\n\n    return args\n"}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    app_args = inspect.getfullargspec(app).args\n    num_app_args = len(app_args)\n\n    # NOTE(kgriffs): Technically someone could name the \"self\" or \"cls\"\n    #   arg something else, but we will make the simplifying\n    #   assumption that this is rare enough to not worry about.\n    if app_args[0] in {'cls', 'self'}:\n        num_app_args -= 1\n\n    is_asgi = num_app_args == 3\n\n    return is_asgi\n"}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None\n"}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if settings.USE_TZ and is_naive(dt):\n        return make_aware(dt, timezone=timezone.utc)\n\n    return dt\n"}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    if cv is None:\n        cv = 1\n    if lv is None:\n        lv = 0\n    return cv + lv\n"}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self\n"}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        policy = ('{\"Statement\":[{\"Resource\":\"%(resource)s\",'\n                  '\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":'\n                  '%(expires)s}}}]}' % locals())\n        return policy\n"}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if not p[0] == \"/\":\n            p = \"/%s\" % p\n        return urllib.parse.quote(p, safe = \"/*\")\n"}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        code = int(resp[start:stop])\n    except ValueError:\n        return 400  # Bad Request\n    else:\n        return code\n"}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (tuple, list, set)):\n        return [to_unicode(s) for s in scope]\n    elif scope is None:\n        return None\n    return scope.strip().split()\n"}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None or isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    return str(x)\n"}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    if isinstance(x, bytes):\n        return x\n    if isinstance(x, str):\n        return x.encode(charset, errors)\n    if isinstance(x, (int, float)):\n        return str(x).encode(charset, errors)\n    return bytes(x)\n"}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    s += b'=' * (-len(s) % 4)\n    return base64.urlsafe_b64decode(s)\n"}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    return conn.execute(\n        \"\"\"\n        select count(*) from sqlite_master\n        where type=\"table\" and name=?\n    \"\"\",\n        [table],\n    ).fetchone()[0]\n"}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        if not os.path.isfile(filename):\n            raise IOError('file %s does not exist' % (filename))\n        GET_TABLENAMES = 'SELECT name FROM sqlite_master WHERE type=\"table\"'\n        with sqlite3.connect(filename) as conn:\n            cursor = conn.execute(GET_TABLENAMES)\n            res = cursor.fetchall()\n\n        return [name[0] for name in res]\n"}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    prefixes = [prefix.lower() for prefix in prefixes]\n    formatted_sql = sqlparse.format(query.lower(), strip_comments=True)\n    return bool(formatted_sql) and formatted_sql.split()[0] in prefixes\n"}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        renderers = [renderer for renderer in renderers\n                     if renderer.format == format]\n        if not renderers:\n            raise Http404\n        return renderers\n"}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    if value is None:\n        return ''\n    return '%s' % value\n"}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict):\n        return 'class=nested'\n    if isinstance(value, list) and any(isinstance(item, (list, dict)) for item in value):\n        return 'class=nested'\n    return ''\n"}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        try:\n            return pickle.loads(bstruct)\n        except Exception:\n            # this block should catch at least:\n            # ValueError, AttributeError, ImportError; but more to be safe\n            raise ValueError\n"}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        storage = self.setdefault('_f_' + queue, [])\n        if allow_duplicate or (msg not in storage):\n            storage.append(msg)\n"}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        storage = self.pop('_f_' + queue, [])\n        return storage\n"}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        return storage\n"}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        token = '0123456789012345678901234567890123456789'\n        self['_csrft_'] = token\n        return token\n"}
{"namespace": "pyramid.view.view_defaults", "completion": "    def wrap(wrapped):\n        wrapped.__view_defaults__ = settings\n        return wrapped\n\n    return wrap\n"}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s\n"}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    result = {}\n    for arg in args:\n        if '=' not in arg:\n            raise ValueError('Variable assignment %r invalid (no \"=\")' % arg)\n        name, value = arg.split('=', 1)\n        result[name] = value\n    return result\n"}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        path = request.path_info\n        # find all routes that match path, regardless of predicates\n        for route in mapper.get_routes():\n            match = route.match(path)\n            if match is not None:\n                info = {'match': match, 'route': route}\n                infos.append(info)\n        return infos\n"}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        server_name = server_name or 'main'\n        settings = loader.get_settings('server:' + server_name, global_conf)\n        if 'port' in settings:\n            return 'http://127.0.0.1:{port}'.format(**settings)\n"}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    chunks = name.split('_')\n    converted = [s.capitalize() for s in chunks]\n    if initial:\n        return ''.join(converted)\n    else:\n        return chunks[0].lower() + ''.join(converted[1:])\n"}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    assert isinstance(b, bytes)\n    b = bytearray(b)  # Used subset of its API is the same on Python 2 and 3.\n    for i in range(len(b) - 1, -1, -1):\n        if b[i] != 0xff:\n            b[i] += 1\n            return bytes(b[:i+1])\n    return None\n"}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    parent_dir = expanduser(dirname(path))\n    if not os.path.exists(parent_dir):\n        os.makedirs(parent_dir)\n"}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if os.path.exists(id_file_path):\n        last_24_hours = datetime.now() - timedelta(hours=24)\n        id_file_modified_time = datetime.fromtimestamp(os.path.getmtime(id_file_path))\n\n        return id_file_modified_time < last_24_hours\n    return False\n"}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    if not command:\n        return False\n\n    try:\n        # call command silentyly\n        with open(devnull, 'wb') as no_out:\n            subprocess.call(command, stdout=no_out, stderr=no_out)\n    except OSError:\n        return False\n    else:\n        return True\n"}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "    if not sql.strip():\n        return None, ''\n\n    parsed = sqlparse.parse(sql)[0]\n    flattened = list(parsed.flatten())\n    flattened = flattened[:len(flattened) - n_skip]\n\n    logical_operators = ('AND', 'OR', 'NOT', 'BETWEEN')\n\n    for t in reversed(flattened):\n        if t.value == '(' or (t.is_keyword and\n                              (t.value.upper() not in logical_operators)\n                             ):\n            # Find the location of token t in the original parsed statement\n            # We can't use parsed.token_index(t) because t may be a child token\n            # inside a TokenList, in which case token_index thows an error\n            # Minimal example:\n            #   p = sqlparse.parse('select * from foo where bar')\n            #   t = list(p.flatten())[-3]  # The \"Where\" token\n            #   p.token_index(t)  # Throws ValueError: not in list\n            idx = flattened.index(t)\n\n            # Combine the string values of all tokens in the original list\n            # up to and including the target keyword token t, to produce a\n            # query string with everything after the keyword token removed\n            text = ''.join(tok.value for tok in flattened[:idx + 1])\n            return t, text\n\n    return None, ''\n"}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s\n"}
{"namespace": "datasette.filters.where_filters", "completion": "    async def inner():\n        where_clauses = []\n        extra_wheres_for_ui = []\n        if \"_where\" in request.args:\n            if not await datasette.permission_allowed(\n                request.actor,\n                \"execute-sql\",\n                resource=database,\n                default=True,\n            ):\n                raise DatasetteError(\"_where= is not allowed\", status=403)\n            else:\n                where_clauses.extend(request.args.getlist(\"_where\"))\n                extra_wheres_for_ui = [\n                    {\n                        \"text\": text,\n                        \"remove_url\": path_with_removed_args(request, {\"_where\": text}),\n                    }\n                    for text in request.args.getlist(\"_where\")\n                ]\n\n        return FilterArguments(\n            where_clauses,\n            extra_context={\n                \"extra_wheres_for_ui\": extra_wheres_for_ui,\n            },\n        )\n\n    return inner\n"}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    path = path or request.path\n    if isinstance(args, dict):\n        args = args.items()\n    args_to_remove = {k for k, v in args if v is None}\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if key not in args_to_remove:\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string\n"}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    path = path or request.path\n    if isinstance(args, dict):\n        args = args.items()\n    keys_to_replace = {p[0] for p in args}\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if key not in keys_to_replace:\n            current.append((key, value))\n    current.extend([p for p in args if p[1] is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string\n"}
{"namespace": "datasette.utils.format_bytes", "completion": "    current = float(bytes)\n    for unit in (\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if current < 1024:\n            break\n        current = current / 1024\n    if unit == \"bytes\":\n        return f\"{int(current)} {unit}\"\n    else:\n        return f\"{current:.1f} {unit}\"\n"}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if allow is True:\n        return True\n    if allow is False:\n        return False\n    if actor is None and allow and allow.get(\"unauthenticated\") is True:\n        return True\n    if allow is None:\n        return True\n    actor = actor or {}\n    for key, values in allow.items():\n        if values == \"*\" and key in actor:\n            return True\n        if not isinstance(values, list):\n            values = [values]\n        actor_values = actor.get(key)\n        if actor_values is None:\n            continue\n        if not isinstance(actor_values, list):\n            actor_values = [actor_values]\n        actor_values = set(actor_values)\n        if actor_values.intersection(values):\n            return True\n    return False\n"}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        if list(config.keys()) == [\"$env\"]:\n            return environ.get(list(config.values())[0])\n        elif list(config.keys()) == [\"$file\"]:\n            return open(list(config.values())[0]).read()\n        else:\n            return {\n                key: resolve_env_secrets(value, environ)\n                for key, value in config.items()\n            }\n    elif isinstance(config, list):\n        return [resolve_env_secrets(value, environ) for value in config]\n    else:\n        return config\n"}
{"namespace": "datasette.utils.display_actor", "completion": "    for key in (\"display\", \"name\", \"username\", \"login\", \"id\"):\n        if actor.get(key):\n            return actor[key]\n    return str(actor)\n"}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    databases = dict([p for p in datasette.databases.items() if p[0] != \"_internal\"])\n    if len(databases) == 1:\n        db_name = next(iter(databases.keys()))\n        path = datasette.urls.database(db_name)\n        # Does this DB only have one table?\n        db = next(iter(databases.values()))\n        tables = await db.table_names()\n        if len(tables) == 1:\n            path = datasette.urls.table(db_name, tables[0])\n    else:\n        path = datasette.urls.instance()\n    return path\n"}
{"namespace": "datasette.utils.tilde_decode", "completion": "    temp = secrets.token_hex(16)\n    s = s.replace(\"%\", temp)\n    decoded = urllib.parse.unquote_plus(s.replace(\"~\", \"%\"))\n    return decoded.replace(temp, \"%\")\n"}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for regex, view in routes:\n        match = regex.match(path)\n        if match is not None:\n            return match, view\n    return None, None\n"}
{"namespace": "datasette.utils.truncate_url", "completion": "    if (not length) or (len(url) <= length):\n        return url\n    bits = url.rsplit(\".\", 1)\n    if len(bits) == 2 and 1 <= len(bits[1]) <= 4 and \"/\" not in bits[1]:\n        rest, ext = bits\n        return rest[: length - 1 - len(ext)] + \"\u2026.\" + ext\n    return url[: length - 1] + \"\u2026\"\n"}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    backend = getattr(request.registry, \"permission\", None)\n    # Permission backend not configured. Ignore.\n    if not backend:\n        return []\n\n    # Safety check when Kinto-Core is used without pyramid_multiauth.\n    if request.prefixed_userid:\n        userid = request.prefixed_userid\n\n    # Query the permission backend only once per request (e.g. batch).\n    reify_key = userid + \"_principals\"\n    if reify_key not in request.bound_data:\n        principals = backend.get_user_principals(userid)\n        request.bound_data[reify_key] = principals\n\n    return request.bound_data[reify_key]\n"}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        kw.setdefault(\"bytes_mode\", rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)\n"}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)\n"}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()\n"}
{"namespace": "kinto.core.utils.current_service", "completion": "    if request.matched_route:\n        services = request.registry.cornice_services\n        pattern = request.matched_route.pattern\n        try:\n            service = services[pattern]\n        except KeyError:\n            return None\n        else:\n            return service\n"}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if Authenticated not in principals:\n        return principals\n\n    # Remove unprefixed user id on effective_principals to avoid conflicts.\n    # (it is added via Pyramid Authn policy effective principals)\n    prefix, userid = request.prefixed_userid.split(\":\", 1)\n    principals = [p for p in principals if p != userid]\n\n    if request.prefixed_userid not in principals:\n        principals = [request.prefixed_userid] + principals\n\n    return principals\n"}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n    settings = request.registry.settings\n    if not settings.get(\"account_validation\", False):\n        return\n\n    for impacted_object in event.impacted_objects:\n        account = impacted_object[\"new\"]\n        user_email = account[\"id\"]\n        activation_key = get_cached_validation_key(user_email, request.registry)\n        if activation_key is None:\n            continue\n\n        # Send an email to the user with the link to activate their account.\n        Emailer(request, account).send_activation(activation_key)\n"}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    pwd_str = password.encode(encoding=\"utf-8\")\n    hashed = bcrypt.hashpw(pwd_str, bcrypt.gensalt())\n    return hashed.decode(encoding=\"utf-8\")\n"}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    path = object_uri.rsplit(\"/\", 2)\n    # len(path) == 1: no '/', probably a broken URL?\n    # len(path) == 2: one '/', doesn't conform to our URL scheme\n    if len(path) < 3:\n        return \"\"\n\n    return path[0]\n"}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def decorate(fn):\n        _registry[name] = fn\n        return fn\n\n    return decorate\n"}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if match:\n        return dest_namespace.replace(\"*\", match.group(1))\n    return None\n"}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    db_name, coll_name = namespace.split(\".\", 1)\n    # A database name cannot contain a '.' character\n    db_regex = re.escape(db_name).replace(r\"\\*\", \"([^.]*)\")\n    # But a collection name can.\n    coll_regex = re.escape(coll_name).replace(r\"\\*\", \"(.*)\")\n    return re.compile(r\"\\A\" + db_regex + r\"\\.\" + coll_regex + r\"\\Z\")\n"}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    seconds = val >> 32\n    increment = val & 0xFFFFFFFF\n\n    return Timestamp(seconds, increment)\n"}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def flatten(doc, path):\n            top_level = len(path) == 0\n            if not top_level:\n                path_string = \".\".join(path)\n            for k in doc:\n                v = doc[k]\n                if isinstance(v, dict):\n                    path.append(k)\n                    for inner_k, inner_v in flatten(v, path):\n                        yield inner_k, inner_v\n                    path.pop()\n                else:\n                    transformed = self.transform_element(k, v)\n                    for new_k, new_v in transformed:\n                        if top_level:\n                            yield new_k, new_v\n                        else:\n                            yield \"%s.%s\" % (path_string, new_k), new_v\n\n        return dict(flatten(document, []))\n"}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    directory = os.path.dirname(path)\n    if not os.path.isdir(directory):\n        raise ValueError('No directory {}'.format(directory))\n\n    if not os.path.exists(path):\n        file_fd = open(path, mode='x+b', buffering=0)\n    else:\n        file_fd = open(path, mode='r+b', buffering=0)\n\n    if platform.system() == 'Windows':\n        # Opening a directory is not possible on Windows, but that is not\n        # a problem since Windows does not need to fsync the directory in\n        # order to persist metadata\n        dir_fd = None\n    else:\n        dir_fd = os.open(directory, os.O_RDONLY)\n\n    return file_fd, dir_fd\n"}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        class ReadTransaction:\n\n            def __enter__(self2):\n                self._lock.reader_lock.acquire()\n\n            def __exit__(self2, exc_type, exc_val, exc_tb):\n                self._lock.reader_lock.release()\n\n        return ReadTransaction()\n"}
{"namespace": "bplustree.utils.pairwise", "completion": "    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)\n"}
{"namespace": "bplustree.utils.iter_slice", "completion": "    start = 0\n    stop = start + n\n    final_offset = len(iterable)\n\n    while True:\n        if start >= final_offset:\n            break\n\n        rv = iterable[start:stop]\n        start = stop\n        stop = start + n\n        yield rv, start >= final_offset\n"}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        rv = obj.encode(encoding='utf-8')\n        assert len(rv) <= key_size\n        return rv\n"}
{"namespace": "psd_tools.utils.pack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.pack(fmt, *args)\n"}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)\n"}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "    height, width = pattern.data.rectangle[2], pattern.data.rectangle[3]\n    return np.stack([\n        _parse_array(c.get_data(), c.pixel_depth)\n        for c in pattern.data.channels if c.is_written\n    ],\n                    axis=1).reshape((height, width, -1))\n"}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    field_size_limit = sys.maxsize\n\n    while True:\n        try:\n            csv.field_size_limit(field_size_limit)\n            break\n        except OverflowError:\n            field_size_limit = int(field_size_limit / 10)\n"}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "    assert isinstance(column_type, str)\n    column_type = column_type.upper().strip()\n    if column_type == \"\":\n        return str  # We differ from spec, which says it should be BLOB\n    if \"INT\" in column_type:\n        return int\n    if \"CHAR\" in column_type or \"CLOB\" in column_type or \"TEXT\" in column_type:\n        return str\n    if \"BLOB\" in column_type:\n        return bytes\n    if \"REAL\" in column_type or \"FLOA\" in column_type or \"DOUB\" in column_type:\n        return float\n    # Default is 'NUMERIC', which we currently also treat as float\n    return float\n"}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    to_fix = [\n        k\n        for k in doc\n        if isinstance(doc[k], dict)\n        and doc[k].get(\"$base64\") is True\n        and \"encoded\" in doc[k]\n    ]\n    if not to_fix:\n        return doc\n    return dict(doc, **{k: base64.b64decode(doc[k][\"encoded\"]) for k in to_fix})\n"}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    iterator = iter(sequence)\n    for item in iterator:\n        yield itertools.chain([item], itertools.islice(iterator, size - 1))\n"}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    to_hash = record\n    if keys is not None:\n        to_hash = {key: record[key] for key in keys}\n    return hashlib.sha1(\n        json.dumps(to_hash, separators=(\",\", \":\"), sort_keys=True, default=repr).encode(\n            \"utf8\"\n        )\n    ).hexdigest()\n"}
{"namespace": "arctic.decorators._get_host", "completion": "    ret = {}\n    if store:\n        try:\n            if isinstance(store, (list, tuple)):\n                store = store[0]\n            ret['l'] = store._arctic_lib.get_name()\n            ret['mnodes'] = [\"{}:{}\".format(h, p) for h, p in store._collection.database.client.nodes]\n            ret['mhost'] = \"{}\".format(store._arctic_lib.arctic.mongo_host)\n        except Exception:\n            # Sometimes get_name(), for example, fails if we're not connected to MongoDB.\n            pass\n    return ret\n"}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    log_all_exceptions = 'arctic' in f.__module__ if f.__module__ else False\n\n    @wraps(f)\n    def f_retry(*args, **kwargs):\n        global _retry_count, _in_retry\n        top_level = not _in_retry\n        _in_retry = True\n        try:\n            while True:\n                try:\n                    return f(*args, **kwargs)\n                except (DuplicateKeyError, ServerSelectionTimeoutError, BulkWriteError) as e:\n                    # Re-raise errors that won't go away.\n                    _handle_error(f, e, _retry_count, **_get_host(args))\n                    raise\n                except (OperationFailure, AutoReconnect) as e:\n                    _retry_count += 1\n                    _handle_error(f, e, _retry_count, **_get_host(args))\n                except Exception as e:\n                    if log_all_exceptions:\n                        _log_exception(f.__name__, e, _retry_count, **_get_host(args))\n                    raise\n        finally:\n            if top_level:\n                _in_retry = False\n                _retry_count = 0\n    return f_retry\n"}
{"namespace": "arctic._util.are_equals", "completion": "    try:\n        if isinstance(o1, DataFrame):\n            assert_frame_equal(o1, o2, kwargs)\n            return True\n        return o1 == o2\n    except Exception:\n        return False\n"}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook\n"}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook\n"}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook\n"}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    if len(array_2d) == 0:\n        return np.empty(0, dtype=object)\n\n    rtn = np.empty(len(slices) + 1, dtype=object)\n    start = 0\n    for i, s in enumerate(slices):\n        rtn[i] = array_2d[start:s]\n        start = s\n    rtn[-1] = array_2d[start:]\n    return rtn\n"}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "    sha = hashlib.sha1()\n    sha.update(symbol.encode('ascii'))\n    for k in sorted(iter(doc.keys()), reverse=True):\n        v = doc[k]\n        if isinstance(v, bytes):\n            sha.update(doc[k])\n        else:\n            sha.update(str(doc[k]).encode('ascii'))\n    return Binary(sha.digest())\n"}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return \"VersionedItem(symbol=%s,library=%s,data=%s,version=%s,metadata=%s,host=%s)\" % \\\n            (self.symbol, self.library, type(self.data), self.version, self.metadata, self.host)\n"}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n        if string.startswith('['):\n            return np.dtype(eval(string), metadata=metadata)\n        return np.dtype(string, metadata=metadata)\n"}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    if not set(dtype1.names).issuperset(set(dtype2.names)):\n        raise Exception(\"Removing columns from dtype not handled\")\n\n    def _promote(type1, type2):\n        if type2 is None:\n            return type1\n        if type1.shape is not None:\n            if not type1.shape == type2.shape:\n                raise Exception(\"We do not handle changes to dtypes that have shape\")\n            return np.promote_types(type1.base, type2.base), type1.shape\n        return np.promote_types(type1, type2)\n    return np.dtype([(n, _promote(dtype1.fields[n][0], dtype2.fields.get(n, (None,))[0])) for n in dtype1.names])\n"}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        if isinstance(data, DataFrame):\n            return DataFrame()\n        else:\n            return Series()\n"}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if 'date' in df.index.names:\n            dates = df.index.get_level_values('date')\n            if not df.index.is_monotonic_increasing:\n                df = df.sort_index()\n        elif 'date' in df.columns:\n            dates = pd.DatetimeIndex(df.date)\n            if not dates.is_monotonic_increasing:\n                # providing support for pandas 0.16.2 to 0.20.x\n                # neither sort method exists in both\n                try:\n                    df = df.sort_values('date')\n                except AttributeError:\n                    df = df.sort(columns='date')\n                dates = pd.DatetimeIndex(df.date)\n        else:\n            raise Exception(\"Data must be datetime indexed or have a column named 'date'\")\n\n        period_obj = dates.to_period(chunk_size)\n        period_obj_reduced = period_obj.drop_duplicates()\n        count = 0\n        for _, g in df.groupby(period_obj._data):\n            start = period_obj_reduced[count].start_time.to_pydatetime(warn=False)\n            end = period_obj_reduced[count].end_time.to_pydatetime(warn=False)\n            count += 1\n            if func:\n                yield start, end, chunk_size, func(g)\n            else:\n                yield start, end, chunk_size, g\n"}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if 'date' in data.index.names:\n            return data[(data.index.get_level_values('date') < range_obj.start) | (data.index.get_level_values('date') > range_obj.end)]\n        elif 'date' in data.columns:\n            return data[(data.date < range_obj.start) | (data.date > range_obj.end)]\n        else:\n            return data\n"}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if not proxy_config.get(\"hostname\"):\n        return None\n\n    scheme = proxy_config.get(\"scheme\") or \"http\"\n    username = proxy_config.get(\"username\")\n    password = proxy_config.get(\"password\")\n    hostname = proxy_config[\"hostname\"]\n    port = proxy_config.get(\"port\")\n    if not port or port < 0:\n        port = 80\n\n    if username and password and auth:\n        return f\"{scheme}://{username}:{password}@{hostname}:{port}\"\n    else:\n        return f\"{scheme}://{hostname}:{port}\"\n"}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[start:end]\n        elif 'date' in data.columns:\n            if start and end:\n                return data[(data.date >= start) & (data.date <= end)]\n            elif start:\n                return data[(data.date >= start)]\n            elif end:\n                return data[(data.date <= end)]\n            else:\n                return data\n        else:\n            return data\n"}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and not value:\n        raise ValueError(\"must be set.\")\n"}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if choices is not None and value not in choices:\n        names = \", \".join(repr(c) for c in choices)\n        raise ValueError(f\"must be one of {names}, not {value}.\")\n"}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if minimum is not None and value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")\n"}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")\n"}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if not choices:\n        return None\n\n    name = name.lower()\n    candidates = [(_levenshtein(name, c), c) for c in choices]\n    candidates.sort()\n\n    if candidates[0][0] <= 3:\n        return candidates[0][1]\n    return None\n"}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n"}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n"}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        return str(value)\n"}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "        if value is True:\n            return \"true\"\n        elif value in (False, None):\n            return \"false\"\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")\n"}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    df_str = data.select_dtypes(include=['object'])\n    df_num = data.select_dtypes(exclude=['object'])\n\n    for colname in df_str.columns:\n        df_num = df_num.join(pd.get_dummies(data[colname], prefix=colname))\n\n    plot_data = df_num.values\n\n    labels=list(df_num.columns.values)\n\n    if return_labels:\n        return plot_data,labels\n    else:\n        return plot_data\n"}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert type(x) is list, \"Input data to center must be list\"\n    x_stacked = np.vstack(x)\n    return [i - np.mean(x_stacked, 0) for i in x]\n"}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    return [val_set.index(val) for val in vals]\n"}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get palette from seaborn\n    palette = np.array(sns.color_palette(cmap, res))\n    ranks = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return [tuple(i) for i in palette[ranks, :]]\n"}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    return list(np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1)\n"}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    x=np.arange(0, len(arr), 1)\n    xx=np.arange(0, len(arr)-1, 1/interp_val)\n    q=pchip(x,arr)\n    return q(xx)\n"}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    args_list = []\n    for i,item in enumerate(x):\n        tmp = []\n        for ii, arg in enumerate(args):\n            if isinstance(arg, (tuple, list)):\n                if len(arg) == len(x):\n                    tmp.append(arg[i])\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp.append(arg)\n        args_list.append(tuple(tmp))\n    return args_list\n"}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    kwargs_list = []\n    for i,item in enumerate(x):\n        tmp = {}\n        for kwarg in kwargs:\n            if isinstance(kwargs[kwarg], (tuple, list)):\n                if len(kwargs[kwarg]) == len(x):\n                    tmp[kwarg]=kwargs[kwarg][i]\n                else:\n                    tmp[kwarg] = None\n            else:\n                tmp[kwarg]=kwargs[kwarg]\n        kwargs_list.append(tmp)\n    return kwargs_list\n"}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    TERM = environ.get('TERM', '').lower()\n    # FIXME: COLORTERM may not be accepted by sshd\n    COLORTERM = environ.get('COLORTERM', '').lower()\n    if 'truecolor' in TERM or 'truecolor' in COLORTERM:\n        return 'truecolor'\n    elif '256' in TERM or '256' in COLORTERM:\n        return '256fgbg'\n    return 'nocolor'\n"}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    if val is None:\n        return val\n    val = int(val)\n    if val <= 0:\n        raise argparse.ArgumentTypeError('Minimum cpu_pool_size is 1')\n    return val\n"}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    pixels = [\n        px[sx, sy]\n        for sy in range(y, y + cell_height)\n        for sx in range(x, x + cell_width)\n    ]\n    return [round(n) for n in map(mean, zip(*pixels))]\n"}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.strip().startswith('https://tenor.com/view/'):\n        gif_id = input_source.rsplit('-', 1)[-1]\n        if gif_id.isdigit():\n            input_source = gif_id\n        else:\n            raise Exception('Bad GIF URL.')\n\n    is_url = input_source.startswith(('http://', 'https://'))\n\n    if not os.path.exists(input_source) and not is_url:\n        # get from Tenor GIF API\n        params = {'key': api_key}\n        if input_source.isdigit():\n            endpoint = 'gifs'\n            params.update({'ids': input_source})\n        elif input_source == '':\n            endpoint = 'trending'\n            params.update({'limit': 1})\n        else:\n            endpoint = 'search'\n            params.update({'limit': 1, 'q': input_source})\n\n        resp = requests.get(\n            'https://api.tenor.com/v1/{}'.format(endpoint),\n            params=params\n        )\n\n        try:\n            resp_json = resp.json()\n        except JSONDecodeError:\n            raise Exception('A server error occurred.')\n\n        if 'error' in resp_json:\n            raise Exception('An error occurred: {}'.format(resp_json['error']))\n\n        results = resp_json.get('results')\n\n        if not results:\n            raise Exception('Could not find GIF.')\n\n        input_source = results[0]['media'][0]['mp4']['url']\n    return input_source\n"}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    categories = list(sorted(set(hue), key=list(hue).index))\n    x_stacked = np.vstack(x)\n    x_reshaped = [[] for _ in categories]\n    labels_reshaped = [[] for _ in categories]\n    if labels is None:\n        labels = [None]*len(hue)\n    for idx, (point, label) in enumerate(zip(hue, labels)):\n        x_reshaped[categories.index(point)].append(x_stacked[idx])\n        labels_reshaped[categories.index(point)].append(labels[idx])\n    return [np.vstack(i) for i in x_reshaped], labels_reshaped\n"}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    if not hasattr(note, \"name\"):\n        return False\n\n    # Lower the case of the name\n    result = note.name[0].lower()\n\n    # Convert #'s and b's to 'is' and 'es' suffixes\n    for accidental in note.name[1:]:\n        if accidental == \"#\":\n            result += \"is\"\n        elif accidental == \"b\":\n            result += \"es\"\n\n    # Place ' and , for octaves\n    if process_octaves:\n        oct = note.octave\n        if oct >= 4:\n            while oct > 3:\n                result += \"'\"\n                oct -= 1\n        elif oct < 3:\n            while oct < 3:\n                result += \",\"\n                oct += 1\n    if standalone:\n        return \"{ %s }\" % result\n    else:\n        return result\n"}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    names = [x.to_shorthand() for x in tuning.tuning]\n    basesize = len(max(names)) + 3\n    barsize = ((width - basesize) - 2) - 1\n\n    # x * 4 + 0.5x - barsize = 0 4.5x = barsize x = barsize / 4.5\n    return max(0, int(barsize / 4.5))\n"}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] != \"b\":\n        return note + \"#\"\n    else:\n        return note[:-1]\n"}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "    if duration == 0:\n        return False\n    elif duration == 1:\n        return True\n    else:\n        r = duration\n        while r != 1:\n            if r % 2 == 1:\n                return False\n            r /= 2\n        return True\n"}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]\n"}
{"namespace": "mingus.core.intervals.invert", "completion": "    interval.reverse()\n    res = list(interval)\n    interval.reverse()\n    return res\n"}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    acc = 0\n    roman_numeral = \"\"\n    suffix = \"\"\n    i = 0\n    for c in progression:\n        if c == \"#\":\n            acc += 1\n        elif c == \"b\":\n            acc -= 1\n        elif c.upper() == \"I\" or c.upper() == \"V\":\n            roman_numeral += c.upper()\n        else:\n            break\n        i += 1\n    suffix = progression[i:]\n    return (roman_numeral, acc, suffix)\n"}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    endian = {'big': '>', 'little': '<'}[byteorder]\n    chars = struct.unpack(endian + ('B' * len(bytes)), bytes)\n    if byteorder == 'big':\n        chars = chars[::-1]\n    return sum(int(char) * 256 ** i for (i, char) in enumerate(chars))\n"}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{%s}}' % key, value)\n    return string\n"}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    match = re.match(r'\\[pid\\s+\\d+\\]\\s*', line)\n    if match:\n        return line[len(match.group()):]\n    return line\n"}
{"namespace": "fs.path.abspath", "completion": "    if not path.startswith(\"/\"):\n        return \"/\" + path\n    return path\n"}
{"namespace": "fs.path.combine", "completion": "    if not path1:\n        return path2.lstrip()\n    return \"{}/{}\".format(path1.rstrip(\"/\"), path2.lstrip(\"/\"))\n"}
{"namespace": "fs.path.split", "completion": "    if \"/\" not in path:\n        return (\"\", path)\n    split = path.rsplit(\"/\", 1)\n    return (split[0] or \"/\", split[1])\n"}
{"namespace": "fs.path.isparent", "completion": "    bits1 = path1.split(\"/\")\n    bits2 = path2.split(\"/\")\n    while bits1 and bits1[-1] == \"\":\n        bits1.pop()\n    if len(bits1) > len(bits2):\n        return False\n    for (bit1, bit2) in zip(bits1, bits2):\n        if bit1 != bit2:\n            return False\n    return True\n"}
{"namespace": "fs.path.forcedir", "completion": "    if not path.endswith(\"/\"):\n        return path + \"/\"\n    return path\n"}
{"namespace": "fs.wildcard.match_any", "completion": "    if not patterns:\n        return True\n    return any(match(pattern, name) for pattern in patterns)\n"}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)\n"}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "    if not val or val.lower() in {'false', '0'}:\n        return False\n    elif val.lower() in {'true', '1'}:\n        return True\n    else:\n        raise ValueError('Invalid boolean environment variable: %s' % val)\n"}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    env = os.getenv('WALE_LOG_DESTINATION', 'stderr,syslog')\n    return env.split(\",\")\n"}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        timeEntry = datetime.datetime.utcnow().strftime(\n            \"time=%Y-%m-%dT%H:%M:%S.%f-00\")\n        pidEntry = \"pid=\" + str(os.getpid())\n\n        rest = sorted('='.join([str(k), str(v)])\n                      for (k, v) in list(d.items()))\n\n        return ' '.join([timeEntry, pidEntry] + rest)\n"}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    touched_directories = set()\n\n    mode = os.O_RDONLY\n\n    # Windows\n    if hasattr(os, 'O_BINARY'):\n        mode |= os.O_BINARY\n\n    for filename in filenames:\n        fd = os.open(filename, mode)\n        os.fsync(fd)\n        os.close(fd)\n        touched_directories.add(os.path.dirname(filename))\n\n    # Some OSes also require us to fsync the directory where we've\n    # created files or subdirectories.\n    if hasattr(os, 'O_DIRECTORY'):\n        for dirname in touched_directories:\n            fd = os.open(dirname, os.O_RDONLY | os.O_DIRECTORY)\n            os.fsync(fd)\n            os.close(fd)\n"}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = \"/\" + prefix\n        file_paths = [os.path.join(root, f)\n                      for root, dirs, files in os.walk(path) for f in files]\n        # convert to an array of Keys\n        return [FileKey(bucket=self, name=f) for f in file_paths]\n"}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    parts = list(path_parts)\n    parts[0:-1] = [part.rstrip(\"/\") for part in parts[0:-1]]\n    return \"/\".join(parts)\n"}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command\n"}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return value\n"}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n"}
{"namespace": "mrjob.compat.map_version", "completion": "    if version is None:\n        raise TypeError\n\n    if not version_map:\n        raise ValueError\n\n    if isinstance(version_map, dict):\n        version_map = sorted((LooseVersion(k), v)\n                             for k, v in version_map.items())\n\n    req_version = LooseVersion(version)\n\n    for min_version, value in reversed(version_map):\n        if req_version >= min_version:\n            return value\n    else:\n        return version_map[0][1]\n"}
{"namespace": "mrjob.conf.combine_values", "completion": "    for v in reversed(values):\n        if v is not None:\n            return v\n    else:\n        return None\n"}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        key_value = line.split(b'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)\n"}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        return b'\\t'.join(\n            x.encode('utf_8') for x in (key, value) if x is not None)\n"}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n\n        key_value = line.split(u'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)\n"}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            return (None, line.decode('utf_8'))\n        except UnicodeDecodeError:\n            return (None, line.decode('latin_1'))\n"}
{"namespace": "mrjob.util.file_ext", "completion": "    stripped_name = filename.lstrip('.')\n    dot_index = stripped_name.find('.')\n\n    if dot_index == -1:\n        return ''\n    return stripped_name[dot_index:]\n"}
{"namespace": "mrjob.util.cmd_line", "completion": "    args = [str(x) for x in args]\n    return ' '.join(pipes.quote(x) for x in args)\n"}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)\n"}
{"namespace": "mrjob.util.save_sys_std", "completion": "    stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr\n\n    try:\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        yield\n\n        # at this point, sys.stdout/stderr may have been patched. Don't\n        # raise an exception if flush() fails\n        try:\n            sys.stdout.flush()\n        except:\n            pass\n\n        try:\n            sys.stderr.flush()\n        except:\n            pass\n    finally:\n        sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr\n"}
{"namespace": "mrjob.util.unarchive", "completion": "    if tarfile.is_tarfile(archive_path):\n        with tarfile.open(archive_path, 'r') as archive:\n            archive.extractall(dest)\n    elif is_zipfile(archive_path):\n        with ZipFile(archive_path, 'r') as archive:\n            for name in archive.namelist():\n                # the zip spec specifies that front slashes are always\n                # used as directory separators\n                dest_path = os.path.join(dest, *name.split('/'))\n\n                # now, split out any dirname and filename and create\n                # one and/or the other\n                dirname, filename = os.path.split(dest_path)\n                if dirname and not os.path.exists(dirname):\n                    os.makedirs(dirname)\n                if filename:\n                    with open(dest_path, 'wb') as dest_file:\n                        dest_file.write(archive.read(name))\n    else:\n        raise IOError('Unknown archive type: %s' % (archive_path,))\n"}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n\n    for item in items:\n        if item in seen:\n            continue\n        else:\n            yield item\n            seen.add(item)\n"}
{"namespace": "mrjob.parse.urlparse", "completion": "    (scheme, netloc, path, params, query, fragment) = (\n        urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs))\n\n    if allow_fragments and '#' in path and not fragment:\n        path, fragment = path.split('#', 1)\n\n    return ParseResult(scheme, netloc, path, params, query, fragment)\n"}
{"namespace": "mrjob.util.which", "completion": "    if hasattr(shutil, 'which'):\n        # added in Python 3.3\n        return shutil.which(cmd, path=path)\n    elif path is None and os.environ.get('PATH') is None:\n        # find_executable() errors if neither path nor $PATH is set\n        return None\n    else:\n        return find_executable(cmd, path=path)\n"}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if rhostport is None or len(rhostport) == 0:\n        return None, None, None, None\n    port = None\n    username = None\n    password = None\n    host = rhostport\n\n    if \"@\" in host:\n        # split username (and possible password) from the host[:port]\n        username, host = host.rsplit(\"@\", 1)\n        # Fix #410 bad username error detect\n        if \":\" in username:\n            # this will even allow for the username to be empty\n            username, password = username.split(\":\")\n\n    if \":\" in host:\n        # IPv6 address and/or got a port specified\n\n        # If it is an IPv6 address with port specification,\n        # then it will look like: [::1]:22\n\n        try:\n            # try to parse host as an IP address,\n            # if that works it is an IPv6 address\n            host = str(ipaddress.ip_address(host))\n        except ValueError:\n            # if that fails parse as URL to get the port\n            parsed = urlparse('//{}'.format(host))\n            try:\n                host = str(ipaddress.ip_address(parsed.hostname))\n            except ValueError:\n                # else if both fails, we have a hostname with port\n                host = parsed.hostname\n            port = parsed.port\n\n    if password is None or len(password) == 0:\n        password = None\n\n    return username, password, port, host\n"}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    if not str_dict:\n        return False\n    value = str(value)\n    try:\n        # + 3 for key right quote, one for colon and one for space\n        key_index = str_dict.index(key) + len(key) + 3\n    except ValueError:\n        return False\n    try:\n        comma_index = str_dict.index(',', key_index)\n    except ValueError:\n        # last value in dict\n        comma_index = str_dict.index('}', key_index)\n    return str(value) == str_dict[key_index:comma_index].strip('\"\\'')\n"}
{"namespace": "flower.utils.abs_path", "completion": "    path = os.path.expanduser(path)\n    if not os.path.isabs(path):\n        cwd = os.environ.get('PWD') or os.getcwd()\n        path = os.path.join(cwd, path)\n    return path\n"}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    if val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    raise ValueError(f\"invalid truth value {val!r}\")\n"}
{"namespace": "sshuttle.methods.get_method", "completion": "    module = importlib.import_module(\"sshuttle.methods.%s\" % method_name)\n    return module.Method(method_name)\n"}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    with open(os.path.join(os.path.dirname(__file__), 'known-iam-actions.txt'), encoding=\"UTF-8\") as iam_file:\n        return {line.rstrip('\\n') for line in iam_file.readlines()}\n"}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    parsed_records = [_parse_record(record) for record in json_records]\n    return [r for r in parsed_records if r is not None]\n"}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b''\n        is_negative = (v < 0)\n        if is_negative:\n            v = -v\n        ba = bytearray()\n        while v >= 256:\n            ba.append(v & 0xff)\n            v >>= 8\n        ba.append(v & 0xff)\n        if ba[-1] >= 128:\n            ba.append(0x80 if is_negative else 0)\n        elif is_negative:\n            ba[-1] |= 0x80\n        return bytes(ba)\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    stack.pop()\n    stack.pop()\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    stack.append(stack[-2])\n    stack.append(stack[-2])\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n"}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    delta = to_date.astimezone(pytz.utc) - from_date.astimezone(pytz.utc)\n\n    days = [to_date - datetime.timedelta(days=delta_days) for delta_days in range(delta.days + 1)]\n    if org_ids:\n        return [_s3_key_prefix_for_org_trails(prefix, day, org_id, account_id, region)\n                for org_id in org_ids\n                for account_id in account_ids\n                for day in days\n                for region in regions]\n\n    return [_s3_key_prefix(prefix, day, account_id, region)\n            for account_id in account_ids\n            for day in days\n            for region in regions]\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-4])\n    stack.append(stack[-4])\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack[-1]:\n        stack.append(stack[-1])\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    v = stack.pop()\n    stack.pop()\n    stack.append(v)\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1)\n    stack.append(v2)\n    stack.append(v1)\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v2 + v1)\n"}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    r, s = sig\n    return ((s * k - signed_value) * generator.inverse(r)) % generator.order()\n"}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n    if r1 != r2:\n        raise ValueError(\"r values of signature do not match\")\n    k = (r2 * val1 - r1 * val2) * generator.inverse(r2 * s1 - r1 * s2)\n    return k % generator.order()\n"}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer()\n    streamer.register_array_count_parse(parse_satoshi_int)\n    streamer.register_functions(parsing_functions)\n    return streamer\n"}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    if path_range == '':\n        yield ''\n        return\n\n    def range_iterator(the_range):\n        for r in the_range.split(\",\"):\n            is_hardened = r[-1] in hardening_chars\n            hardened_char = hardening_chars[-1] if is_hardened else ''\n            if is_hardened:\n                r = r[:-1]\n            if '-' in r:\n                low, high = [int(x) for x in r.split(\"-\", 1)]\n                for t in range(low, high+1):\n                    yield \"%d%s\" % (t, hardened_char)\n            else:\n                yield \"%s%s\" % (r, hardened_char)\n\n    components = path_range.split(\"/\")\n    iterators = [range_iterator(c) for c in components]\n    for v in itertools.product(*iterators):\n        yield '/'.join(v)\n"}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    if os.path.splitext(path)[1] == '.py':\n        return True\n    return False\n"}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    try:\n        return binascii.unhexlify(h.encode(\"ascii\"))\n    except Exception:\n        raise ValueError(\"h2b failed on %s\" % h)\n"}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    average = 0\n\n    for key, neighbors in graph.items():\n        average += len([n for n in neighbors if n])\n    average /= float(len(graph.items()))\n\n    return average\n"}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    if k > n:\n        return 0\n    if k == 0:\n        return 1\n\n    r = 1\n    for d in range(1, k + 1):\n        r *= n\n        r /= d\n        n -= 1\n\n    return r\n"}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    password_chars = {}\n    for char in list(password):\n        password_chars[char] = True\n\n    subtable = {}\n    for letter, subs in table.items():\n        relevant_subs = [sub for sub in subs if sub in password_chars]\n        if len(relevant_subs) > 0:\n            subtable[letter] = relevant_subs\n\n    return subtable\n"}
{"namespace": "zxcvbn.matching.translate", "completion": "    chars = []\n    for char in list(string):\n        if chr_map.get(char, False):\n            chars.append(chr_map[char])\n        else:\n            chars.append(char)\n\n    return ''.join(chars)\n"}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    net = db.GetNet(obj)\n    results.append((obj, net))\n  return results\n"}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    port = db.GetService(svc)\n    results.append((svc, port))\n  return results\n"}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  token = options.token\n  results = []\n  for ip in options.ip:\n    rval = db.GetIpParents(ip)\n    if token in rval:\n      results = '%s is in %s' % (ip, token)\n    else:\n      results = '%s is _not_ in %s' % (ip, token)\n  return results\n"}
{"namespace": "tools.cgrep.get_services", "completion": "  results = []\n  port, protocol = options.port\n  # swap values if they were passed in wrong order\n  if port.isalpha() and protocol.isdigit():\n    port, protocol = protocol, port\n  results = db.GetPortParents(port, protocol)\n  return port, protocol, results\n"}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8', errors='strict')\n\n    return len(value).to_bytes(4, 'big') + value\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    seq1_counts_ls = copy.deepcopy(seq1_counts)\n    seq2_counts_ls = copy.deepcopy(seq2_counts)\n\n    cmds: List[str] = list(seq1_counts_ls.keys()) + [unk_token]\n    for cmd1 in cmds:\n        for cmd2 in cmds:\n            if cmd1 != end_token and cmd2 != start_token:\n                seq1_counts_ls[cmd1] += 1\n                seq2_counts_ls[cmd1][cmd2] += 1\n                seq1_counts_ls[cmd2] += 1\n\n    return seq1_counts_ls, seq2_counts_ls\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    params: List[str] = list(param_counts.keys()) + [unk_token]\n    for cmd in cmds:\n        for param in params:\n            if param in cmd_param_counts_ls[cmd] or param == unk_token:\n                param_counts_ls[param] += 1\n                cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts_ls.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls\n"}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    if not 0 <= delta <= 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n\n    if not allow_zero and epsilon + delta == 0:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n"}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if secure:\n        if isinstance(seed, secrets.SystemRandom):\n            return seed\n\n        if seed is None or seed is np.random.mtrand._rand:  # pylint: disable=protected-access\n            return secrets.SystemRandom()\n    elif isinstance(seed, secrets.SystemRandom):\n        raise ValueError(\"secrets.SystemRandom instance cannot be passed when secure is False.\")\n\n    return skl_check_random_state(seed)\n"}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1) / clip\n    norms[norms < 1] = 1\n\n    return array / norms[:, np.newaxis]\n"}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        del y\n\n        self._fit(X)\n\n        return self.transform(X)\n"}
{"namespace": "discord.utils.get_slots", "completion": "    for mro in reversed(cls.__mro__):\n        try:\n            yield from mro.__slots__  # type: ignore\n        except AttributeError:\n            continue\n"}
{"namespace": "discord.utils.is_inside_class", "completion": "    if func.__qualname__ == func.__name__:\n        return False\n    (remaining, _, _) = func.__qualname__.rpartition('.')\n    return not remaining.endswith('<locals>')\n"}
{"namespace": "faker.utils.decorators.slugify", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs))\n\n    return wrapper\n"}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper\n"}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_unicode=True)\n\n    return wrapper\n"}
{"namespace": "faker.utils.loading.get_path", "completion": "    if getattr(sys, \"frozen\", False):\n        # frozen\n\n        if getattr(sys, \"_MEIPASS\", False):\n            # PyInstaller\n            lib_dir = Path(getattr(sys, \"_MEIPASS\"))\n        else:\n            # others\n            lib_dir = Path(sys.executable).parent / \"lib\"\n\n        path = lib_dir.joinpath(*module.__package__.split(\".\"))  # type: ignore\n    else:\n        # unfrozen\n        if module.__file__ is not None:\n            path = Path(module.__file__).parent\n        else:\n            raise RuntimeError(f\"Can't find path from module `{module}.\")\n    return str(path)\n"}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    def digits_of(n: float) -> List[int]:\n        return [int(d) for d in str(n)]\n\n    digits = digits_of(number)\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = 0\n    checksum += sum(odd_digits)\n    for d in even_digits:\n        checksum += sum(digits_of(d * 2))\n    return checksum % 10\n"}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    items = [odict.items() for odict in odicts]\n    return OrderedDictType(chain(*items))\n"}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    weights_for_check_digit = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    integer_characters = [\n        (ord(character) - 55) if isinstance(character, str) else character for character in characters\n    ]\n    check_digit = sum(weight * ch for weight, ch in zip(weights_for_check_digit, integer_characters)) % 10\n    return check_digit\n"}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights_for_check_digit = [8, 9, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 8):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n"}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8][-len(value) :]\n    check_sum = 0\n    for number, factor in zip(value, factors):\n        check_sum += int(number) * factor\n\n    return str((check_sum % 11) % 10)\n"}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 13):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n"}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    return check_digit\n"}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    digits = list(digits)\n    weights = 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2\n\n    dv = sum(w * d for w, d in zip(weights[1:], digits))\n    dv = (11 - dv) % 11\n    dv = 0 if dv >= 10 else dv\n    digits.append(dv)\n\n    dv2 = sum(w * d for w, d in zip(weights, digits))\n    dv2 = (11 - dv2) % 11\n    dv2 = 0 if dv2 >= 10 else dv2\n    digits.append(dv2)\n\n    return digits[-2:]\n"}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        if self.generator._is_seeded:\n            blob = [self.generator.random.randrange(256) for _ in range(length)]\n            return bytes(blob)\n\n        # Generator is unseeded anyway, just use urandom\n        return os.urandom(length)\n"}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            chars = \"\".join(self.random_letters(length=max_chars))\n        else:\n            assert max_chars >= min_chars, \"Maximum length must be greater than or equal to minimum length\"\n            chars = \"\".join(\n                self.random_letters(\n                    length=self.generator.random.randint(min_chars, max_chars),\n                ),\n            )\n\n        return prefix + chars + suffix\n"}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        new_read_only = {name: msg for name in names}\n        if getattr(self, \"_read_only\", False):\n            self._read_only.update(new_read_only)\n        else:\n            object.__setattr__(self, \"_read_only\", new_read_only)\n"}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        for name in names:\n            value = self.get(name)\n            if value:\n                return value\n        if not names:\n            return next(iter(self), {})\n"}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if config.assets_external_path:\n        prefix = config.assets_external_path\n    else:\n        prefix = config.requests_pathname_prefix\n    return \"/\".join(\n        [\n            # Only take the first part of the pathname\n            prefix.rstrip(\"/\"),\n            config.assets_url_path.lstrip(\"/\"),\n            path,\n        ]\n    )\n"}
{"namespace": "peewee.sort_models", "completion": "    models = set(models)\n    seen = set()\n    ordering = []\n    def dfs(model):\n        if model in models and model not in seen:\n            seen.add(model)\n            for foreign_key, rel_model in model._meta.refs.items():\n                # Do not depth-first search deferred foreign-keys as this can\n                # cause tables to be created in the incorrect order.\n                if not foreign_key.deferred:\n                    dfs(rel_model)\n            if model._meta.depends_on:\n                for dependency in model._meta.depends_on:\n                    dfs(dependency)\n            ordering.append(model)\n\n    names = lambda m: (m._meta.name, m._meta.table_name)\n    for m in sorted(models, key=names):\n        dfs(m)\n    return ordering\n"}
{"namespace": "dash._grouping.grouping_len", "completion": "    if isinstance(grouping, (tuple, list)):\n        return sum([grouping_len(group_el) for group_el in grouping])\n\n    if isinstance(grouping, dict):\n        return sum([grouping_len(group_el) for group_el in grouping.values()])\n\n    return 1\n"}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n"}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n"}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    pub_bytes = certificate.public_key().public_bytes(encoding=Encoding.DER, format=PublicFormat.SubjectPublicKeyInfo)\n    digest = sha256(pub_bytes).digest()\n    return digest\n"}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if all(titles[0] == title for title in titles[1:]):\n        return titles[0]\n    else:\n        title = \", \".join(titles[:-1])\n        return f\"<em>Comparing</em> {title} <em>and</em> {titles[-1]}\"\n"}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f} {unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f} Yi{suffix}\"\n"}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases and round(value, 3) == 0 and value > 0:\n        return \"< 0.1%\"\n    if edge_cases and round(value, 3) == 1 and value < 1:\n        return \"> 99.9%\"\n\n    return f\"{value*100:2.1f}%\"\n"}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    fmtted = f\"{{:.{precision}g}}\".format(value)\n    for v in [\"e+\", \"e-\"]:\n        if v in fmtted:\n            sign = \"-\" if v in \"e-\" else \"\"\n            fmtted = fmtted.replace(v, \" \u00d7 10<sup>\") + \"</sup>\"\n            fmtted = fmtted.replace(\"<sup>0\", \"<sup>\")\n            fmtted = fmtted.replace(\"<sup>\", f\"<sup>{sign}\")\n\n    return fmtted\n"}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    with np.printoptions(threshold=3, edgeitems=threshold):\n        return_value = str(value)\n\n    return return_value\n"}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value == 2:\n        return \"Strictly increasing\"\n    elif value == 1:\n        return \"Increasing\"\n    elif value == 0:\n        return \"Not monotonic\"\n    elif value == -1:\n        return \"Decreasing\"\n    elif value == -2:\n        return \"Strictly decreasing\"\n    else:\n        raise ValueError(\"Value should be integer ranging from -2 to 2.\")\n"}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "    def make_autopct(values: pd.Series) -> Callable:\n        def my_autopct(pct: float) -> str:\n            total = np.sum(values)\n            val = int(round(pct * total / 100.0))\n            return f\"{pct:.1f}%  ({val:d})\"\n\n        return my_autopct\n\n    _, ax = plt.subplots(figsize=(4, 4))\n    wedges, _, _ = plt.pie(\n        data,\n        autopct=make_autopct(data),\n        textprops={\"color\": \"w\"},\n        colors=colors,\n    )\n\n    legend = None\n    if not hide_legend:\n        legend = plt.legend(\n            wedges,\n            data.index.values,\n            fontsize=\"large\",\n            bbox_to_anchor=(0, 0),\n            loc=\"upper left\",\n        )\n\n    return ax, legend\n"}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    if sortby is None:\n        sortbykey = \"_index\"\n        df = dataframe[entity_column].copy().reset_index()\n        df.columns = [sortbykey, entity_column]\n\n    else:\n        if isinstance(sortby, str):\n            sortby = [sortby]\n        cols = [entity_column, *sortby]\n        df = dataframe[cols].copy()\n        sortbykey = sortby[0]\n\n    if df[sortbykey].dtype == \"O\":\n        try:\n            df[sortbykey] = pd.to_datetime(df[sortbykey])\n        except Exception as ex:\n            raise ValueError(\n                f\"column {sortbykey} dtype {df[sortbykey].dtype} is not supported.\"\n            ) from ex\n    nbins = np.min([50, df[sortbykey].nunique()])\n\n    df[\"__bins\"] = pd.cut(\n        df[sortbykey], bins=nbins, include_lowest=True, labels=range(nbins)\n    )\n\n    df = df.groupby([entity_column, \"__bins\"])[sortbykey].count()\n    df = df.reset_index().pivot_table(entity_column, \"__bins\", sortbykey).T\n\n    if selected_entities:\n        df = df[selected_entities].T\n    else:\n        df = df.T[:max_entities]\n\n    return df\n"}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    _, ax = plt.subplots(figsize=figsize)\n    cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\n        \"report\", [\"white\", color], N=64\n    )\n    pc = ax.pcolormesh(df, edgecolors=ax.get_facecolor(), linewidth=0.25, cmap=cmap)\n    pc.set_clim(0, np.nanmax(df))\n    ax.set_yticks([x + 0.5 for x in range(len(df))])\n    ax.set_yticklabels(df.index)\n    ax.set_xticks([])\n    ax.set_xlabel(\"Time\")\n    ax.invert_yaxis()\n    return ax\n"}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    batch.expect_column_to_exist(name)\n\n    if summary[\"n_missing\"] == 0:\n        batch.expect_column_values_to_not_be_null(name)\n\n    if summary[\"p_unique\"] == 1.0:\n        batch.expect_column_values_to_be_unique(name)\n\n    return name, summary, batch\n"}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    from great_expectations.profile.base import ProfilerTypeMapping\n\n    numeric_type_names = (\n        ProfilerTypeMapping.INT_TYPE_NAMES + ProfilerTypeMapping.FLOAT_TYPE_NAMES\n    )\n\n    batch.expect_column_values_to_be_in_type_list(\n        name,\n        numeric_type_names,\n        meta={\n            \"notes\": {\n                \"format\": \"markdown\",\n                \"content\": [\n                    \"The column values should be stored in one of these types.\"\n                ],\n            }\n        },\n    )\n\n    if summary[\"monotonic_increase\"]:\n        batch.expect_column_values_to_be_increasing(\n            name, strictly=summary[\"monotonic_increase_strict\"]\n        )\n\n    if summary[\"monotonic_decrease\"]:\n        batch.expect_column_values_to_be_decreasing(\n            name, strictly=summary[\"monotonic_decrease_strict\"]\n        )\n\n    if any(k in summary for k in [\"min\", \"max\"]):\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary.get(\"min\"), max_value=summary.get(\"max\")\n        )\n\n    return name, summary, batch\n"}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    absolute_threshold = 10\n    relative_threshold = 0.2\n    if (\n        summary[\"n_distinct\"] < absolute_threshold\n        or summary[\"p_distinct\"] < relative_threshold\n    ):\n        batch.expect_column_values_to_be_in_set(\n            name, set(summary[\"value_counts_without_nan\"].keys())\n        )\n    return name, summary, batch\n"}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    if any(k in summary for k in [\"min\", \"max\"]):\n        batch.expect_column_values_to_be_between(\n            name,\n            min_value=summary.get(\"min\"),\n            max_value=summary.get(\"max\"),\n            parse_strings_as_datetimes=True,\n        )\n\n    return name, summary, batch\n"}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    batch.expect_file_to_exist(name)\n\n    return name, summary, batch\n"}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    series = pd.Series(vc.index, index=vc)\n    word_lists = series.str.lower().str.split()\n    words = word_lists.explode().str.strip(string.punctuation + string.whitespace)\n    word_counts = pd.Series(words.index, index=words)\n    # fix for pandas 1.0.5\n    word_counts = word_counts[word_counts.index.notnull()]\n    word_counts = word_counts.groupby(level=0, sort=False).sum()\n    word_counts = word_counts.sort_values(ascending=False)\n\n    # Remove stop words\n    if len(stop_words) > 0:\n        stop_words = [x.lower() for x in stop_words]\n        word_counts = word_counts.loc[~word_counts.index.isin(stop_words)]\n\n    return {\"word_counts\": word_counts} if not word_counts.empty else {}\n"}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    if n_classes > 1:\n        # casting to numpy array to ensure correct dtype when a categorical integer\n        # variable is evaluated\n        value_counts = value_counts.to_numpy(dtype=float)\n        return 1 - (entropy(value_counts, base=2) / log2(n_classes))\n    return 0\n"}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(dict(self).values(), [])\n        return list(self)\n"}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    try:\n        package_name = package.__name__\n        package_path = package.__path__\n    except AttributeError:\n        # package isn't a package.\n        return False\n\n    full_module_name = package_name + \".\" + module_name\n    try:\n        return importlib_find(full_module_name, package_path) is not None\n    except ModuleNotFoundError:\n        # When module_name is an invalid dotted path, Python raises\n        # ModuleNotFoundError.\n        return False\n"}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() // 60\n    sign = \"-\" if offset < 0 else \"+\"\n    hhmm = \"%02d%02d\" % divmod(abs(offset), 60)\n    name = sign + hhmm\n    return timezone(timedelta(minutes=offset), name)\n"}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    if path is None:\n        return path\n    # I know about `os.sep` and `os.altsep` but I want to leave\n    # some flexibility for hardcoding separators.\n    return quote(str(path).replace(\"\\\\\", \"/\"), safe=\"/~!*()'\")\n"}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    elif not isinstance(value, str):\n        raise TypeError(\"Invalid path type: %s\" % type(value).__name__)\n    return Path(value)\n"}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    sections = [\n        \" \".join(random.sample(WORDS, random.randint(3, 12)))\n        for i in range(random.randint(1, 5))\n    ]\n    s = \", \".join(sections)\n    # Convert to sentence case and add end punctuation.\n    return \"%s%s%s\" % (s[0].upper(), s[1:], random.choice(\"?.\"))\n"}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort is None:\n        return dct\n\n    sort = sort.lower()\n    if sort.startswith(\"asc\"):\n        dct = dict(sorted(dct.items(), key=lambda x: x[0].casefold()))\n    elif sort.startswith(\"desc\"):\n        dct = dict(sorted(dct.items(), key=lambda x: x[0].casefold(), reverse=True))\n    else:\n        raise ValueError('\"sort\" should be \"ascending\", \"descending\" or None.')\n    return dct\n"}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n    except ValueError:\n        return False\n    return True\n"}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    s = s.encode()\n    try:\n        return base64.urlsafe_b64decode(s.ljust(len(s) + len(s) % 4, b\"=\"))\n    except (LookupError, BinasciiError) as e:\n        raise ValueError(e)\n"}
{"namespace": "django.utils.http.parse_etags", "completion": "    if etag_str.strip() == \"*\":\n        return [\"*\"]\n    else:\n        # Parse each ETag individually, and return any that are valid.\n        etag_matches = (ETAG_MATCH.match(etag.strip()) for etag in etag_str.split(\",\"))\n        return [match[1] for match in etag_matches if match]\n"}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if not pattern:\n        return False\n\n    pattern = pattern.lower()\n    return (\n        pattern[0] == \".\"\n        and (host.endswith(pattern) or host == pattern[1:])\n        or pattern == host\n    )\n"}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if filename:\n        disposition = \"attachment\" if as_attachment else \"inline\"\n        try:\n            filename.encode(\"ascii\")\n            file_expr = 'filename=\"{}\"'.format(\n                filename.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', r\"\\\"\")\n            )\n        except UnicodeEncodeError:\n            file_expr = \"filename*=utf-8''{}\".format(quote(filename))\n        return f\"{disposition}; {file_expr}\"\n    elif as_attachment:\n        return \"attachment\"\n    else:\n        return None\n"}
{"namespace": "pysnooper.utils.truncate", "completion": "    if (max_length is None) or (len(string) <= max_length):\n        return string\n    else:\n        left = (max_length - 3) // 2\n        right = max_length - 3 - left\n        return u'{}...{}'.format(string[:left], string[-right:])\n"}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    def code(s):\n        return compile(s, '<variable>', 'eval').co_code\n\n    return code('{}.x'.format(source)) != code('({}).x'.format(source))\n"}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    _orig_sys_path = sys.path[:]\n    sys.path.extend(paths)\n    try:\n        yield\n    finally:\n        sys.path = _orig_sys_path\n"}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    if mean.shape and len(mean) != 4 and mean.shape != img.shape:\n        mean = np.array(mean.tolist() + [0] * (4 - len(mean)), dtype=np.float64)\n    if not denominator.shape:\n        denominator = np.array([denominator.tolist()] * 4, dtype=np.float64)\n    elif len(denominator) != 4 and denominator.shape != img.shape:\n        denominator = np.array(denominator.tolist() + [1] * (4 - len(denominator)), dtype=np.float64)\n\n    img = np.ascontiguousarray(img.astype(\"float32\"))\n    cv2.subtract(img, mean.astype(np.float64), img)\n    cv2.multiply(img, denominator.astype(np.float64), img)\n    return img\n"}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    img = img.astype(np.float32)\n    img -= mean\n    img *= denominator\n    return img\n"}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    if img.dtype == np.uint8:\n        table = (np.arange(0, 256.0 / 255, 1.0 / 255) ** gamma) * 255\n        img = cv2.LUT(img, table.astype(np.uint8))\n    else:\n        img = np.power(img, gamma)\n\n    return img\n"}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    new_image = image.copy()\n\n    for tile in tiles:\n        new_image[tile[0] : tile[0] + tile[4], tile[1] : tile[1] + tile[5]] = image[\n            tile[2] : tile[2] + tile[4], tile[3] : tile[3] + tile[5]\n        ]\n\n    return new_image\n"}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    x, y, a, s = keypoint[:4]\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, a + math.radians(angle), s\n"}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    (\n        x,\n        y,\n        a,\n        s,\n    ) = keypoint[:4]\n    height, width = rows, cols\n    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * width\n    matrix[1, 2] += dy * height\n\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    angle = a + math.radians(angle)\n    scale = s * scale\n\n    return x, y, angle, scale\n"}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    two_pi = 2 * math.pi\n    return angle % two_pi\n"}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    img = np.rot90(img, factor)\n    return np.ascontiguousarray(img)\n"}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    return [\n        convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]\n"}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]\n"}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if low is not None and bias is not None:\n        raise ValueError(\"Arguments low and bias are mutually exclusive\")\n\n    if param is None:\n        return param\n\n    if isinstance(param, (int, float)):\n        if low is None:\n            param = -param, +param\n        else:\n            param = (low, param) if low < param else (param, low)\n    elif isinstance(param, Sequence):\n        if len(param) != 2:\n            raise ValueError(\"to_tuple expects 1 or 2 values\")\n        param = tuple(param)\n    else:\n        raise ValueError(\"Argument param must be either scalar (int, float) or tuple\")\n\n    if bias is not None:\n        return tuple(bias + x for x in param)\n\n    return tuple(param)\n"}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        augs = ReplayCompose._restore_for_replay(saved_augmentations)\n        return augs(force_apply=True, **kwargs)\n"}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    splitted = class_fullname.split(\".\")\n    if len(splitted) == 1:\n        return class_fullname\n    top_module, *_, class_name = splitted\n    if top_module == \"albumentations\":\n        return class_name\n    return class_fullname\n"}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    if platform.system() == \"Windows\":\n        path = path.replace(\"\\\\\", \"/\")\n    return path\n"}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    cleaned = re.sub(r\"[^a-zA-Z0-9_\\-.]\", \"_\", name)\n    if len(cleaned) <= 128:\n        return cleaned\n    # truncate with dots in the middle using regex\n    return re.sub(r\"(^.{63}).*(.{63}$)\", r\"\\g<1>..\\g<2>\", cleaned)\n"}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    if not d or unsafe_keys.isdisjoint(d):\n        return d\n    safe_dict = d.copy()\n    safe_dict.update({k: redact_str for k in unsafe_keys.intersection(d)})\n    return safe_dict\n"}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    full_version = sys.version.split()[0].split(\".\")\n    major = full_version[0]\n    version = \".\".join(full_version[:2]) if len(full_version) >= 2 else major + \".0\"\n    return version, major\n"}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        import wandb.sdk.artifacts.storage_policies  # noqa: F401\n\n        for sub in cls.__subclasses__():\n            if sub.name() == name:\n                return sub\n        raise NotImplementedError(f\"Failed to find storage policy '{name}'\")\n"}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    alphabet = string.ascii_lowercase + string.digits\n    return \"\".join(secrets.choice(alphabet) for _ in range(length))\n"}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        offsets = sorted(list(console.keys()))\n        intervals: List = []\n        for i, num in enumerate(offsets):\n            if i == 0:\n                intervals.append([num, num])\n                continue\n            largest = intervals[-1][1]\n            if num == largest + 1:\n                intervals[-1][1] = num\n            else:\n                intervals.append([num, num])\n        return intervals\n"}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        try:\n            stats = {}\n            devices = self._gc_ipu_info.getDevices()\n\n            for device in devices:\n                device_metrics: Dict[str, str] = dict(device)\n\n                pid = device_metrics.get(\"user process id\")\n                if pid is None or int(pid) != self._pid:\n                    continue\n\n                device_id = device_metrics.get(\"id\")\n                initial_call = device_id not in self._devices_called\n                if device_id is not None:\n                    self._devices_called.add(device_id)\n\n                for key, value in device_metrics.items():\n                    log_metric = initial_call or key in self.variable_metric_keys\n                    if not log_metric:\n                        continue\n                    parsed = self.parse_metric(key, value)\n                    if parsed is None:\n                        continue\n                    parsed_key, parsed_value = parsed\n                    stats[self.name.format(device_id, parsed_key)] = parsed_value\n\n            self.samples.append(stats)\n\n        except Exception as e:\n            wandb.termwarn(f\"IPU stats error {e}\", repeat=False)\n"}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    rows = list(rows)\n    fixed_row = rows[0][:]\n\n    for row in rows[1:]:\n        if len(row) == 0:\n            row = ['']\n\n        fixed_row[-1] += f\"{joiner}{row[0]}\"\n        fixed_row.extend(row[1:])\n\n    return fixed_row\n"}
{"namespace": "csvkit.convert.guess_format", "completion": "    last_period = filename.rfind('.')\n\n    if last_period == -1:\n        # No extension: assume fixed-width\n        return 'fixed'\n\n    extension = filename[last_period + 1:].lower()\n\n    if extension in ('csv', 'dbf', 'fixed', 'xls', 'xlsx'):\n        return extension\n    if extension in ('json', 'js'):\n        return 'json'\n\n    return None\n"}
{"namespace": "folium.utilities.normalize", "completion": "    out = \"\".join([line.strip() for line in rendered.splitlines() if line.strip()])\n    out = out.replace(\", \", \",\")\n    return out\n"}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.statistics['generation'] = 0\n    individual.statistics['mutation_count'] = 0\n    individual.statistics['crossover_count'] = 0\n    individual.statistics['predecessor'] = 'ROOT',\n"}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    indices_to_remove: list[int] = []\n    for i, arg in enumerate(cmd_args):\n        # regex matching click.option\n        if re.match(r\"^--env=?.*\", arg):\n            indices_to_remove.append(i)\n            if \"=\" not in arg:\n                indices_to_remove.append(i + 1)\n\n    new_cmd_args: list[str] = []\n    for i, arg in enumerate(cmd_args):\n        if i not in indices_to_remove:\n            new_cmd_args.append(arg)\n    return new_cmd_args\n"}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    path = os.path.abspath(path)\n    if psutil.WINDOWS:\n        return pathlib.PureWindowsPath(path).as_uri()\n    if psutil.POSIX:\n        return pathlib.PurePosixPath(path).as_uri()\n    raise ValueError(\"Unsupported OS\")\n"}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed = urlparse(uri)\n    if parsed.scheme not in (\"file\", \"filesystem\", \"unix\"):\n        raise ValueError(\"Unsupported URI scheme\")\n    host = \"{0}{0}{mnt}{0}\".format(os.path.sep, mnt=parsed.netloc)\n    return os.path.normpath(os.path.join(host, url2pathname(unquote(parsed.path))))\n"}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(\"labels must be a dict!\")\n\n    for key, val in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"label keys must be strings\")\n\n        if not isinstance(val, str):\n            raise ValueError(\"label values must be strings\")\n"}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        _ = ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False\n"}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return pd.concat(batches, ignore_index=True), indices  # type: ignore (incomplete panadas types)\n"}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        return [\n            batch.iloc[indices[i] : indices[i + 1]].reset_index(drop=True)\n            for i in range(len(indices) - 1)\n        ]\n"}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batch: list[t.Any] = []\n        for subbatch in batches:\n            batch.extend(subbatch)\n        indices = list(itertools.accumulate(len(subbatch) for subbatch in batches))\n        indices = [0] + indices\n        return batch, indices\n"}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n"}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, str):\n        return value.encode(\"utf-8\")\n    elif isinstance(value, bytes):\n        return value\n    else:\n        raise TypeError(\"Expected a string value\")\n"}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    columns = shutil.get_terminal_size().columns\n    max_width = int(columns * scale)\n\n    filled = int(round(max_width * bytes_received / float(filesize)))\n    remaining = max_width - filled\n    progress_bar = ch * filled + \" \" * remaining\n    percent = round(100.0 * bytes_received / float(filesize), 1)\n    text = f\" \u21b3 |{progress_bar}| {percent}%\\r\"\n    sys.stdout.write(text)\n    sys.stdout.flush()\n"}
{"namespace": "pytube.cli._download", "completion": "    filesize_megabytes = stream.filesize // 1048576\n    print(f\"{filename or stream.default_filename} | {filesize_megabytes} MB\")\n    file_path = stream.get_file_path(filename=filename, output_path=target)\n    if stream.exists_at_path(file_path):\n        print(f\"Already downloaded at:\\n{file_path}\")\n        return\n\n    stream.download(output_path=target, filename=filename)\n    sys.stdout.write(\"\\n\")\n"}
{"namespace": "pytube.cli.display_streams", "completion": "    for stream in youtube.streams:\n        print(stream)\n"}
{"namespace": "pytube.cli._unique_name", "completion": "    counter = 0\n    while True:\n        file_name = f\"{base}_{media_type}_{counter}\"\n        file_path = os.path.join(target, f\"{file_name}.{subtype}\")\n        if not os.path.exists(file_path):\n            return file_name\n        counter += 1\n"}
{"namespace": "pytube.cli._print_available_captions", "completion": "    print(\n        f\"Available caption codes are: {', '.join(c.code for c in captions)}\"\n    )\n"}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    reverse_copy = arr.copy()[::-1]\n    for i in range(len(reverse_copy)):\n        arr[i] = reverse_copy[i]\n"}
{"namespace": "pytube.helpers.setup_logger", "completion": "    fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n    date_fmt = \"%H:%M:%S\"\n    formatter = logging.Formatter(fmt, datefmt=date_fmt)\n\n    # https://github.com/pytube/pytube/issues/163\n    logger = logging.getLogger(\"pytube\")\n    logger.setLevel(level)\n\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n\n    if log_filename is not None:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n"}
{"namespace": "pytube.helpers.deprecated", "completion": "    def decorator(func1):\n        message = \"Call to deprecated function {name} ({reason}).\"\n\n        @functools.wraps(func1)\n        def new_func1(*args, **kwargs):\n            warnings.simplefilter(\"always\", DeprecationWarning)\n            warnings.warn(\n                message.format(name=func1.__name__, reason=reason),\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            warnings.simplefilter(\"default\", DeprecationWarning)\n            return func1(*args, **kwargs)\n\n        return new_func1\n\n    return decorator\n"}
{"namespace": "pytube.helpers.uniqueify", "completion": "    seen: Dict[Any, bool] = {}\n    result = []\n    for item in duped_list:\n        if item in seen:\n            continue\n        seen[item] = True\n        result.append(item)\n    return result\n"}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path:\n        if not os.path.isabs(output_path):\n            output_path = os.path.join(os.getcwd(), output_path)\n    else:\n        output_path = os.getcwd()\n    os.makedirs(output_path, exist_ok=True)\n    return output_path\n"}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        \"This is a private video. Please sign in to verify that you may see it.\",\n        \"\\\"simpleText\\\":\\\"Private video\\\"\",\n        \"This video is private.\"\n    ]\n    for string in private_strings:\n        if string in watch_html:\n            return True\n    return False\n"}
{"namespace": "pymc.math.cartesian", "completion": "    N = len(arrays)\n    arrays_np = [np.asarray(x) for x in arrays]\n    arrays_2d = [x[:, None] if np.asarray(x).ndim == 1 else x for x in arrays_np]\n    arrays_integer = [np.arange(len(x)) for x in arrays_2d]\n    product_integers = np.stack(np.meshgrid(*arrays_integer, indexing=\"ij\"), -1).reshape(-1, N)\n    return np.concatenate(\n        [array[product_integers[:, i]] for i, array in enumerate(arrays_2d)], axis=-1\n    )\n"}
{"namespace": "pymc.math.log1mexp", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    return pt.log1mexp(x)\n"}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    x = np.asarray(x, dtype=\"float\")\n\n    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp_numpy will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    out = np.empty_like(x)\n    mask = x < -0.6931471805599453  # log(1/2)\n    out[mask] = np.log1p(-np.exp(x[mask]))\n    mask = ~mask\n    out[mask] = np.log(-np.expm1(x[mask]))\n    return out\n"}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    nidata = arviz.InferenceData(attrs=idata.attrs)\n    for gname, group in idata.items():\n        if \"sample_stat\" in gname:\n            group = group.drop_vars(names=[\"warning\", \"warning_dim_0\"], errors=\"ignore\")\n        nidata.add_groups({gname: group}, coords=group.coords, dims=group.dims)\n    return nidata\n"}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    def expand(var):\n        new_vars = expand_fn(var)\n\n        if var.owner and var not in stop_at_vars:\n            new_vars.extend(reversed(var.owner.inputs))\n\n        return new_vars\n\n    yield from walk(graphs, expand, bfs=False)\n"}
{"namespace": "pymc.testing.select_by_precision", "completion": "    decimal = float64 if pytensor.config.floatX == \"float64\" else float32\n    return decimal\n"}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    def f(x, args):\n        if args is None:\n            return func(x)\n        else:\n            if not isinstance(args, tuple):\n                args = (args,)\n            return func(x, *args)\n\n    return f\n"}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "    if isinstance(X, TensorConstant):\n        X = X.value\n    elif isinstance(X, (np.ndarray, tuple, list)):\n        X = np.asarray(X)\n    else:\n        raise TypeError(\n            \"To use K-means initialization, \"\n            \"please provide X as a type that \"\n            \"can be cast to np.ndarray, instead \"\n            \"of {}\".format(type(X))\n        )\n    scaling = np.std(X, 0)\n    # if std of a column is very small (zero), don't normalize that column\n    scaling[scaling <= 1e-6] = 1.0\n    Xw = X / scaling\n\n    if \"k_or_guess\" in kmeans_kwargs:\n        warnings.warn(\"Use `n_inducing` to set the `k_or_guess` parameter instead.\")\n\n    Xu, distortion = kmeans(Xw, k_or_guess=n_inducing, **kmeans_kwargs)\n    return Xu * scaling\n"}
{"namespace": "pymc.pytensorf.floatX", "completion": "    try:\n        return X.astype(pytensor.config.floatX)\n    except AttributeError:\n        # Scalar passed\n        return np.asarray(X, dtype=pytensor.config.floatX)\n"}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        np.linalg.cholesky(AA)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n"}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    i = pt.arange(1, p + 1)\n    return p * (p - 1) * pt.log(np.pi) / 4.0 + pt.sum(gammaln(a + (1.0 - i) / 2.0), axis=0)\n"}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    warnings.warn(\n        \"incomplete_beta has been deprecated. Use pytensor.tensor.betainc instead.\",\n        FutureWarning,\n        stacklevel=2,\n    )\n    return pt.betainc(a, b, value)\n"}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    deterministics = model.deterministics\n    observed_rvs = set(model.observed_RVs)\n    blockers = model.basic_RVs\n    return [\n        deterministic\n        for deterministic in deterministics\n        if observed_rvs & set(ancestors([deterministic], blockers=blockers))\n    ]\n"}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    lnw = len(weights)\n    arange = np.arange(lnw)\n    uniform = (rng.random(1) + arange) / lnw\n\n    idx = 0\n    weight_accu = weights[0]\n    new_indices = np.empty(lnw, dtype=int)\n    for i in arange:\n        while uniform[i] > weight_accu:\n            idx += 1\n            weight_accu += weights[idx]\n        new_indices[i] = idx\n\n    return new_indices\n"}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "    if combine:\n        results = np.concatenate(results)\n        if not squeeze:\n            results = [results]\n    else:\n        if squeeze and len(results) == 1:\n            results = results[0]\n    return results\n"}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        value = pt.as_tensor(value)\n        log_value = pt.log(value)\n        N = value.shape[-1].astype(value.dtype)\n        shift = pt.sum(log_value, -1, keepdims=True) / N\n        return log_value[..., :-1] - shift\n"}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.concatenate([value, -pt.sum(value, -1, keepdims=True)], axis=-1)\n        exp_value_max = pt.exp(value - pt.max(value, -1, keepdims=True))\n        return exp_value_max / pt.sum(exp_value_max, -1, keepdims=True)\n"}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    def expand(var: TensorVariable, stop_at_vars=stop_at_vars) -> List[TensorVariable]:\n        new_vars = expand_fn(var)\n\n        if (\n            var.owner\n            and (walk_past_rvs or not isinstance(var.owner.op, MeasurableVariable))\n            and (var not in stop_at_vars)\n        ):\n            new_vars.extend(reversed(var.owner.inputs))\n\n        return new_vars\n\n    yield from walk(graphs, expand, False)\n"}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    metrics_by_name = {}\n    for metric_entry in logged_metrics:\n        if metric_entry.name not in metrics_by_name:\n            metrics_by_name[metric_entry.name] = {\n                \"steps\": [],\n                \"values\": [],\n                \"timestamps\": [],\n                \"name\": metric_entry.name,\n            }\n        metrics_by_name[metric_entry.name][\"steps\"].append(metric_entry.step)\n        metrics_by_name[metric_entry.name][\"values\"].append(metric_entry.value)\n        metrics_by_name[metric_entry.name][\"timestamps\"].append(metric_entry.timestamp)\n    return metrics_by_name\n"}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path[:-1]:\n        if p not in current_option:\n            current_option[p] = dict()\n        current_option = current_option[p]\n    current_option[split_path[-1]] = value\n"}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    if not path:\n        return d\n    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option\n"}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "    scan_op = Scan(scan_args.inner_inputs, scan_args.inner_outputs, scan_args.info, **kwargs)\n    node = scan_op.make_node(*scan_args.outer_inputs)\n    updates = OrderedUpdates(zip(scan_args.outer_in_shared, scan_args.outer_out_shared))\n    return node.outputs, updates\n"}
{"namespace": "sacred.utils.is_prefix", "completion": "    pre_path = pre_path.strip(\".\")\n    path = path.strip(\".\")\n    return not pre_path or path.startswith(pre_path + \".\")\n"}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n    work = [cls]\n    while work:\n        parent = work.pop()\n        for child in parent.__subclasses__():\n            if child not in subclasses:\n                subclasses.add(child)\n                work.append(child)\n    return subclasses\n"}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()\n"}
{"namespace": "sacred.utils.module_exists", "completion": "    try:\n        return pkgutil.find_loader(modname) is not None\n    except ImportError:\n        # TODO: Temporary fix for tf 1.14.0.\n        # Should be removed once fixed in tf.\n        return True\n"}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    orig_lines = text.split(\"\\n\")\n    orig_lines_len = len(orig_lines)\n    new_lines = []\n    for orig_line_idx, orig_line in enumerate(orig_lines):\n        chars, cursor = [], 0\n        orig_line_len = len(orig_line)\n        for orig_char_idx, orig_char in enumerate(orig_line):\n            if orig_char == \"\\r\" and (\n                orig_char_idx != orig_line_len - 1\n                or orig_line_idx != orig_lines_len - 1\n            ):\n                cursor = 0\n            elif orig_char == \"\\b\":\n                cursor = max(0, cursor - 1)\n            else:\n                if (\n                    orig_char == \"\\r\"\n                    and orig_char_idx == orig_line_len - 1\n                    and orig_line_idx == orig_lines_len - 1\n                ):\n                    cursor = len(chars)\n                if cursor == len(chars):\n                    chars.append(orig_char)\n                else:\n                    chars[cursor] = orig_char\n                cursor += 1\n        new_lines.append(\"\".join(chars))\n    return \"\\n\".join(new_lines)\n"}
{"namespace": "sacred.commands.help_for_command", "completion": "    help_text = pydoc.text.document(command)\n    # remove backspaces\n    return re.subn(\".\\\\x08\", \"\", help_text)[0]\n"}
{"namespace": "sacred.optional.optional_import", "completion": "    try:\n        packages = [importlib.import_module(pn) for pn in package_names]\n        return True, packages[0]\n    except ImportError:\n        return False, None\n"}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith((\".py\", \".so\", \".pyd\", \".ipynb\")):\n        return pyc_name\n    assert pyc_name.endswith(\".pyc\")\n    non_compiled_file = pyc_name[:-1]\n    if os.path.exists(non_compiled_file):\n        return non_compiled_file\n    return pyc_name\n"}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            if hasattr(iterable, \"keys\"):\n                for key in iterable:\n                    self[key] = iterable[key]\n            else:\n                for (key, value) in iterable:\n                    self[key] = value\n        for key in kwargs:\n            self[key] = kwargs[key]\n"}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    sline = line.strip()\n    return sline == \"\" or sline.startswith(\"#\")\n"}
{"namespace": "boltons.funcutils.copy_function", "completion": "    ret = FunctionType(orig.__code__,\n                       orig.__globals__,\n                       name=orig.__name__,\n                       argdefs=getattr(orig, \"__defaults__\", None),\n                       closure=getattr(orig, \"__closure__\", None))\n    if copy_dict:\n        ret.__dict__.update(orig.__dict__)\n    return ret\n"}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    for i, (line_sym, indent_sym) in enumerate(zip(line, indent)):\n        if line_sym != indent_sym:\n            start = i\n            break\n    else:\n        start = len(indent)\n    return line[start:]\n"}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    _repr = kw.pop('repr', repr)\n    if kw:\n        raise TypeError('unexpected keyword args: %r' % ', '.join(kw.keys()))\n    kwargs = kwargs or {}\n    a_text = ', '.join([_repr(a) for a in args])\n    if isinstance(kwargs, dict):\n        kwarg_items = [(k, kwargs[k]) for k in sorted(kwargs)]\n    else:\n        kwarg_items = kwargs\n    kw_text = ', '.join(['%s=%s' % (k, _repr(v)) for k, v in kwarg_items])\n\n    all_args_text = a_text\n    if all_args_text and kw_text:\n        all_args_text += ', '\n    all_args_text += kw_text\n\n    return '%s(%s)' % (name, all_args_text)\n"}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        if item_index == dest_index:\n            return\n        item = self.pop(item_index)\n        self.insert(dest_index, item)\n"}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    out = StringIO()\n    f = GzipFile(fileobj=out, mode='wb', compresslevel=level)\n    f.write(bytestring)\n    f.close()\n    return out.getvalue()\n"}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if not isinstance(obj, uuid.UUID):\n        try:\n            obj = uuid.UUID(obj)\n        except (TypeError, ValueError, AttributeError):\n            return False\n    if version and obj.version != int(version):\n        return False\n    return True\n"}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    output = []\n\n    for x in range_string.strip().split(delim):\n\n        # Range\n        if range_delim in x:\n            range_limits = list(map(int, x.split(range_delim)))\n            output += list(range(min(range_limits), max(range_limits)+1))\n\n        # Empty String\n        elif not x:\n            continue\n\n        # Integer\n        else:\n            output.append(int(x))\n\n    return sorted(output)\n"}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n"}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    start = float(start)\n    stop = float(stop)\n    factor = float(factor)\n    if start < 0.0:\n        raise ValueError('expected start >= 0, not %r' % start)\n    if factor < 1.0:\n        raise ValueError('expected factor >= 1.0, not %r' % factor)\n    if stop == 0.0:\n        raise ValueError('expected stop >= 0')\n    if stop < start:\n        raise ValueError('expected stop >= start, not %r' % stop)\n    if count is None:\n        denom = start if start else 1\n        count = 1 + math.ceil(math.log(stop/denom, factor))\n        count = count if start else count + 1\n    if count != 'repeat' and count < 0:\n        raise ValueError('count must be positive or \"repeat\", not %r' % count)\n    if jitter:\n        jitter = float(jitter)\n        if not (-1.0 <= jitter <= 1.0):\n            raise ValueError('expected jitter -1 <= j <= 1, not: %r' % jitter)\n\n    cur, i = start, 0\n    while count == 'repeat' or i < count:\n        if not jitter:\n            cur_ret = cur\n        elif jitter:\n            cur_ret = cur - (cur * jitter * random.random())\n        yield cur_ret\n        i += 1\n        if cur == 0:\n            cur = 1\n        elif cur < stop:\n            cur *= factor\n        if cur > stop:\n            cur = stop\n    return\n"}
{"namespace": "boltons.cacheutils.cached", "completion": "    def cached_func_decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_func_decorator\n"}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    a_milli = 1000000.0\n    td_ds = td.seconds + (td.days * 86400)  # 24 * 60 * 60\n    td_micro = td.microseconds + (td_ds * a_milli)\n    return td_micro / a_milli\n"}
{"namespace": "boltons.gcutils.get_all", "completion": "    if not isinstance(type_obj, type):\n        raise TypeError('expected a type, not %r' % type_obj)\n    try:\n        type_is_tracked = gc.is_tracked(type_obj)\n    except AttributeError:\n        type_is_tracked = False  # Python 2.6 and below don't get the speedup\n    if type_is_tracked:\n        to_check = gc.get_referrers(type_obj)\n    else:\n        to_check = gc.get_objects()\n\n    if include_subtypes:\n        ret = [x for x in to_check if isinstance(x, type_obj)]\n    else:\n        ret = [x for x in to_check if type(x) is type_obj]\n    return ret\n"}
{"namespace": "boltons.timeutils.daterange", "completion": "    if not isinstance(start, date):\n        raise TypeError(\"start expected datetime.date instance\")\n    if stop and not isinstance(stop, date):\n        raise TypeError(\"stop expected datetime.date instance or None\")\n    try:\n        y_step, m_step, d_step = step\n    except TypeError:\n        y_step, m_step, d_step = 0, 0, step\n    else:\n        y_step, m_step = int(y_step), int(m_step)\n    if isinstance(d_step, int):\n        d_step = timedelta(days=int(d_step))\n    elif isinstance(d_step, timedelta):\n        pass\n    else:\n        raise ValueError('step expected int, timedelta, or tuple'\n                         ' (year, month, day), not: %r' % step)\n    \n    m_step += y_step * 12\n\n    if stop is None:\n        finished = lambda now, stop: False\n    elif start <= stop:\n        finished = operator.gt if inclusive else operator.ge\n    else:\n        finished = operator.lt if inclusive else operator.le\n    now = start\n\n    while not finished(now, stop):\n        yield now\n        if m_step:\n            m_y_step, cur_month = divmod((now.month - 1) + m_step, 12)\n            now = now.replace(year=now.year + m_y_step,\n                              month=(cur_month + 1))\n        now = now + d_step\n    return\n"}
{"namespace": "boltons.mathutils.clamp", "completion": "    if upper < lower:\n        raise ValueError('expected upper bound (%r) >= lower bound (%r)'\n                         % (upper, lower))\n    return min(max(x, lower), upper)\n"}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    options = sorted(options)\n    i = bisect.bisect_left(options, x)\n    if i == len(options):\n        raise ValueError(\"no ceil options greater than or equal to: %r\" % x)\n    return options[i]\n"}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    formatter = Formatter()\n    fargs, fkwargs, _dedup = [], [], set()\n\n    def _add_arg(argname, type_char='s'):\n        if argname not in _dedup:\n            _dedup.add(argname)\n            argtype = _TYPE_MAP.get(type_char, str)  # TODO: unicode\n            try:\n                fargs.append((int(argname), argtype))\n            except ValueError:\n                fkwargs.append((argname, argtype))\n\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if fname is not None:\n            type_char = fspec[-1:]\n            fname_list = re.split('[.[]', fname)\n            if len(fname_list) > 1:\n                raise ValueError('encountered compound format arg: %r' % fname)\n            try:\n                base_fname = fname_list[0]\n                assert base_fname\n            except (IndexError, AssertionError):\n                raise ValueError('encountered anonymous positional argument')\n            _add_arg(fname, type_char)\n            for sublit, subfname, _, _ in formatter.parse(fspec):\n                # TODO: positional and anon args not allowed here.\n                if subfname is not None:\n                    _add_arg(subfname)\n    return fargs, fkwargs\n"}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n\n    i = bisect.bisect_right(options, x)\n    if not i:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i - 1]\n"}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]\n"}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if isinstance(dict_or_iterable, dict):\n            for val in dict_or_iterable.values():\n                hash(val)\n                keys_vals = list(dict_or_iterable.items())\n        else:\n            for key, val in dict_or_iterable:\n                hash(key)\n                hash(val)\n                keys_vals = list(dict_or_iterable)\n        for val in kw.values():\n            hash(val)\n        keys_vals.extend(kw.items())\n        for key, val in keys_vals:\n            self[key] = val\n"}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n"}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        data = dict(self)\n        data.update(*a, **kw)\n        return type(self)(data)\n"}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = d.keys()\n    if drop is None:\n        drop = []\n\n    keys = set(keep) - set(drop)\n\n    return type(d)([(k, v) for k, v in d.items() if k in keys])\n"}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        cn = self.__class__.__name__\n        return '%s(%s)' % (cn, dict.__repr__(self))\n"}
{"namespace": "gunicorn.config.validate_callable", "completion": "    def _validate_callable(val):\n        if isinstance(val, str):\n            try:\n                mod_name, obj_name = val.rsplit(\".\", 1)\n            except ValueError:\n                raise TypeError(\"Value '%s' is not import string. \"\n                                \"Format: module[.submodules...].object\" % val)\n            try:\n                mod = __import__(mod_name, fromlist=[obj_name])\n                val = getattr(mod, obj_name)\n            except ImportError as e:\n                raise TypeError(str(e))\n            except AttributeError:\n                raise TypeError(\"Can not load '%s' from '%s'\"\n                                \"\" % (obj_name, mod_name))\n        if not callable(val):\n            raise TypeError(\"Value is not callable: %s\" % val)\n        if arity != -1 and arity != util.get_arity(val):\n            raise TypeError(\"Value must have an arity of: %s\" % arity)\n        return val\n    return _validate_callable\n"}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    config_path = os.path.join(os.path.abspath(os.getcwd()),\n                               'gunicorn.conf.py')\n    if os.path.exists(config_path):\n        return config_path\n    return None\n"}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:  # not a valid address\n        return False\n    except ValueError:  # ipv6 not supported on this platform\n        return False\n    return True\n"}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    fds = int(os.environ.get('LISTEN_FDS', 0))\n    listen_pid = int(os.environ.get('LISTEN_PID', 0))\n\n    if listen_pid != os.getpid():\n        return 0\n\n    if unset_environment:\n        os.environ.pop('LISTEN_PID', None)\n        os.environ.pop('LISTEN_FDS', None)\n\n    return fds\n"}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n    s = email.utils.formatdate(timestamp, localtime=False, usegmt=True)\n    return s\n"}
{"namespace": "gunicorn.util.parse_address", "completion": "    if re.match(r'unix:(//)?', netloc):\n        return re.split(r'unix:(//)?', netloc)[-1]\n\n    if netloc.startswith(\"fd://\"):\n        fd = netloc[5:]\n        try:\n            return int(fd)\n        except ValueError:\n            raise RuntimeError(\"%r is not a valid file descriptor.\" % fd) from None\n\n    if netloc.startswith(\"tcp://\"):\n        netloc = netloc.split(\"tcp://\")[1]\n    host, port = netloc, default_port\n\n    if '[' in netloc and ']' in netloc:\n        host = netloc.split(']')[0][1:]\n        port = (netloc.split(']:') + [default_port])[1]\n    elif ':' in netloc:\n        host, port = (netloc.split(':') + [default_port])[:2]\n    elif netloc == \"\":\n        host, port = \"0.0.0.0\", default_port\n\n    try:\n        port = int(port)\n    except ValueError:\n        raise RuntimeError(\"%r is not a valid port number.\" % port)\n\n    return host.lower(), port\n"}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, bytes):\n        return value\n    if not isinstance(value, str):\n        raise TypeError('%r is not a string' % value)\n\n    return value.encode(encoding)\n"}
{"namespace": "gunicorn.util.warn", "completion": "    print(\"!!!\", file=sys.stderr)\n\n    lines = msg.splitlines()\n    for i, line in enumerate(lines):\n        if i == 0:\n            line = \"WARNING: %s\" % line\n        print(\"!!! %s\" % line, file=sys.stderr)\n\n    print(\"!!!\\n\", file=sys.stderr)\n    sys.stderr.flush()\n"}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    if uri.startswith(\"//\"):\n        # When the path starts with //, urlsplit considers it as a\n        # relative uri while the RFC says we should consider it as abs_path\n        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2\n        # We use temporary dot prefix to workaround this behaviour\n        parts = urllib.parse.urlsplit(\".\" + uri)\n        return parts._replace(path=parts.path[1:])\n\n    return urllib.parse.urlsplit(uri)\n"}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        if not getattr(self, \"has_next_page\", True):\n            return None\n        return getattr(self, \"end_cursor\", None)\n"}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        to_set = [\"+all\"]\n    else:\n        to_set = [\"-all\"]\n        omitted = sorted(known_permissions - set(permissions))\n        to_set.extend(f\"-{x}\" for x in omitted)\n        to_set.extend(f\"+{x}\" for x in permissions)\n    return \",\".join(to_set)\n"}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        import json\n\n        if self.pretty:\n            self.json_indent = 2\n            self.json_separators = None\n\n        j_string = json.dumps(\n            self.data_out,\n            indent=self.json_indent,\n            separators=self.json_separators,\n            ensure_ascii=self.ascii_only\n        )\n\n        if not self.mono:\n            class JcStyle(Style):\n                styles: CustomColorType = self.custom_colors\n\n            return str(highlight(j_string, JsonLexer(), Terminal256Formatter(style=JcStyle))[0:-1])\n\n        return j_string\n"}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "    if dependency.find(\"@\") > 0 and (\n            dependency.find(\"@\") < dependency.find(\"://\") or\n            \"://\" not in dependency\n            ):\n        # WORKAROUND FOR UPSTREAM BUG:\n        # https://github.com/pypa/pip/issues/6097\n        # (Please REMOVE workaround once that is fixed & released upstream!)\n        #\n        # Basically, setup_requires() can contain a format pip won't install\n        # from a requirements.txt (PEP 508 URLs).\n        # To avoid this, translate to an #egg= reference:\n        if dependency.endswith(\"#\"):\n            dependency = dependency[:-1]\n        url = (dependency.partition(\"@\")[2].strip().partition(\"#egg\")[0] +\n               \"#egg=\" +\n               dependency.partition(\"@\")[0].strip()\n              )\n        return url\n    return dependency\n"}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    deps = [\n        ((dep.lower(),)\n         if not isinstance(dep, (list, tuple))\n         else tuple([dep_entry.lower()\n                     for dep_entry in dep\n                    ]))\n        for dep in deps\n    ]\n    return deps\n"}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for dirn, subdirs, filens in walk(base_dir):\n\n        # Remove invalid subdirs so that they will not be walked\n        for i in reversed(range(len(subdirs))):\n            subdir = subdirs[i]\n            if subdir in invalid_dir_names:\n                subdirs.pop(i)\n\n        for filen in filens:\n            for pattern in invalid_file_patterns:\n                if fnmatch(filen, pattern):\n                    break\n            else:\n                yield join(dirn, filen)\n"}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    def rank_bootstrap(bootstrap):\n        \"\"\" Returns a ranking index for each bootstrap,\n            with higher priority ranked with higher number. \"\"\"\n        if bootstrap.name in default_recipe_priorities:\n            return default_recipe_priorities.index(bootstrap.name) + 1\n        return 0\n\n    # Rank bootstraps in order:\n    rank_a = rank_bootstrap(a)\n    rank_b = rank_bootstrap(b)\n    if rank_a != rank_b:\n        return (rank_b - rank_a)\n    else:\n        if a.name < b.name:  # alphabetic sort for determinism\n            return -1\n        else:\n            return 1\n"}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        forbidden_dirs = ('__pycache__', 'common')\n        bootstraps_dir = join(dirname(__file__), 'bootstraps')\n        result = set()\n        for name in listdir(bootstraps_dir):\n            if name in forbidden_dirs:\n                continue\n            filen = join(bootstraps_dir, name)\n            if isdir(filen):\n                result.add(name)\n        return result\n"}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    img_type = img.dtype\n    img = img.astype(np.float32)\n    if img_type == np.float32:\n        pass\n    elif img_type == np.uint8:\n        img /= 255.\n    else:\n        raise TypeError('The img type should be np.float32 or np.uint8, '\n                        f'but got {img_type}')\n    return img\n"}
{"namespace": "mackup.utils.error", "completion": "    fail = \"\\033[91m\"\n    end = \"\\033[0m\"\n    sys.exit(fail + \"Error: {}\".format(message) + end)\n"}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    if dst_type not in (np.uint8, np.float32):\n        raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                        f'but got {dst_type}')\n    if dst_type == np.uint8:\n        img = img.round()\n    else:\n        img /= 255.\n    return img.astype(dst_type)\n"}
{"namespace": "mackup.utils.is_process_running", "completion": "    is_running = False\n\n    # On systems with pgrep, check if the given process is running\n    if os.path.isfile(\"/usr/bin/pgrep\"):\n        dev_null = open(os.devnull, \"wb\")\n        returncode = subprocess.call([\"/usr/bin/pgrep\", process_name], stdout=dev_null)\n        is_running = bool(returncode == 0)\n\n    return is_running\n"}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.execute('SHOW server_version;').first()[0]\n    version_string = re.search('^(\\d+\\.\\d+)', server_version).group(0)\n    version = [int(x) for x in version_string.split('.')]\n    return 'pid' if version >= [9, 2] else 'procpid'\n"}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if not isinstance(s, str):\n        return s\n\n    res = bytearray()\n\n    b64_buffer: List[str] = []\n\n    def consume_b64_buffer(buf: List[str]) -> None:\n        \"\"\"\n        Consume the buffer by encoding it into a modified base 64 representation\n        and surround it with shift characters & and -\n        \"\"\"\n        if buf:\n            res.extend(b\"&\" + base64_utf7_encode(buf) + b\"-\")\n            del buf[:]\n\n    for c in s:\n        # printable ascii case should not be modified\n        o = ord(c)\n        if 0x20 <= o <= 0x7E:\n            consume_b64_buffer(b64_buffer)\n            # Special case: & is used as shift character so we need to escape it in ASCII\n            if o == 0x26:  # & = 0x26\n                res.extend(b\"&-\")\n            else:\n                res.append(o)\n\n        # Bufferize characters that will be encoded in base64 and append them later\n        # in the result, when iterating over ASCII character or the end of string\n        else:\n            b64_buffer.append(c)\n\n    # Consume the remaining buffer if the string finish with non-ASCII characters\n    consume_b64_buffer(b64_buffer)\n\n    return bytes(res)\n"}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo\n    v = \"%d.%d.%d\" % (major, minor, micro)\n    if releaselevel != \"final\":\n        v += \"-\" + releaselevel\n    return v\n"}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce = server_nonce.to_bytes(16, 'little', signed=True)\n    new_nonce = new_nonce.to_bytes(32, 'little', signed=True)\n    hash1 = sha1(new_nonce + server_nonce).digest()\n    hash2 = sha1(server_nonce + new_nonce).digest()\n    hash3 = sha1(new_nonce + new_nonce).digest()\n\n    key = hash1 + hash2[:12]\n    iv = hash2[12:20] + hash3 + new_nonce[:4]\n    return key, iv\n"}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    try:\n        return int.from_bytes(data, byteorder='big')\n    except:\n        return data\n"}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if response[\"result\"] == \"error\" and hasattr(controller, \"view\"):\n        controller.report_error([response[\"msg\"]])\n"}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        try:\n            return int(message_id)\n        except ValueError:\n            return None\n"}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        error = self._validate_narrow_link(parsed_link)\n\n        if error:\n            self.controller.report_error([f\" {error}\"])\n        else:\n            self._switch_narrow_to(parsed_link)\n\n            # Exit pop-up if MessageLinkButton exists in one.\n            if self.controller.is_any_popup_open():\n                self.controller.exit_popup()\n"}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    prop_n = \"_\".join([p.upper() for p in prop])\n    prop_v = \" , \".join([p.lower() for p in prop])\n    updated_colors: Any = Enum(  # type: ignore # Ref: python/mypy#529, #535 and #5317\n        \"Color\",\n        {\n            **{c.name: c.value for c in colors},\n            **{c.name + f\"__{prop_n}\": c.value + f\" , {prop_v}\" for c in colors},\n        },\n    )\n    return updated_colors\n"}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if not d:\n        return d\n    return Decimal(d, BasicContext)\n"}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except (TypeError, ValueError):\n        return i\n"}
{"namespace": "twilio.base.serialize.object", "completion": "    if isinstance(obj, dict) or isinstance(obj, list):\n        return json.dumps(obj)\n    return obj\n"}
{"namespace": "twilio.base.serialize.map", "completion": "    if not isinstance(lst, list):\n        return lst\n    return [serialize_func(e) for e in lst]\n"}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "    def deprecated_method_wrapper(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            msg = \"Function method .{}() is deprecated\".format(func.__name__)\n            msg += (\n                \" in favor of .{}()\".format(new_func)\n                if isinstance(new_func, str)\n                else \"\"\n            )\n            warnings.warn(msg, DeprecationWarning)\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if callable(new_func):\n        return deprecated_method_wrapper(new_func)\n\n    return deprecated_method_wrapper\n"}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items <= len(array):\n        return sample(array, nb_items)\n    return deepcopy(array)\n"}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string\n"}
{"namespace": "chatette.utils.str_to_bool", "completion": "    text = text.lower()\n    if text == \"true\":\n        return True\n    if text == \"false\":\n        return False\n    raise ValueError(\"Cannot convert '\" + str(text) + \"' into a boolean\")\n"}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is None and n2 is None:\n        return None\n    elif n1 is None:\n        return n2\n    elif n2 is None:\n        return n1\n    return min(n1, n2)\n"}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = [value]\n    else:\n        dict_of_lists[key].append(value)\n"}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)\n"}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        return \\\n            word.endswith(\"/\") or word.endswith(\"/g\") \\\n            or word.endswith(\"/i\") or word.endswith(\"/ig\") \\\n            or word.endswith(\"/gi\")\n"}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        if len(self.command_tokens) < 3:\n            self.print_wrapper.error_log(\n                \"Missing some arguments\\nUsage: \" + self.usage_str\n            )\n            return\n\n        unit_type = CommandStrategy.get_unit_type_from_str(self.command_tokens[1])\n        if unit_type is None:\n            self.print_wrapper.error_log(\n                \"Unknown unit type: '\" + str(self.command_tokens[1]) + \"'.\"\n            )\n            return\n\n        unit_regex = self.get_regex_name(self.command_tokens[2])\n        if unit_regex is None:\n            try:\n                [unit_name, variation_name] = \\\n                    CommandStrategy.split_exact_unit_name(\n                        self.command_tokens[2]\n                    )\n            except SyntaxError:\n                self.print_wrapper.error_log(\n                    \"Unit identifier couldn't be interpreted. \" + \\\n                    \"Did you mean to escape some hashtags '#'?\")\n                return\n            self.execute_on_unit(unit_type, unit_name, variation_name)\n        else:\n            count = 0\n            for unit_name in self.next_matching_unit_name(\n                unit_type, unit_regex\n            ):\n                self.execute_on_unit(unit_type, unit_name)\n                count += 1\n            if count == 0:\n                self.print_wrapper.write(\"No \" + unit_type.name + \" matched.\")\n        self.finish_execution()\n"}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    rng = rng or random\n\n    all_records.sort(key=lambda x: x[:2])\n\n    for priority, records in itertools.groupby(\n            all_records,\n            lambda x: x[0]):\n\n        records = list(records)\n        total_weight = sum(\n            weight\n            for _, weight, _ in records)\n\n        while records:\n            if len(records) == 1:\n                yield records[0][-1]\n                break\n\n            value = rng.randint(0, total_weight)\n            running_weight_sum = 0\n            for i, (_, weight, addr) in enumerate(records):\n                running_weight_sum += weight\n                if running_weight_sum >= value:\n                    yield addr\n                    del records[i]\n                    total_weight -= weight\n                    break\n"}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self[feature_cls]\n        except KeyError:\n            return default\n"}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "        def context_factory(transport):\n            ssl_context = metadata.ssl_context_factory()\n\n            if hasattr(ssl_context, \"set_alpn_protos\"):\n                try:\n                    ssl_context.set_alpn_protos([b'xmpp-client'])\n                except NotImplementedError:\n                    logger.warning(\n                        \"the underlying OpenSSL library does not support ALPN\"\n                    )\n            else:\n                logger.warning(\n                    \"OpenSSL.SSL.Context lacks set_alpn_protos - \"\n                    \"please update pyOpenSSL to a recent version\"\n                )\n\n            verifier.setup_context(ssl_context, transport)\n            return ssl_context\n        return context_factory\n"}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    segments = []\n    parent = el.getparent()\n\n    while parent != upto:\n        similar = list(parent.iterchildren(el.tag))\n        index = similar.index(el)\n        segments.insert(0, (el.tag, index))\n        el = parent\n        parent = el.getparent()\n\n    base = \"/\" + el.tag\n    if segments:\n        return base + \"/\" + \"/\".join(\n            \"{}[{}]\".format(tag, index)\n            for tag, index in segments\n        )\n    return base\n"}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        nodedomain, sep, resource = s.partition(\"/\")\n        if not sep:\n            resource = None\n\n        localpart, sep, domain = nodedomain.partition(\"@\")\n        if not sep:\n            domain = localpart\n            localpart = None\n        return cls(localpart, domain, resource, strict=strict)\n"}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    result = {\n        \"subject\": (\n            ((\"commonName\", x509.get_subject().commonName),),\n        )\n    }\n\n    for ext_idx in range(x509.get_extension_count()):\n        ext = x509.get_extension(ext_idx)\n        sn = ext.get_short_name()\n        if sn != b\"subjectAltName\":\n            continue\n\n        data = pyasn1.codec.der.decoder.decode(\n            ext.get_data(),\n            asn1Spec=pyasn1_modules.rfc2459.SubjectAltName())[0]\n        for name in data:\n            dNSName = name.getComponentByPosition(2)\n            if dNSName is None:\n                continue\n\n            if hasattr(dNSName, \"isValue\") and not dNSName.isValue:\n                continue\n\n            result.setdefault(\"subjectAltName\", []).append(\n                (\"DNS\", str(dNSName))\n            )\n\n    return result\n"}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    return OpenSSL.crypto.dump_certificate(\n        OpenSSL.crypto.FILETYPE_ASN1,\n        x509)\n"}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    return pyasn1.codec.der.decoder.decode(\n        blob,\n        asn1Spec=pyasn1_modules.rfc2459.Certificate()\n    )[0]\n"}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    pk = pyasn1_struct.getComponentByName(\n        \"tbsCertificate\"\n    ).getComponentByName(\n        \"subjectPublicKeyInfo\"\n    )\n\n    return pyasn1.codec.der.encoder.encode(pk)\n"}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_wrapper(f):\n            if not hasattr(f, \"__call__\"):\n                raise TypeError(\"must be callable, got {!r}\".format(f))\n            return functools.partial(cls._async_wrapper,\n                                     f,\n                                     loop)\n\n        return create_wrapper\n"}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        loop = asyncio.get_event_loop() if loop is None else loop\n\n        def spawn(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be coroutine, got {!r}\".format(f))\n\n            def wrapper(args, kwargs):\n                task = asyncio.ensure_future(f(*args, **kwargs), loop=loop)\n                task.add_done_callback(\n                    functools.partial(\n                        log_spawned,\n                        logger,\n                    )\n                )\n                return True\n\n            return wrapper\n\n        return spawn\n"}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    fut = asyncio.Future()\n    for signal in signals:\n        signal.connect(fut, signal.AUTO_FUTURE)\n    return fut\n"}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "        __groups = set(__groups) | {()}\n\n        return asyncio.ensure_future(__coro_fun(*args, **kwargs))\n"}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "    fut = asyncio.Future()\n    wait_for = list(wait_for)\n\n    def receive(obj):\n        nonlocal fut, stack\n        if cb is not None:\n            cb(obj)\n        fut.set_result(obj)\n        stack.close()\n\n    failure_future = xmlstream.error_future()\n\n    with contextlib.ExitStack() as stack:\n        for anticipated_cls in wait_for:\n            xmlstream.stanza_parser.add_class(\n                anticipated_cls,\n                receive)\n            stack.callback(\n                xmlstream.stanza_parser.remove_class,\n                anticipated_cls,\n            )\n\n        for to_send in send:\n            xmlstream.send_xso(to_send)\n\n        done, pending = await asyncio.wait(\n            [\n                fut,\n                failure_future,\n            ],\n            timeout=timeout,\n            return_when=asyncio.FIRST_COMPLETED,\n        )\n\n        for other_fut in pending:\n            other_fut.cancel()\n\n        if fut in done:\n            return fut.result()\n\n        if failure_future in done:\n            failure_future.result()\n        else:\n            failure_future.cancel()\n\n        raise TimeoutError()\n"}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    loop = loop or asyncio.get_event_loop()\n\n    local_future = asyncio.ensure_future(coroutine, loop=loop)\n    remote_future = asyncio.ensure_future(peer_coroutine, loop=loop)\n\n    done, pending = loop.run_until_complete(\n        asyncio.wait(\n            [\n                local_future,\n                remote_future,\n            ],\n            timeout=timeout,\n            return_when=asyncio.FIRST_EXCEPTION)\n    )\n    if not done:\n        raise asyncio.TimeoutError(\"Test timed out\")\n\n    if pending:\n        pending_fut = next(iter(pending))\n        pending_fut.cancel()\n        fut = next(iter(done))\n        try:\n            fut.result()\n        except:  # NOQA: E722\n            # everything is fine, the other one failed\n            raise\n        else:\n            if pending_fut == remote_future:\n                raise asyncio.TimeoutError(\n                    \"Peer coroutine did not return in time\")\n            else:\n                raise asyncio.TimeoutError(\n                    \"Coroutine under test did not return in time\")\n\n    if local_future.exception():\n        # re-throw the error properly\n        local_future.result()\n\n    remote_future.result()\n    return local_future.result()\n"}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    result = unittest.mock.Mock([])\n    names = {\n        name\n        for type_ in type(instance).__mro__\n        for name in type_.__dict__\n    }\n    for name in names:\n        signal = getattr(instance, name)\n        if not isinstance(signal, callbacks.AdHocSignal):\n            continue\n        cb = unittest.mock.Mock()\n        setattr(result, name, cb)\n        cb.return_value = None\n        signal.connect(cb)\n    return result\n"}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            payload=vcard,\n            to=jid,\n        )\n        await self.client.send(iq)\n"}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        if isinstance(self, type):\n            result = self()\n        else:\n            result = copy.deepcopy(self)\n        result.max_ = max_\n        return result\n"}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return {\n            aioxmpp.im.conversation.ConversationFeature.BAN,\n            aioxmpp.im.conversation.ConversationFeature.BAN_WITH_KICK,\n            aioxmpp.im.conversation.ConversationFeature.KICK,\n            aioxmpp.im.conversation.ConversationFeature.SEND_MESSAGE,\n            aioxmpp.im.conversation.ConversationFeature.SEND_MESSAGE_TRACKED,\n            aioxmpp.im.conversation.ConversationFeature.SET_TOPIC,\n            aioxmpp.im.conversation.ConversationFeature.SET_NICK,\n            aioxmpp.im.conversation.ConversationFeature.INVITE,\n            aioxmpp.im.conversation.ConversationFeature.INVITE_DIRECT,\n        }\n"}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        result = expr.eval(self)\n        iterator = iter(result)\n        try:\n            next(iterator)\n        except StopIteration:\n            return False\n        else:\n            return True\n        finally:\n            if hasattr(iterator, \"close\"):\n                iterator.close()\n"}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        if self.eval_leaf(ec):\n            yield True\n"}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 1\n    while depth:\n        ev = yield\n        if ev[0] == \"start\":\n            depth += 1\n        elif ev[0] == \"end\":\n            depth -= 1\n"}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 1\n    try:\n        next(dest)\n        while True:\n            ev = yield\n            if ev[0] == \"start\":\n                depth += 1\n            elif ev[0] == \"end\":\n                depth -= 1\n            try:\n                dest.send(ev)\n            except StopIteration as exc:\n                return exc.value\n    finally:\n        while depth > 0:\n            ev_type, *_ = yield\n            if ev_type == \"end\":\n                depth -= 1\n            elif ev_type == \"start\":\n                depth += 1\n"}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    _i = iter(receiver)\n    try:\n        _y = next(_i)\n    except StopIteration as _e:\n        return _e.value\n\n    try:\n        while True:\n            try:\n                _s = yield _y\n            except GeneratorExit as _e:\n                try:\n                    _m = _i.close\n                except AttributeError:\n                    pass\n                else:\n                    _m()\n                    raise _e\n            except BaseException as _e:\n                _x = sys.exc_info()\n                try:\n                    _m = _i.throw\n                except AttributeError:\n                    raise _e\n                else:\n                    try:\n                        _y = _m(*_x)\n                    except StopIteration as _e:\n                        _r = _e.value\n                        break\n            else:\n                dest.append(_s)\n                try:\n                    if _s is None:\n                        _y = next(_i)\n                    else:\n                        _y = _i.send(_s)\n                except StopIteration as _e:\n                    _r = _e.value\n                    break\n    except:  # NOQA\n        dest.clear()\n        raise\n    return _r\n"}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    name_stack = []\n\n    for ev_type, *ev_args in events:\n        if ev_type == \"start\":\n            name = (ev_args[0], ev_args[1])\n            dest.startElementNS(name, None, ev_args[2])\n            name_stack.append(name)\n        elif ev_type == \"end\":\n            name = name_stack.pop()\n            dest.endElementNS(name, None)\n        elif ev_type == \"text\":\n            dest.characters(ev_args[0])\n"}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=command_name,\n        )\n        return response\n"}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    identities = [\n        b\"/\".join([\n            escape(identity.category).encode(\"utf-8\"),\n            escape(identity.type_).encode(\"utf-8\"),\n            escape(str(identity.lang or \"\")).encode(\"utf-8\"),\n            escape(identity.name or \"\").encode(\"utf-8\"),\n        ])\n        for identity in identities\n    ]\n\n    if len(set(identities)) != len(identities):\n        raise ValueError(\"duplicate identity\")\n\n    identities.sort()\n    identities.append(b\"\")\n    return b\"<\".join(identities)\n"}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = list(escape(feature).encode(\"utf-8\") for feature in features)\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)\n"}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    types = set()\n    forms_list = []\n    for form in forms:\n        try:\n            form_types = set(\n                value\n                for field in form.fields.filter(attrs={\"var\": \"FORM_TYPE\"})\n                for value in field.values\n            )\n        except KeyError:\n            continue\n\n        if len(form_types) > 1:\n            raise ValueError(\"form with multiple types\")\n        elif not form_types:\n            continue\n\n        type_ = escape(next(iter(form_types))).encode(\"utf-8\")\n        if type_ in types:\n            raise ValueError(\"multiple forms of type {!r}\".format(type_))\n        types.add(type_)\n        forms_list.append((type_, form))\n    forms_list.sort()\n\n    parts = []\n\n    for type_, form in forms_list:\n        parts.append(type_)\n\n        field_list = sorted(\n            (\n                (escape(field.var).encode(\"utf-8\"), field.values)\n                for field in form.fields\n                if field.var != \"FORM_TYPE\"\n            ),\n            key=lambda x: x[0]\n        )\n\n        for var, values in field_list:\n            parts.append(var)\n            parts.extend(sorted(\n                escape(value).encode(\"utf-8\") for value in values\n            ))\n\n    parts.append(b\"\")\n    return b\"<\".join(parts)\n"}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        quoted = urllib.parse.quote(self.node, safe=\"\")\n        return (pathlib.Path(\"hashes\") /\n                \"{}_{}.xml\".format(self.algo, quoted))\n"}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    parts = [\n        feature.encode(\"utf-8\")+b\"\\x1f\"\n        for feature in features\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n"}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n"}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(form)\n        for form in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n"}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    impl = aioxmpp.hashes.hash_from_algo(algo)\n    impl.update(hash_input)\n    return impl.digest()\n"}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return \"urn:xmpp:caps#{}.{}\".format(\n            self.algo,\n            base64.b64encode(self.digest).decode(\"ascii\")\n        )\n"}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        encoded = base64.b32encode(\n            self.digest\n        ).decode(\"ascii\").rstrip(\"=\").lower()\n        return (pathlib.Path(\"caps2\") /\n                urllib.parse.quote(self.algo, safe=\"\") /\n                encoded[:2] /\n                encoded[2:4] /\n                \"{}.xml\".format(encoded[4:]))\n"}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is None:\n            return ()\n\n        return (\n            Key(algo, digest)\n            for algo, digest in presence.xep0390_caps.digests.items()\n            if aioxmpp.hashes.is_algo_supported(algo)\n        )\n"}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBED,\n                            to=peer_jid)\n        )\n"}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )\n"}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )\n"}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n"}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n"}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass\n"}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n"}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n"}
{"namespace": "cupy.random._generator.reset_states", "completion": "    global _random_states\n    _random_states = {}\n"}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    dtype = numpy.dtype(dtype)\n    if dtype.char not in ('f', 'd'):\n        raise TypeError('cupy.random only supports float32 and float64')\n    return dtype\n"}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    sources_selected = []\n    sources_others = []\n    for src in sources:\n        if os.path.splitext(src)[1] == extension:\n            sources_selected.append(src)\n        else:\n            sources_others.append(src)\n    return sources_selected, sources_others\n"}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "\n    import pyarrow.ipc as ipc\n    import pyarrow as pa\n    with open(filename, 'rb') as f:\n"}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "\n\ndef _in_memory_arrow_table_from_buffer(buffer: pa.Buffer) -> pa.Table:\n    stream = pa.BufferReader(buffer)\n"}
{"namespace": "datasets.table._interpolation_search", "completion": "    Raises:\n        `IndexError`: if the array is empty or if the query is outside the array values\n    \"\"\"\n    i, j = 0, len(arr) - 1\n    while i < j and arr[i] <= x < arr[j]:\n        k = i + ((j - i) * (x - arr[i]) // (arr[j] - arr[i]))\n        if arr[k] <= x < arr[k + 1]:\n            return k\n        elif arr[k] < x:\n            i, j = k + 1, j\n"}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    data_dirs_to_ignore_in_path = [part for part in PurePath(matched_rel_path).parent.parts if part.startswith(\"__\")]\n    data_dirs_to_ignore_in_pattern = [part for part in PurePath(pattern).parent.parts if part.startswith(\"__\")]\n    return len(data_dirs_to_ignore_in_path) != len(data_dirs_to_ignore_in_pattern)\n"}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    hidden_directories_in_path = [\n        part for part in PurePath(matched_rel_path).parts if part.startswith(\".\") and not set(part) == {\".\"}\n    ]\n    hidden_directories_in_pattern = [\n        part for part in PurePath(pattern).parts if part.startswith(\".\") and not set(part) == {\".\"}\n    ]\n    return len(hidden_directories_in_path) != len(hidden_directories_in_pattern)\n"}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    n_examples = len(batch[next(iter(batch))])\n    for i in range(n_examples):\n        yield {col: array[i] for col, array in batch.items()}\n"}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    cols = {col: None for example in examples for col in example}\n    # when an example is missing a column, we set the value to None with .get()\n    arrays = [[example.get(col) for example in examples] for col in cols]\n    return dict(zip(cols, arrays))\n"}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        if p is None:\n            while True:\n                yield from (int(i) for i in rng.integers(0, num_sources, size=random_batch_size))\n        else:\n            while True:\n                yield from (int(i) for i in rng.choice(num_sources, size=random_batch_size, p=p))\n"}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from (int(i) for i in rng.integers(0, buffer_size, size=random_batch_size))\n"}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        original_features = self._info.features.copy() if self._info.features else None\n        ds_iterable = self.map(remove_columns=column_names)\n        if original_features is not None:\n            ds_iterable._info.features = original_features.copy()\n            for col, _ in original_features.items():\n                if col in column_names:\n                    del ds_iterable._info.features[col]\n            # check that it's still valid, especially with regard to task templates\n            try:\n                ds_iterable._info.copy()\n            except ValueError:\n                ds_iterable._info.task_templates = None\n\n        return ds_iterable\n"}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_format(type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs)\n        return dataset\n"}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_transform(transform=transform, columns=columns, output_all_columns=output_all_columns)\n        return dataset\n"}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict(\n            {\n                k: dataset.align_labels_with_mapping(label2id=label2id, label_column=label_column)\n                for k, dataset in self.items()\n            }\n        )\n"}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.map(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    drop_last_batch=drop_last_batch,\n                    remove_columns=remove_columns,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )\n"}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )\n"}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return self._indices.num_rows\n        return self._data.num_rows\n"}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if \"://\" in dataset_path:\n        dataset_path = dataset_path.split(\"://\")[1]\n    return dataset_path\n"}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    if fs is not None:\n        protocols = (p,) if isinstance(p := fs.protocol, str) else p\n        if \"file\" not in protocols:\n            return True\n    return False\n"}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    url_bytes = url.encode(\"utf-8\")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\"utf-8\")\n        etag_hash = sha256(etag_bytes)\n        filename += \".\" + etag_hash.hexdigest()\n\n    if url.endswith(\".py\"):\n        filename += \".py\"\n\n    return filename\n"}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    if version.parse(hfh.__version__).release < version.parse(\"0.11.0\").release:\n        # old versions of hfh don't url-encode the file path\n        path = quote(path)\n    return hfh.hf_hub_url(repo_id, path, repo_type=\"dataset\", revision=revision)\n"}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    lists_lengths = {key: len(value) for key, value in gen_kwargs.items() if isinstance(value, list)}\n    if len(set(lists_lengths.values())) > 1:\n        raise RuntimeError(\n            (\n                \"Sharding is ambiguous for this dataset: \"\n                + \"we found several data sources lists of different lengths, and we don't know over which list we should parallelize:\\n\"\n                + \"\\n\".join(f\"\\t- key {key} has length {length}\" for key, length in lists_lengths.items())\n                + \"\\nTo fix this, check the 'gen_kwargs' and make sure to use lists only for data sources, \"\n                + \"and use tuples otherwise. In the end there should only be one single list, or several lists with the same length.\"\n            )\n        )\n    max_length = max(lists_lengths.values(), default=0)\n    return max(1, max_length)\n"}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    shards_indices_per_group = []\n    for group_idx in range(max_num_jobs):\n        num_shards_to_add = num_shards // max_num_jobs + (group_idx < (num_shards % max_num_jobs))\n        if num_shards_to_add == 0:\n            break\n        start = shards_indices_per_group[-1].stop if shards_indices_per_group else 0\n        shard_indices = range(start, start + num_shards_to_add)\n        shards_indices_per_group.append(shard_indices)\n    return shards_indices_per_group\n"}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    original = getattr(obj, attr, None)\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        setattr(obj, attr, original)\n"}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        os.makedirs(output_path, exist_ok=True)\n        tar_file = tarfile.open(input_path)\n        tar_file.extractall(output_path, members=TarExtractor.safemembers(tar_file, output_path))\n        tar_file.close()\n"}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "        magic_number_max_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_max_length)\n        for extractor_format, extractor in cls.extractors.items():\n            if extractor.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n"}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    def _is_dataclass_instance(obj):\n        # https://docs.python.org/3/library/dataclasses.html#dataclasses.is_dataclass\n        return is_dataclass(obj) and not isinstance(obj, type)\n\n    def _asdict_inner(obj):\n        if _is_dataclass_instance(obj):\n            result = {}\n            for f in fields(obj):\n                value = _asdict_inner(getattr(obj, f.name))\n                if not f.init or value != f.default or f.metadata.get(\"include_in_asdict_even_if_is_default\", False):\n                    result[f.name] = value\n            return result\n        elif isinstance(obj, tuple) and hasattr(obj, \"_fields\"):\n            # obj is a namedtuple\n            return type(obj)(*[_asdict_inner(v) for v in obj])\n        elif isinstance(obj, (list, tuple)):\n            # Assume we can create an object of this type by passing in a\n            # generator (which is not true for namedtuples, handled\n            # above).\n            return type(obj)(_asdict_inner(v) for v in obj)\n        elif isinstance(obj, dict):\n            return {_asdict_inner(k): _asdict_inner(v) for k, v in obj.items()}\n        else:\n            return copy.deepcopy(obj)\n\n    if not isinstance(obj, dict) and not _is_dataclass_instance(obj):\n        raise TypeError(f\"{obj} is not a dict or a dataclass\")\n\n    return _asdict_inner(obj)\n"}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        if dataset_card_data.get(cls.FIELD_NAME):\n            metadata_configs = dataset_card_data[cls.FIELD_NAME]\n            if not isinstance(metadata_configs, list):\n                raise ValueError(f\"Expected {cls.FIELD_NAME} to be a list, but got '{metadata_configs}'\")\n            for metadata_config in metadata_configs:\n                if \"config_name\" not in metadata_config:\n                    raise ValueError(\n                        f\"Each config must include `config_name` field with a string name of a config, \"\n                        f\"but got {metadata_config}. \"\n                    )\n                cls._raise_if_data_files_field_not_valid(metadata_config)\n            return cls(\n                {\n                    config[\"config_name\"]: {param: value for param, value in config.items() if param != \"config_name\"}\n                    for config in metadata_configs\n                }\n            )\n        return cls()\n"}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)\n"}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.datapipeline.layer1 import DataPipelineConnection\n    return connect('datapipeline', region_name,\n                   connection_cls=DataPipelineConnection, **kw_params)\n"}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        lines = []\n        kmax = max(len(k) for k in self.settings)\n        for k in sorted(self.settings):\n            v = self.settings[k].value\n            if callable(v):\n                v = \"<{}()>\".format(v.__qualname__)\n            lines.append(\"{k:{kmax}} = {v}\".format(k=k, v=v, kmax=kmax))\n        return \"\\n\".join(lines)\n"}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        try:\n            return self.__getitem__(key)\n        except KeyError:\n            return default\n"}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    from mingus.core.mt_exceptions import RangeError\n    from mingus.core.mt_exceptions import FormatError\n    if note_int not in range(12):\n        raise RangeError(\"int out of bounds (0-11): %d\" % note_int)\n    ns = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n    nf = [\"C\", \"Db\", \"D\", \"Eb\", \"E\", \"F\", \"Gb\", \"G\", \"Ab\", \"A\", \"Bb\", \"B\"]\n    if accidentals == \"#\":\n        return ns[note_int]\n    elif accidentals == \"b\":\n        return nf[note_int]\n    else:\n        raise FormatError(\"'%s' not valid as accidental\" % accidentals)\n"}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.Hostname()\n        schema[\"port\"] = config_lib.Port()\n        schema[\"static_dir\"] = config_lib.Deprecated()\n        schema[\"zeroconf\"] = config_lib.String(optional=True)\n        schema[\"allowed_origins\"] = config_lib.List(\n            optional=True,\n            unique=True,\n            subtype=config_lib.String(transformer=lambda x: x.lower()),\n        )\n        schema[\"csrf_protection\"] = config_lib.Boolean(optional=True)\n        schema[\"default_app\"] = config_lib.String(optional=True)\n        return schema\n"}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    codegen = TrackingCodeGenerator(ast.environment)  # type: ignore\n    codegen.visit(ast)\n    return codegen.undeclared_identifiers\n"}
{"namespace": "pyinfra.operations.files.file", "completion": "    path = _validate_path(path)\n\n    mode = ensure_mode_int(mode)\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if not present:\n        if info:\n            yield StringCommand(\"rm\", \"-f\", QuoteString(path))\n        else:\n            host.noop(\"file {0} does not exist\")\n        return\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        yield StringCommand(\"touch\", QuoteString(path))\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n        if user or group:\n            yield file_utils.chown(path, user, group)\n\n    else:  # update\n        changed = False\n\n        if touch:\n            changed = True\n            yield StringCommand(\"touch\", QuoteString(path))\n\n        # Check mode\n        if mode and (not info or info[\"mode\"] != mode):\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        # Check user/group\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} already exists\".format(path))\n"}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    keywords = (\"drop\", \"shutdown\", \"delete\", \"truncate\", \"alter\")\n    return queries_start_with(queries, keywords)\n"}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        captured = self.command(function)\n        self.default_command = captured.__name__\n        return captured\n"}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(\"path1 must be a prefix of path2\")\n    return path2[len(path1) :]\n"}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg[\"type\"] == \"private\":\n        return False\n    # In a topic narrow\n    elif len(model.narrow) == 2:\n        return False\n    elif model.is_muted_stream(msg[\"stream_id\"]):\n        return True\n    elif model.is_muted_topic(msg[\"stream_id\"], msg[\"subject\"]):\n        return True\n    return False\n"}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        intr = self.get(category_name, discriminator)\n        if intr is None:\n            return\n        L = self._refs.pop(intr, [])\n        for d in L:\n            L2 = self._refs[d]\n            L2.remove(intr)\n        category = self._categories[intr.category_name]\n        del category[intr.discriminator]\n        del category[intr.discriminator_hash]\n"}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        line = 'reporter:status:%s\\n' % (msg,)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n"}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            if not hasattr(callable, '__name__'):\n                raise ValueError(\n                    'missing __name__, must specify \"name\" for property'\n                )\n            name = callable.__name__\n        name = get_callable_name(name)\n        is_data_descriptor = inspect.isdatadescriptor(callable)\n        if reify and is_data_descriptor:\n            raise ValueError('cannot reify a data descriptor')\n        if is_data_descriptor:\n            fn = callable\n        else:\n            wrapped = lambda this: callable(this)\n            wrapped.__name__ = name\n            wrapped.__doc__ = callable.__doc__\n\n            if reify:\n                import pyramid.decorator  # avoid circular import\n\n                fn = pyramid.decorator.reify(wrapped)\n            else:\n                fn = SettableProperty(wrapped)\n\n        return name, fn\n"}
{"namespace": "falcon.request.Request.client_prefers", "completion": "        try:\n            # NOTE(kgriffs): best_match will return '' if no match is found\n            preferred_type = mimeparse.best_match(media_types, self.accept)\n        except ValueError:\n            # Value for the accept header was not formatted correctly\n            preferred_type = ''\n\n        return preferred_type if preferred_type else None\n"}
{"namespace": "falcon.request.Request.content_length", "completion": "        try:\n            value = self.env['CONTENT_LENGTH']\n        except KeyError:\n            return None\n\n        # NOTE(kgriffs): Normalize an empty value to behave as if\n        # the header were not included; wsgiref, at least, inserts\n        # an empty CONTENT_LENGTH value if the request does not\n        # set the header. Gunicorn and uWSGI do not do this, but\n        # others might if they are trying to match wsgiref's\n        # behavior too closely.\n        if not value:\n            return None\n\n        try:\n            value_as_int = int(value)\n        except ValueError:\n            msg = 'The value of the header must be a number.'\n            raise errors.HTTPInvalidHeader(msg, 'Content-Length')\n\n        if value_as_int < 0:\n            msg = 'The value of the header must be a positive number.'\n            raise errors.HTTPInvalidHeader(msg, 'Content-Length')\n\n        return value_as_int\n"}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        self._assert_resolved()\n        return hash(self.discriminator)\n"}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    char_class_bases = {\n        'alpha_lower': 26,\n        'alpha_upper': 26,\n        'alpha': 52,\n        'alphanumeric': 62,\n        'digits': 10,\n        'symbols': 33,\n    }\n    if match['regex_name'] in char_class_bases:\n        return char_class_bases[match['regex_name']] ** len(match['token'])\n    elif match['regex_name'] == 'recent_year':\n        # conservative estimate of year space: num years from REFERENCE_YEAR.\n        # if year is close to REFERENCE_YEAR, estimate a year space of\n        # MIN_YEAR_SPACE.\n        year_space = abs(int(match['regex_match'].group(0)) - REFERENCE_YEAR)\n        year_space = max(year_space, MIN_YEAR_SPACE)\n\n        return year_space\n"}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )\n"}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        arg_names = tuple(self.args) + tuple(getattr(self, 'kwonlyargs', ()))\n        if only_required:\n            defaults_dict = self.get_defaults_dict()\n            arg_names = tuple([an for an in arg_names if an not in defaults_dict])\n        return arg_names\n"}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                log.info('Looking for history log in %s...' % log_dir)\n                # logs aren't always in a subdir named history/\n                yield [log_dir]\n"}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = [cls._format_title(\"Session Renegotiation\")]\n\n        # Client-initiated reneg\n        client_reneg_txt = (\n            \"VULNERABLE - Server honors client-initiated renegotiations\"\n            if result.is_vulnerable_to_client_renegotiation_dos\n            else \"OK - Not vulnerable\"\n        )\n        result_txt.append(cls._format_field(\"Client Renegotiation DoS Attack:\", client_reneg_txt))\n\n        # Secure reneg\n        secure_txt = (\n            \"OK - Supported\"\n            if result.supports_secure_renegotiation\n            else \"VULNERABLE - Secure renegotiation not supported\"\n        )\n        result_txt.append(cls._format_field(\"Secure Renegotiation:\", secure_txt))\n\n        return result_txt\n"}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos)\n"}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        url = near_stream_message_url(server_url, message)\n    else:\n        url = near_pm_message_url(server_url, message)\n    return url\n"}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if self._cookies is None:\n            self._cookies = http_cookies.SimpleCookie()\n\n        self._cookies[name] = ''\n\n        # NOTE(Freezerburn): SimpleCookie apparently special cases the\n        # expires attribute to automatically use strftime and set the\n        # time as a delta from the current time. We use -1 here to\n        # basically tell the browser to immediately expire the cookie,\n        # thus removing it from future request objects.\n        self._cookies[name]['expires'] = -1\n\n        # NOTE(CaselIT): Set SameSite to Lax to avoid setting invalid cookies.\n        # See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie/SameSite#Fixing_common_warnings  # noqa: E501\n        self._cookies[name]['samesite'] = 'Lax'\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]['path'] = path\n"}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        if self.anon:\n            return ['anon']\n        else:\n            return ['s3']\n"}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            # https://github.com/simonw/datasette/issues/448\n            facet_sql = \"\"\"\n                with inner as ({sql}),\n                deduped_array_items as (\n                    select\n                        distinct j.value,\n                        inner.*\n                    from\n                        json_each([inner].{col}) j\n                        join inner\n                )\n                select\n                    value as value,\n                    count(*) as count\n                from\n                    deduped_array_items\n                group by\n                    value\n                order by\n                    count(*) desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(\n                                self.request, {\"_facet_array\": column}\n                            )\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}__arraycontains\", value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}__arraycontains\": value}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}__arraycontains\": value}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": value,\n                            \"label\": value,\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, toggle_path\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out\n"}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(3):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res\n"}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    frequencies: List[Frequency] = []\n    if not result:\n        return None\n\n    d = result.groupdict()\n    resolution_width = int(d[\"resolution_width\"])\n    resolution_height = int(d[\"resolution_height\"])\n    is_high_resolution = d[\"is_high_resolution\"] is not None\n\n    mode: Mode = {\n        \"resolution_width\": resolution_width,\n        \"resolution_height\": resolution_height,\n        \"is_high_resolution\": is_high_resolution,\n        \"frequencies\": frequencies,\n    }\n\n    result = re.finditer(_frequencies_pattern, d[\"rest\"])\n    if not result:\n        return mode\n\n    for match in result:\n        d = match.groupdict()\n        frequency = float(d[\"frequency\"])\n        is_current = len(d[\"star\"].strip()) > 0\n        is_preferred = len(d[\"plus\"].strip()) > 0\n        f: Frequency = {\n            \"frequency\": frequency,\n            \"is_current\": is_current,\n            \"is_preferred\": is_preferred,\n        }\n        mode[\"frequencies\"].append(f)\n    return mode\n"}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        rule = IPPermissions(self)\n        rule.ip_protocol = ip_protocol\n        rule.from_port = from_port\n        rule.to_port = to_port\n        self.rules.append(rule)\n        rule.add_grant(\n            src_group_name,\n            src_group_owner_id,\n            cidr_ip,\n            src_group_group_id,\n            dry_run=dry_run\n        )\n"}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    from ..._compat import to_unicode\n    return to_unicode(object).lower()\n"}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        shells = self.find_all_shells()\n\n        shell = None\n        user_shell = self.args.python_shell.lower()\n\n        if not user_shell:\n            preferred_shells = self.preferred_shells\n            if not preferred_shells:\n                # by default prioritize all shells above python\n                preferred_shells = [k for k in shells.keys() if k != 'python']\n            max_weight = len(preferred_shells)\n\n            def order(x):\n                # invert weight to reverse sort the list\n                # (closer to the front is higher priority)\n                try:\n                    return preferred_shells.index(x[0].lower()) - max_weight\n                except ValueError:\n                    return 1\n\n            sorted_shells = sorted(shells.items(), key=order)\n\n            if len(sorted_shells) > 0:\n                shell = sorted_shells[0][1]\n\n        else:\n            runner = shells.get(user_shell)\n\n            if runner is not None:\n                shell = runner\n\n            if shell is None:\n                raise ValueError(\n                    'could not find a shell named \"%s\"' % user_shell\n                )\n\n        if shell is None:\n            # should never happen, but just incase entry points are borked\n            shell = self.default_runner\n\n        return shell\n"}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "        self.get_hadoop_version()\n\n        if self._has_hadoop_streaming_steps():\n            self.get_hadoop_streaming_jar()\n\n        if self._has_spark_steps():\n            self.get_spark_submit_bin()\n"}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    pieces = []\n    for piece in template.split(\"/\"):\n        if (\n            os.path.sep in piece\n            or (os.path.altsep and os.path.altsep in piece)\n            or piece == os.path.pardir\n        ):\n            raise TemplateNotFound(template)\n        elif piece and piece != \".\":\n            pieces.append(piece)\n    return pieces\n"}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n"}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "        from falcon.constants import MEDIA_JSON\n        obj = self.to_dict(OrderedDict)\n        if handler is None:\n            handler = _DEFAULT_JSON_HANDLER\n        return handler.serialize(obj, MEDIA_JSON)\n"}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(\n            singular, plural, n, domain=domain, mapping=mapping\n        )\n"}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        msg = (\"Creating of DAWGs with DAWG-Python is \"\n               \"not supported; install 'dawg' package.\")\n        raise NotImplementedError(msg)\n"}
{"namespace": "boto.s3.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.s3.connection import S3Connection\n    if 'host' in kw_params:\n        host = kw_params.pop('host')\n        if host not in ['', None]:\n            region = S3RegionInfo(\n                name='custom',\n                endpoint=host,\n                connection_cls=S3Connection\n            )\n            return region.connect(**kw_params)\n\n    return connect('s3', region_name, region_cls=S3RegionInfo,\n                   connection_cls=S3Connection, **kw_params)\n"}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        if self.conn is not None:\n            self.conn.commit(blocking)\n"}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(\n            tname,\n            col.name,\n            schema=schema,\n            _reverse=AddColumnOp.from_column_and_tablename(schema, tname, col),\n        )\n"}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        info = self.describe()\n        return info['Table'].get('ItemCount', 0)\n"}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key in key_fields:\n            key_data[key] = self[key]\n\n        return key_data\n"}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        domain = getattr(translations, 'domain', self.DEFAULT_DOMAIN)\n        if domain == self.DEFAULT_DOMAIN and self.plural is DEFAULT_PLURAL:\n            self.plural = translations.plural\n\n        if merge and domain == self.domain:\n            return self.merge(translations)\n\n        existing = self._domains.get(domain)\n        if merge and existing is not None:\n            existing.merge(translations)\n        else:\n            translations.add_fallback(self)\n            self._domains[domain] = translations\n\n        return self\n"}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n"}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )\n"}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        if filter_kwargs is None:\n            return\n\n        filters = {}\n\n        for field_and_op, value in filter_kwargs.items():\n            field_bits = field_and_op.split('__')\n            fieldname = '__'.join(field_bits[:-1])\n\n            try:\n                op = using[field_bits[-1]]\n            except KeyError:\n                raise exceptions.UnknownFilterTypeError(\n                    \"Operator '%s' from '%s' is not recognized.\" % (\n                        field_bits[-1],\n                        field_and_op\n                    )\n                )\n\n            lookup = {\n                'AttributeValueList': [],\n                'ComparisonOperator': op,\n            }\n\n            # Special-case the ``NULL/NOT_NULL`` case.\n            if field_bits[-1] == 'null':\n                del lookup['AttributeValueList']\n\n                if value is False:\n                    lookup['ComparisonOperator'] = 'NOT_NULL'\n                else:\n                    lookup['ComparisonOperator'] = 'NULL'\n            # Special-case the ``BETWEEN`` case.\n            elif field_bits[-1] == 'between':\n                if len(value) == 2 and isinstance(value, (list, tuple)):\n                    lookup['AttributeValueList'].append(\n                        self._dynamizer.encode(value[0])\n                    )\n                    lookup['AttributeValueList'].append(\n                        self._dynamizer.encode(value[1])\n                    )\n            # Special-case the ``IN`` case\n            elif field_bits[-1] == 'in':\n                for val in value:\n                    lookup['AttributeValueList'].append(self._dynamizer.encode(val))\n            else:\n                # Fix up the value for encoding, because it was built to only work\n                # with ``set``s.\n                if isinstance(value, (list, tuple)):\n                    value = set(value)\n                lookup['AttributeValueList'].append(\n                    self._dynamizer.encode(value)\n                )\n\n            # Finally, insert it into the filters.\n            filters[fieldname] = lookup\n\n        return filters\n"}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('Closing WAL with uncommitted data, discarding it')\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        for page, page_start in self._committed_pages.items():\n            page_data = read_from_file(\n                self._fd,\n                page_start,\n                page_start + self._page_size\n            )\n            yield page, page_data\n\n        self._fd.close()\n        os.unlink(self.filename)\n        if self._dir_fd is not None:\n            os.fsync(self._dir_fd)\n            os.close(self._dir_fd)\n"}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n"}
{"namespace": "pylatex.utils.dumps_list", "completion": "    strings = (\n        _latex_item_to_string(i, escape=escape, as_content=as_content) for i in l\n    )\n\n    if mapper is not None:\n        if not isinstance(mapper, list):\n            mapper = [mapper]\n\n        for m in mapper:\n            strings = [m(s) for s in strings]\n        strings = [_latex_item_to_string(s) for s in strings]\n\n    return NoEscape(token.join(strings))\n"}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    word = match['token']\n\n    if ALL_LOWER.match(word) or word.lower() == word:\n        return 1\n\n    for regex in [START_UPPER, END_UPPER, ALL_UPPER]:\n        if regex.match(word):\n            return 2\n\n    U = sum(1 for c in word if c.isupper())\n    L = sum(1 for c in word if c.islower())\n    variations = 0\n    for i in range(1, min(U, L) + 1):\n        variations += nCk(U + L, i)\n\n    return variations\n"}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    return is_perfect_consonant(\n        note1, note2, include_fourths\n    ) or is_imperfect_consonant(note1, note2)\n"}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins = []\n    plugin_to_distinfo = dict(pm.list_plugin_distinfo())\n    for plugin in pm.get_plugins():\n        static_path = None\n        templates_path = None\n        if plugin.__name__ not in DEFAULT_PLUGINS:\n            try:\n                if pkg_resources.resource_isdir(plugin.__name__, \"static\"):\n                    static_path = pkg_resources.resource_filename(\n                        plugin.__name__, \"static\"\n                    )\n                if pkg_resources.resource_isdir(plugin.__name__, \"templates\"):\n                    templates_path = pkg_resources.resource_filename(\n                        plugin.__name__, \"templates\"\n                    )\n            except (KeyError, ImportError):\n                # Caused by --plugins_dir= plugins - KeyError/ImportError thrown in Py3.5\n                pass\n        plugin_info = {\n            \"name\": plugin.__name__,\n            \"static_path\": static_path,\n            \"templates_path\": templates_path,\n            \"hooks\": [h.name for h in pm.get_hookcallers(plugin)],\n        }\n        distinfo = plugin_to_distinfo.get(plugin)\n        if distinfo:\n            plugin_info[\"version\"] = distinfo.version\n            plugin_info[\"name\"] = distinfo.project_name\n        plugins.append(plugin_info)\n    return plugins\n"}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    def yield_record_strings(lines):\n        record_lines = []\n        start_line = 0\n\n        for line_num, line in enumerate(lines):\n            record_lines.append(line)\n            if line.endswith(' .\\n'):\n                yield start_line, len(record_lines), ''.join(record_lines)\n                record_lines = []\n                start_line = line_num + 1\n\n    for start_line, num_lines, record_str in yield_record_strings(lines):\n        record_match = _PRE_YARN_HISTORY_RECORD.match(record_str)\n\n        if not record_match:\n            continue\n\n        record_type = record_match.group('type')\n        key_pairs = record_match.group('key_pairs')\n\n        fields = {}\n        for m in _PRE_YARN_HISTORY_KEY_PAIR.finditer(key_pairs):\n            key = m.group('key')\n            value = _pre_yarn_history_unescape(m.group('escaped_value'))\n\n            fields[key] = value\n\n        yield dict(\n            fields=fields,\n            num_lines=num_lines,\n            start_line=start_line,\n            type=record_type,\n        )\n"}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    chunk_count = int(math.ceil(len(bytestring) / float(chunk_size)))\n    hashes = []\n    for i in range(chunk_count):\n        start = i * chunk_size\n        end = (i + 1) * chunk_size\n        hashes.append(hashlib.sha256(bytestring[start:end]).digest())\n    if not hashes:\n        return [hashlib.sha256(b'').digest()]\n    return hashes\n"}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    def timestamp():\n        try:\n            return time.monotonic()\n        except AttributeError:\n            return time.time()  # Python 2.\n    try:\n        value = package_name_cache[dependency]\n        if value[0] + 600.0 > timestamp() and use_cache:\n            return value[1]\n    except KeyError:\n        pass\n    result = _extract_info_from_package(dependency, extract_type=\"name\")\n    package_name_cache[dependency] = (timestamp(), result)\n    return result\n"}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not ACCEPTABLE_URI_SCHEMES:\n        return _urljoin(base, rel or '')\n    if not base:\n        return rel or ''\n    if not rel:\n        try:\n            scheme = urllib.parse.urlparse(base)[0]\n        except ValueError:\n            return ''\n        if not scheme or scheme in ACCEPTABLE_URI_SCHEMES:\n            return base\n        return ''\n    uri = _urljoin(base, rel)\n    if uri.strip().split(':', 1)[0] not in ACCEPTABLE_URI_SCHEMES:\n        return ''\n    return uri\n"}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )\n"}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        lines = [u'Traceback (most recent call last):']\n\n        for frame in self.frames:\n            lines.append(u'  File \"%s\", line %s, in %s' % (frame['filepath'],\n                                                           frame['lineno'],\n                                                           frame['funcname']))\n            source_line = frame.get('source_line')\n            if source_line:\n                lines.append(u'    %s' % (source_line,))\n        if self.exc_msg:\n            lines.append(u'%s: %s' % (self.exc_type, self.exc_msg))\n        else:\n            lines.append(u'%s' % (self.exc_type,))\n        return u'\\n'.join(lines)\n"}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])\n"}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        value = etag_str\n\n        is_weak = False\n        if value.startswith(('W/', 'w/')):\n            is_weak = True\n            value = value[2:]\n\n        # NOTE(kgriffs): We allow for an unquoted entity-tag just in case,\n        #   although it has been non-standard to do so since at least 1999\n        #   with the advent of RFC 2616.\n        if value[:1] == value[-1:] == '\"':\n            value = value[1:-1]\n\n        t = cls(value)\n        t.is_weak = is_weak\n\n        return t\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        n_val = sum(values.values())\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / n_val\n\n    tot_val = sum(value_counts.values())\n    for value, count in value_counts.items():\n        value_probs[value] = count / tot_val\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm\n"}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        if name in self._fs_names:\n            raise ValueError('name %r is already taken' % name)\n\n        setattr(self, name, fs)\n        self._fs_names.append(name)\n\n        if disable_if:\n            self._disable_if[name] = disable_if\n"}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        request.session[self.userid_key] = userid\n        return []\n"}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()\n"}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    errors = []\n    for exc, fn in app._error_handlers.items():\n        source_info, name = _get_source_info_and_name(fn)\n        info = ErrorHandlerInfo(exc.__name__, name, source_info, _is_internal(fn))\n        errors.append(info)\n    return errors\n"}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            return name\n        if parent is not None:\n            name = self.join_path(name, parent)\n\n        return self._load_template(name, globals)\n"}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\n                self.homebrew_formula_name, installed=True\n            )\n            is not None\n        )\n"}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        diff = self.__int__() - 57\n        return 2 ** (diff / 12.0) * standard_pitch\n"}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        gs = self.make_globals(globals)\n        cls = template_class or self.template_class\n        return cls.from_code(self, self.compile(source), gs, None)\n"}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        if self.closed:\n            raise UserCritical(msg='attempt to transfer wal after closing',\n                               hint='report a bug')\n\n        g = gevent.Greenlet(self.transferer, segment)\n        g.link(self._complete_execution)\n        self.greenlets.add(g)\n\n        # Increment .expect before starting the greenlet, or else a\n        # very unlucky .join could be fooled as to when pool is\n        # complete.\n        self.expect += 1\n\n        g.start()\n"}
{"namespace": "falcon.response.Response.set_header", "completion": "        value = str(value)\n\n        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to set cookies')\n\n        self._headers[name] = value\n"}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        if len(self.command_tokens) < 3:\n            self.print_wrapper.error_log(\n                \"Missing some arguments\\nUsage: \" +\n                'unhide <unit-type> \"<unit-name>\"'\n            )\n            return\n\n        unit_type = CommandStrategy.get_unit_type_from_str(self.command_tokens[1])\n        if unit_type is None:\n            self.print_wrapper.error_log(\n                \"Unknown unit type: '\" + str(self.command_tokens[1]) + \"'.\"\n            )\n            return\n\n        unit_regex = self.get_regex_name(self.command_tokens[2])\n        if unit_regex is None:\n            try:\n                [unit_name, variation_name] = \\\n                    CommandStrategy.split_exact_unit_name(self.command_tokens[2])\n            except SyntaxError:\n                self.print_wrapper.error_log(\n                    \"Unit identifier couldn't be interpreted. \" + \\\n                    \"Did you mean to escape some hashtags '#'?\"\n                )\n                return\n            self.execute_on_unit(unit_type, unit_name, variation_name)\n        else:\n            unit_names = [\n                unit_name\n                for unit_name in HideCommand.stored_units[unit_type.name]\n                if unit_regex.match(unit_name)\n            ]\n            if len(unit_names) == 0:\n                self.print_wrapper.write(\"No \" + unit_type.name + \" matched.\")\n\n            for unit_name in unit_names:\n                self.execute_on_unit(unit_type, unit_name)\n"}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    val = note_to_int(note[0])\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n        else:\n            raise NoteFormatError(\"Unknown note format '%s'\" % note)\n    if val >= note_to_int(note[0]):\n        return int_to_note(val % 12)\n    else:\n        return int_to_note(val % 12, \"b\")\n"}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n"}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        try:\n            oui_prefix = mac.upper().replace(':', '')[0:6]\n            if oui_prefix in self.db:\n                return self.db[oui_prefix]\n        except Exception:\n            pass\n\n        return ''\n"}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        if permissions:\n            assert (\n                not action and not resource\n            ), \"Can't use action= or resource= with permissions=\"\n        else:\n            permissions = [(action, resource)]\n        try:\n            await self.ensure_permissions(actor, permissions)\n        except Forbidden:\n            return False, False\n        # User can see it, but can the anonymous user see it?\n        try:\n            await self.ensure_permissions(None, permissions)\n        except Forbidden:\n            # It's visible but private\n            return True, True\n        # It's visible to everyone\n        return True, False\n"}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "    if full_text.startswith('\\\\i '):\n        return (Path(),)\n\n    # This is a temporary hack; the exception handling\n    # here should be removed once sqlparse has been fixed\n    try:\n        stmt = SqlStatement(full_text, text_before_cursor)\n    except (TypeError, AttributeError):\n        return []\n\n    # Check for special commands and handle those separately\n    if stmt.parsed:\n        # Be careful here because trivial whitespace is parsed as a\n        # statement, but the statement won't have a first token\n        tok1 = stmt.parsed.token_first()\n        if tok1 and tok1.value == '\\\\':\n            text = stmt.text_before_cursor + stmt.word_before_cursor\n            return suggest_special(text)\n\n    return suggest_based_on_last_token(stmt.last_token, stmt)\n"}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    for d in config_string.split():\n        p = provider_for_descriptor_and_netcode(d, netcode)\n        if p:\n            providers.append(p)\n        else:\n            warnings.warn(\"can't parse provider %s in config string\" % d)\n    return providers\n"}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    from mopidy import exceptions\n    logger.debug(\"Validating extension: %s\", data.extension.ext_name)\n\n    if data.extension.ext_name != data.entry_point.name:\n        logger.warning(\n            \"Disabled extension %(ep)s: entry point name (%(ep)s) \"\n            \"does not match extension name (%(ext)s)\",\n            {\"ep\": data.entry_point.name, \"ext\": data.extension.ext_name},\n        )\n        return False\n\n    try:\n        data.entry_point.require()\n    except pkg_resources.DistributionNotFound as exc:\n        logger.info(\n            \"Disabled extension %s: Dependency %s not found\",\n            data.extension.ext_name,\n            exc,\n        )\n        return False\n    except pkg_resources.VersionConflict as exc:\n        if len(exc.args) == 2:\n            found, required = exc.args\n            logger.info(\n                \"Disabled extension %s: %s required, but found %s at %s\",\n                data.extension.ext_name,\n                required,\n                found,\n                found.location,\n            )\n        else:\n            logger.info(\n                \"Disabled extension %s: %s\", data.extension.ext_name, exc\n            )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except exceptions.ExtensionError as exc:\n        logger.info(\"Disabled extension %s: %s\", data.extension.ext_name, exc)\n        return False\n    except Exception:\n        logger.exception(\n            \"Validating extension %s failed with an exception.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    if not data.config_schema:\n        logger.error(\n            \"Extension %s does not have a config schema, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n    elif not isinstance(data.config_schema.get(\"enabled\"), config_lib.Boolean):\n        logger.error(\n            'Extension %s does not have the required \"enabled\" config'\n            \" option, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    for key, value in data.config_schema.items():\n        if not isinstance(value, config_lib.ConfigValue):\n            logger.error(\n                \"Extension %s config schema contains an invalid value\"\n                ' for the option \"%s\", disabling.',\n                data.extension.ext_name,\n                key,\n            )\n            return False\n\n    if not data.config_defaults:\n        logger.error(\n            \"Extension %s does not have a default config, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True\n"}
{"namespace": "mopidy.config.load", "completion": "    from mopidy.config import keyring\n    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    defaults.extend(ext_defaults)\n    raw_config = _load(files, defaults, keyring.fetch() + (overrides or []))\n\n    schemas = _schemas[:]\n    schemas.extend(ext_schemas)\n    return _validate(raw_config, schemas)\n"}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    from .log4j import _parse_hadoop_log4j_records\n    return _parse_task_syslog_records(_parse_hadoop_log4j_records(lines))\n"}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            return math.ceil(len(nvidia_gpus) * workers_per_resource)\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                if isinstance(workers_per_resource, float):\n                    raise ValueError(\n                        \"Fractional CPU multi threading support is not yet supported.\"\n                    )\n                return workers_per_resource\n\n            return math.ceil(cpus) * workers_per_resource\n\n        # this should not be reached by user since we always read system resource as default\n        raise ValueError(\n            f\"No known supported resource available for {runnable_class}. Please check your resource request. \"\n            \"Leaving it blank will allow BentoML to use system resources.\"\n        )\n"}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n"}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return (\n            datetime.datetime.strptime(s, ISO8601_DATE_FORMAT)\n            .replace(tzinfo=datetime.timezone.utc)\n            .date()\n        )\n    except (TypeError, ValueError):\n        return s\n"}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    p = parse(sql)[0]\n\n    # Make sure the first meaningful token is \"WITH\" which is necessary to\n    # define CTEs\n    idx, tok = p.token_next(-1, skip_ws=True, skip_cm=True)\n    if not (tok and tok.ttype == CTE):\n        return [], sql\n\n    # Get the next (meaningful) token, which should be the first CTE\n    idx, tok = p.token_next(idx)\n    if not tok:\n        return ([], '')\n    start_pos = token_start_pos(p.tokens, idx)\n    ctes = []\n\n    if isinstance(tok, IdentifierList):\n        # Multiple ctes\n        for t in tok.get_identifiers():\n            cte_start_offset = token_start_pos(tok.tokens, tok.token_index(t))\n            cte = get_cte_from_token(t, start_pos + cte_start_offset)\n            if not cte:\n                continue\n            ctes.append(cte)\n    elif isinstance(tok, Identifier):\n        # A single CTE\n        cte = get_cte_from_token(tok, start_pos)\n        if cte:\n            ctes.append(cte)\n\n    idx = p.token_index(tok) + 1\n\n    # Collapse everything after the ctes into a remainder query\n    remainder = u''.join(str(tok) for tok in p.tokens[idx:])\n\n    return ctes, remainder\n"}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, case_sensitive=False) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res, re.IGNORECASE)\n    return re_pat.match(name) is not None\n"}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        return cursor.fetchone() is not None\n"}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                ret = super(LRI, self).pop(key)\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                ret = default\n            else:\n                self._remove_from_ll(key)\n            return ret\n"}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "    from awesome_autodl.utils import load_yaml, dump_yaml\n    from awesome_autodl.data_cls import AutoDLpaper\n\n    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n    for topic, xpath in topic2path.items():\n        if not xpath.exists():\n            ValueError(f\"Can not find {topic} at {xpath}.\")\n        papers = []\n        raw_data = load_yaml(xpath)\n        assert isinstance(\n            raw_data, (list, tuple)\n        ), f\"invalid type of raw data: {type(raw_data)}\"\n        for per_data in raw_data:\n            papers.append(AutoDLpaper(per_data))\n        topic2papers[topic] = papers\n        print(f\"Load {topic} completed with {len(papers)} papers.\")\n    return topic2papers\n"}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        cn = self.__class__.__name__\n        return ('%s(user=%r, group=%r, other=%r)'\n                % (cn, self.user, self.group, self.other))\n"}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n        assert (stream_id is None and stream_name is not None) or (\n            stream_id is not None and stream_name is None\n        )\n\n        model = self.model\n        # Validate stream ID and name.\n        if (stream_id and not model.is_user_subscribed_to_stream(stream_id)) or (\n            stream_name and not model.is_valid_stream(stream_name)\n        ):\n            # TODO: Narrow to the concerned stream in a 'preview' mode or\n            # report whether the stream id is invalid instead.\n            return \"The stream seems to be either unknown or unsubscribed\"\n\n        # Patch the optional value.\n        if not stream_id:\n            stream_id = cast(int, model.stream_id_from_name(stream_name))\n            parsed_link[\"stream\"][\"stream_id\"] = stream_id\n        else:\n            stream_name = cast(str, model.stream_dict[stream_id][\"name\"])\n            parsed_link[\"stream\"][\"stream_name\"] = stream_name\n\n        return \"\"\n"}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\"):\n            return name\n        return name.split(\".\")[0]\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens and (start_token is None or end_token is None):\n        raise MsticpyException(\n            \"start_token and end_token should not be set to None when \"\n            \"use_start_end_tokens is set to True\"\n        )\n\n    likelihoods = []\n    sess = session.copy()\n    if use_start_end_tokens and end_token:\n        sess += [Cmd(name=str(end_token), params={})]\n    end = len(sess) - window_len\n    for i in range(end + 1):\n        window = sess[i : i + window_len]  # noqa E203\n        use_start = use_start_end_tokens if i == 0 else False\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=use_start,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            k = window_len\n            lik = lik ** (1 / k)\n        likelihoods.append(lik)\n\n    return likelihoods\n"}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        version = self.get_hadoop_version()\n\n        # use -p on Hadoop 2 (see #991, #845)\n        if uses_yarn(version):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)\n"}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        batch_dict = {}\n        key_list = []\n        for key in self.keys:\n            if isinstance(key, tuple):\n                hash_key, range_key = key\n            else:\n                hash_key = key\n                range_key = None\n            k = self.table.layer2.build_key_from_values(self.table.schema,\n                                                        hash_key, range_key)\n            key_list.append(k)\n        batch_dict['Keys'] = key_list\n        if self.attributes_to_get:\n            batch_dict['AttributesToGet'] = self.attributes_to_get\n        if self.consistent_read:\n            batch_dict['ConsistentRead'] = True\n        else:\n            batch_dict['ConsistentRead'] = False\n        return batch_dict\n"}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if dict.__contains__(self, k):\n            return dict.__getitem__(self, k)\n        else:\n            return self.fallback.get(k, d)\n"}
{"namespace": "boto.vpc.connect_to_region", "completion": "    from boto.regioninfo import connect\n    return connect('ec2', region_name, connection_cls=VPCConnection,\n                   **kw_params)\n"}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        from mrjob.step import _JOB_STEP_FUNC_PARAMS\n        kwargs = dict(\n            (func_name, getattr(self, func_name))\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\n            if (_im_func(getattr(self, func_name)) is not\n                _im_func(getattr(MRJob, func_name))))\n\n        # special case for spark()\n        # TODO: support jobconf as well\n        if 'spark' in kwargs:\n            if sorted(kwargs) != ['spark']:\n                raise ValueError(\n                    \"Can't mix spark() and streaming functions\")\n            return [SparkStep(\n                spark=kwargs['spark'],\n                spark_args=self.spark_args())]\n\n        # MRStep takes commands as strings, but the user defines them in the\n        # class as functions that return strings, so call the functions.\n        updates = {}\n        for k, v in kwargs.items():\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\n                updates[k] = v()\n\n        kwargs.update(updates)\n\n        if kwargs:\n            return [MRStep(**kwargs)]\n        else:\n            return []\n"}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        new_object = self.request.validated[\"body\"].get(\"data\", {})\n\n        existing = None\n        # If id was specified, then add it to posted body and look-up\n        # the existing object.\n        if self.object_id is not None:\n            new_object[self.model.id_field] = self.object_id\n            try:\n                existing = self._get_object_or_404(self.object_id)\n            except HTTPNotFound:\n                pass\n\n        self._raise_412_if_modified(obj=existing)\n\n        if existing:\n            obj = existing\n            action = ACTIONS.READ\n        else:\n            new_object = self.process_object(new_object)\n            obj = self.model.create_object(new_object)\n            self.request.response.status_code = 201\n            action = ACTIONS.CREATE\n\n        timestamp = obj[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(obj, action=action)\n"}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n"}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        cn = self.__class__.__name__\n        if self.typed or not self.scoped:\n            return (\"%s(func=%r, scoped=%r, typed=%r)\"\n                    % (cn, self.func, self.scoped, self.typed))\n        return \"%s(func=%r)\" % (cn, self.func)\n"}
{"namespace": "mingus.core.chords.determine", "completion": "    if chord == []:\n        return []\n    elif len(chord) == 1:\n        return chord\n    elif len(chord) == 2:\n        return [intervals.determine(chord[0], chord[1])]\n    elif len(chord) == 3:\n        return determine_triad(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 4:\n        return determine_seventh(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 5:\n        return determine_extended_chord5(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 6:\n        return determine_extended_chord6(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 7:\n        return determine_extended_chord7(chord, shorthand, no_inversions, no_polychords)\n    else:\n        return determine_polychords(chord, shorthand)\n"}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.split(\"\\n\")\n    # find indentation by first line\n    indent = \"\"\n    for line in lines:\n        if is_empty_or_comment(line):\n            continue\n        else:\n            indent = re.match(r\"^\\s*\", line).group()\n            break\n\n    out_lines = [dedent_line(line, indent) for line in lines]\n    return \"\\n\".join(out_lines)\n"}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    key, old = _server_keys.get(fingerprint, [None, None])\n    if (not key) or (old and not use_old):\n        return None\n\n    # len(sha1.digest) is always 20, so we're left with 255 - 20 - x padding\n    to_encrypt = sha1(data).digest() + data + os.urandom(235 - len(data))\n\n    # rsa module rsa.encrypt adds 11 bits for padding which we don't want\n    # rsa module uses rsa.transform.bytes2int(to_encrypt), easier way:\n    payload = int.from_bytes(to_encrypt, 'big')\n    encrypted = rsa.core.encrypt_int(payload, key.e, key.n)\n    # rsa module uses transform.int2bytes(encrypted, keylength), easier:\n    block = encrypted.to_bytes(256, 'big')\n    return block\n"}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if isinstance(scope, (set, tuple, list)):\n        return \" \".join([to_unicode(s) for s in scope])\n    if scope is None:\n        return scope\n    return to_unicode(scope)\n"}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        self.closed = True\n\n        while self.expect > 0:\n            val = self.wait_change.get()\n            self.expect -= 1\n\n            if val is not None:\n                # Wait a while for all running greenlets to exit, and\n                # then attempt to force them to exit so join()\n                # terminates in a reasonable amount of time.\n                gevent.joinall(list(self.greenlets), timeout=30)\n                gevent.killall(list(self.greenlets), block=True, timeout=30)\n                raise val\n"}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        results = load_opts_from_mrjob_conf(runner_alias)\n    else:\n        # don't include conf files that were loaded earlier in conf_paths\n        already_loaded = []\n\n        # load configs in reversed order so that order of conf paths takes\n        # precedence over inheritance\n        results = []\n\n        for path in reversed(conf_paths):\n            results = load_opts_from_mrjob_conf(\n                runner_alias, path, already_loaded=already_loaded) + results\n\n    if runner_alias and not any(conf for path, conf in results):\n        log.warning('No configs specified for %s runner' % runner_alias)\n\n    return results\n"}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        rs = self.connection.get_all_network_interfaces(\n            [self.id],\n            dry_run=dry_run\n        )\n        if len(rs) > 0:\n            self._update(rs[0])\n        elif validate:\n            raise ValueError('%s is not a valid ENI ID' % self.id)\n        return self.status\n"}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "    if constraint.name is None:\n        return None\n    assert dialect is not None\n    if sqla_14:\n        # for SQLAlchemy 1.4 we would like to have the option to expand\n        # the use of \"deferred\" names for constraints as well as to have\n        # some flexibility with \"None\" name and similar; make use of new\n        # SQLAlchemy API to return what would be the final compiled form of\n        # the name for this dialect.\n        return dialect.identifier_preparer.format_constraint(\n            constraint, _alembic_quote=False\n        )\n    else:\n        # prior to SQLAlchemy 1.4, work around quoting logic to get at the\n        # final compiled name without quotes.\n        if hasattr(constraint.name, \"quote\"):\n            # might be quoted_name, might be truncated_name, keep it the\n            # same\n            quoted_name_cls: type = type(constraint.name)\n        else:\n            quoted_name_cls = quoted_name\n\n        new_name = quoted_name_cls(str(constraint.name), quote=False)\n        constraint = constraint.__class__(name=new_name)\n\n        if isinstance(constraint, schema.Index):\n            # name should not be quoted.\n            d = dialect.ddl_compiler(dialect, None)  # type: ignore[arg-type]\n            return d._prepared_index_name(  # type: ignore[attr-defined]\n                constraint\n            )\n        else:\n            # name should not be quoted.\n            return dialect.identifier_preparer.format_constraint(constraint)\n"}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        values = category.values()\n        values = sorted(set(values), key=sort_key)\n        return [\n            {'introspectable': intr, 'related': self.related(intr)}\n            for intr in values\n        ]\n"}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return str(d.date())\n    elif isinstance(d, datetime.date):\n        return str(d)\n    elif isinstance(d, str):\n        return d\n"}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "    def fake(cls, path_with_query_string, method=\"GET\", scheme=\"http\", url_vars=None):\n        \"\"\"Useful for constructing Request objects for tests\"\"\"\n        path, _, query_string = path_with_query_string.partition(\"?\")\n        scope = {\n            \"http_version\": \"1.1\",\n            \"method\": method,\n            \"path\": path,\n            \"raw_path\": path_with_query_string.encode(\"latin-1\"),\n            \"query_string\": query_string.encode(\"latin-1\"),\n            \"scheme\": scheme,\n            \"type\": \"http\",\n        }\n        if url_vars:\n"}
{"namespace": "asyncssh.mac.get_mac", "completion": "    handler, hash_size, args = _mac_handler[mac_alg]\n    return handler(key, hash_size, *args)\n"}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        obj = self._get_object_or_404(self.object_id)\n        timestamp = obj[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified(obj)\n        self._raise_412_if_modified(obj)\n\n        partial_fields = self._extract_partial_fields()\n        if partial_fields:\n            obj = dict_subset(obj, partial_fields)\n\n        return self.postprocess(obj)\n"}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if any(self._called.values()):\n            return f\"{self.title} (id: {self.pageid}, ns: {self.ns})\"\n        return f\"{self.title} (id: ??, ns: {self.ns})\"\n"}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        from .lib import plugin_parser_mod_list\n        from .lib import streaming_parser_mod_list\n        from .lib import parser_mod_list\n        from .lib import standard_parser_mod_list\n        return {\n            'name': 'jc',\n            'version': info.version,\n            'description': info.description,\n            'author': info.author,\n            'author_email': info.author_email,\n            'website': info.website,\n            'copyright': info.copyright,\n            'license': info.license,\n            'python_version': '.'.join((str(sys.version_info.major), str(sys.version_info.minor), str(sys.version_info.micro))),\n            'python_path': sys.executable,\n            'parser_count': len(parser_mod_list(show_hidden=True, show_deprecated=True)),\n            'standard_parser_count': len(standard_parser_mod_list(show_hidden=True, show_deprecated=True)),\n            'streaming_parser_count': len(streaming_parser_mod_list(show_hidden=True, show_deprecated=True)),\n            'plugin_parser_count': len(plugin_parser_mod_list(show_hidden=True, show_deprecated=True)),\n            'parsers': all_parser_info(show_hidden=True, show_deprecated=True)\n        }\n"}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        locale_name = negotiate_locale_name(self)\n        return locale_name\n"}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        if include_static is True:\n            return self.routelist + self.static_routes\n\n        return self.routelist\n"}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        (is_empty_value, data) = self.validate_empty_values(data)\n        if is_empty_value:\n            return data\n        value = self.to_internal_value(data)\n        self.run_validators(value)\n        return value\n"}
{"namespace": "boto.ec2.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.regioninfo import RegionInfo\n    if 'region' in kw_params and isinstance(kw_params['region'], RegionInfo)\\\n       and region_name == kw_params['region'].name:\n        return EC2Connection(**kw_params)\n\n    return connect('ec2', region_name,\n                   connection_cls=EC2Connection, **kw_params)\n"}
{"namespace": "pyramid.request.Request.session", "completion": "        from pyramid.interfaces import ISessionFactory\n        factory = self.registry.queryUtility(ISessionFactory)\n        if factory is None:\n            raise AttributeError(\n                'No session factory registered '\n                '(see the Sessions chapter of the Pyramid documentation)'\n            )\n        return factory(self)\n"}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages:\n            if not self.use_uid:\n                raise ValueError(\"cannot EXPUNGE by ID when not using uids\")\n            return self._command_and_check(\n                \"EXPUNGE\", join_message_ids(messages), uid=True\n            )\n        tag = self._imap._command(\"EXPUNGE\")\n        return self._consume_until_tagged_response(tag, \"EXPUNGE\")\n"}
{"namespace": "mingus.core.progressions.substitute", "completion": "    res = []\n    simple_substitutions = [\n        (\"I\", \"III\"),\n        (\"I\", \"VI\"),\n        (\"IV\", \"II\"),\n        (\"IV\", \"VI\"),\n        (\"V\", \"VII\"),\n        (\"V\", \"VIIdim7\"),\n        (\"V\", \"IIdim7\"),\n        (\"V\", \"IVdim7\"),\n        (\"V\", \"bVIIdim7\"),\n    ]\n    p = progression[substitute_index]\n    (roman, acc, suff) = parse_string(p)\n\n    # Do the simple harmonic substitutions\n    if suff == \"\" or suff == \"7\":\n        for subs in simple_substitutions:\n            r = None\n            if roman == subs[0]:\n                r = subs[1]\n            elif roman == subs[1]:\n                r = subs[0]\n            if r != None:\n                res.append(tuple_to_string((r, acc, \"\")))\n\n                # Add seventh or triad depending on r\n                if r[-1] != \"7\":\n                    res.append(tuple_to_string((r, acc, \"7\")))\n                else:\n                    res.append(tuple_to_string((r[:-1], acc, \"\")))\n\n    if suff == \"\" or suff == \"M\" or suff == \"m\":\n        res.append(tuple_to_string((roman, acc, suff + \"7\")))\n\n    if suff == \"m\" or suff == \"m7\":\n        n = skip(roman, 2)\n        a = interval_diff(roman, n, 3) + acc\n        res.append(tuple_to_string((n, a, \"M\")))\n        res.append(tuple_to_string((n, a, \"M7\")))\n\n    # Major to minor substitution\n    if suff == \"M\" or suff == \"M7\":\n        n = skip(roman, 5)\n        a = interval_diff(roman, n, 9) + acc\n        res.append(tuple_to_string((n, a, \"m\")))\n        res.append(tuple_to_string((n, a, \"m7\")))\n\n    if suff == \"dim7\" or suff == \"dim\":\n        # Add the corresponding dominant seventh\n        res.append(tuple_to_string((skip(roman, 5), acc, \"dom7\")))\n\n        n = skip(roman, 1)\n        res.append(tuple_to_string((n, acc + interval_diff(roman, n, 1), \"dom7\")))\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    res2 = []\n    if depth > 0:\n        for x in res:\n            new_progr = progression\n            new_progr[substitute_index] = x\n            res2 += substitute(new_progr, substitute_index, depth - 1)\n    return res + res2\n"}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            item = super(LRI, self).popitem()\n            self._remove_from_ll(item[0])\n            return item\n"}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        resolved_id, branch_label = self._resolve_revision_number(id_)\n        if len(resolved_id) > 1:\n            raise MultipleHeads(resolved_id, id_)\n\n        resolved: Union[str, Tuple[()]] = resolved_id[0] if resolved_id else ()\n        return self._revision_for_ident(resolved, branch_label)\n"}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)\n"}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        from kinto.core import logger\n        uri = self.oid_config[\"userinfo_endpoint\"]\n        # Opaque access token string. Fetch user info from profile.\n        try:\n            resp = requests.get(uri, headers={\"Authorization\": \"Bearer \" + access_token})\n            resp.raise_for_status()\n            userprofile = resp.json()\n            return userprofile\n\n        except (requests.exceptions.HTTPError, ValueError, KeyError) as e:\n            logger.debug(\"Unable to fetch user profile from %s (%s)\" % (uri, e))\n            return None\n"}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n        return schema_data\n"}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        raw_key_data = {}\n\n        for key, value in self.get_keys().items():\n            raw_key_data[key] = self._dynamizer.encode(value)\n\n        return raw_key_data\n"}
{"namespace": "twtxt.config.Config.following", "completion": "        following = []\n        try:\n            for (nick, url) in self.cfg.items(\"following\"):\n                source = Source(nick, url)\n                following.append(source)\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n\n        return following\n"}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        self.connection.delete_table(self.table_name)\n        return True\n"}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "    _kex_algs.append(alg)\n\n    if default:\n        _default_kex_algs.append(alg)\n\n    _kex_handlers[alg] = (handler, hash_alg, args)\n"}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    parts = urllib.parse.urlparse(uri)\n    query_params = parse_unique_urlencoded(parts.query)\n    query_params.update(params)\n    new_query = urllib.parse.urlencode(query_params)\n    new_parts = parts._replace(query=new_query)\n    return urllib.parse.urlunparse(new_parts)\n"}
{"namespace": "boto.rds2.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.rds2.layer1 import RDSConnection\n    return connect('rds', region_name, connection_cls=RDSConnection,\n                   **kw_params)\n"}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n        current_pos = self.tell()\n        if self.buffer.tell() + len(s.encode('utf-8')) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s.encode('utf-8'))\n        self._tell = current_pos + len(s)\n"}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(\n        d, \"revision d from b\", head=b, splice=True, refresh=True\n    )\n    write_script(\n        script,\n        d,\n        \"\"\"\\\n\"Rev D\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 4\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 4\")\n\n\"\"\"\n        % (d, b),\n    )\n\n    script.generate_revision(\n        e, \"revision e from d\", head=d, splice=True, refresh=True\n    )\n    write_script(\n        script,\n        e,\n        \"\"\"\\\n\"Rev E\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 5\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 5\")\n\n\"\"\"\n        % (e, d),\n    )\n\n    script.generate_revision(\n        f, \"revision f from b\", head=b, splice=True, refresh=True\n    )\n    write_script(\n        script,\n        f,\n        \"\"\"\\\n\"Rev F\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 6\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 6\")\n\n\"\"\"\n        % (f, b),\n    )\n\n    return d, e, f\n"}
{"namespace": "kinto.config.render_template", "completion": "    template = os.path.join(HERE, template)\n    folder = os.path.dirname(destination)\n\n    if folder and not os.path.exists(folder):\n        os.makedirs(folder)\n\n    logger.info(f\"Created config {os.path.abspath(destination)}\")\n\n    with codecs.open(template, \"r\", encoding=\"utf-8\") as f:\n        raw_template = f.read()\n        rendered = raw_template.format_map(kwargs)\n        with codecs.open(destination, \"w+\", encoding=\"utf-8\") as output:\n            output.write(rendered)\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    from ..encoding.hash import double_sha256\n    stack.append(double_sha256(stack.pop()))\n"}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        from pyramid.interfaces import IRoutesMapper\n        try:\n            reg = self.registry\n        except AttributeError:\n            reg = get_current_registry()  # b/c\n        mapper = reg.getUtility(IRoutesMapper)\n        route = mapper.get_route(route_name)\n\n        if route is None:\n            raise KeyError('No such route named %s' % route_name)\n\n        if route.pregenerator is not None:\n            elements, kw = route.pregenerator(self, elements, kw)\n\n        app_url, qs, anchor = parse_url_overrides(self, kw)\n\n        path = route.generate(kw)  # raises KeyError if generate fails\n\n        if elements:\n            suffix = _join_elements(elements)\n            if not path.endswith('/'):\n                suffix = '/' + suffix\n        else:\n            suffix = ''\n\n        return app_url + path + suffix + qs + anchor\n"}
{"namespace": "mrjob.conf.combine_lists", "completion": "    result = []\n\n    for seq in seqs:\n        if seq is None:\n            continue\n\n        if isinstance(seq, (bytes, string_types, dict)):\n            result.append(seq)\n        else:\n            try:\n                result.extend(seq)\n            except:\n                result.append(seq)\n\n    return result\n"}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        del self.param_container.parameters[self.parameter_dropdown.value]\n        # Clear the input widgets\n        self._blank_parameter()\n        self._changed_data = True\n"}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        result_as_txt = [cls._format_title(\"OpenSSL Heartbleed\")]\n        heartbleed_txt = (\n            \"VULNERABLE - Server is vulnerable to Heartbleed\"\n            if result.is_vulnerable_to_heartbleed\n            else \"OK - Not vulnerable to Heartbleed\"\n        )\n        result_as_txt.append(cls._format_field(\"\", heartbleed_txt))\n        return result_as_txt\n"}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for command_name, command in ingredient.commands.items():\n                cmd_name = join_paths(ingredient.path, command_name)\n                cmd_name = self.post_process_name(cmd_name, ingredient)\n                yield cmd_name, command\n"}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    val = 0\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n    result = note[0]\n    while val > 0:\n        result = augment(result)\n        val -= 1\n    while val < 0:\n        result = diminish(result)\n        val += 1\n    return result\n"}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        algorithm = to_bytes(algorithm)\n        if not self.has_capability(b\"THREAD=\" + algorithm):\n            raise exceptions.CapabilityError(\n                \"The server does not support %s threading algorithm\" % algorithm\n            )\n\n        args = [algorithm, to_bytes(charset)] + _normalise_search_criteria(\n            criteria, charset\n        )\n        data = self._raw_command_untagged(b\"THREAD\", args)\n        return parse_response(data)\n"}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )\n"}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        for t in self._traverse_with_paths():\n            yield t[1]\n"}
{"namespace": "mopidy.ext.load_extensions", "completion": "    installed_extensions = []\n\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        logger.debug(\"Loading entry point: %s\", entry_point)\n        try:\n            extension_class = entry_point.resolve()\n        except Exception as e:\n            logger.exception(\n                f\"Failed to load extension {entry_point.name}: {e}\"\n            )\n            continue\n\n        try:\n            if not issubclass(extension_class, Extension):\n                raise TypeError  # issubclass raises TypeError on non-class\n        except TypeError:\n            logger.error(\n                \"Entry point %s did not contain a valid extension\" \"class: %r\",\n                entry_point.name,\n                extension_class,\n            )\n            continue\n\n        try:\n            extension = extension_class()\n            # Ensure required extension attributes are present after try block\n            _ = extension.dist_name\n            _ = extension.ext_name\n            _ = extension.version\n            extension_data = ExtensionData(\n                entry_point=entry_point,\n                extension=extension,\n                config_schema=extension.get_config_schema(),\n                config_defaults=extension.get_default_config(),\n                command=extension.get_command(),\n            )\n        except Exception:\n            logger.exception(\n                \"Setup of extension from entry point %s failed, \"\n                \"ignoring extension.\",\n                entry_point.name,\n            )\n            continue\n\n        installed_extensions.append(extension_data)\n\n        logger.debug(\n            \"Loaded extension: %s %s\", extension.dist_name, extension.version\n        )\n\n    names = (ed.extension.ext_name for ed in installed_extensions)\n    logger.debug(\"Discovered extensions: %s\", \", \".join(names))\n    return installed_extensions\n"}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if not value:\n            return None\n\n        output_format = getattr(self, 'format', api_settings.DATETIME_FORMAT)\n\n        if output_format is None or isinstance(value, str):\n            return value\n\n        value = self.enforce_timezone(value)\n\n        if output_format.lower() == ISO_8601:\n            value = value.isoformat()\n            if value.endswith('+00:00'):\n                value = value[:-6] + 'Z'\n            return value\n        return value.strftime(output_format)\n"}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index = self.length() + index\n        size = len(self._locked_chain)\n        if index < size:\n            return self._locked_chain[index]\n        index -= size\n\n        longest_chain = self._longest_local_block_chain()\n        the_hash = longest_chain[-index-1]\n        parent_hash = self.parent_hash if index <= 0 else self._longest_chain_cache[-index]\n        weight = self.weight_lookup.get(the_hash)\n        return (the_hash, parent_hash, weight)\n"}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        node = self._cache.get(page)\n        if node is not None:\n            return node\n\n        data = self._wal.get_page(page)\n        if not data:\n            data = self._read_page(page)\n\n        node = Node.from_page_data(self._tree_conf, data=data, page=page)\n        self._cache[node.page] = node\n        return node\n"}
{"namespace": "fs.permissions.Permissions.create", "completion": "        if init is None:\n            return cls(mode=0o777)\n        if isinstance(init, cls):\n            return init\n        if isinstance(init, int):\n            return cls(mode=init)\n        if isinstance(init, list):\n            return cls(names=init)\n        raise ValueError(\"permissions is invalid\")\n"}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        if self._reverse is not None:\n            return self._reverse.column\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.column(self.column_name, NULLTYPE)\n"}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "    from pyinfra.progress import progress_spinner\n    def get_fact_with_context(state, host, *args, **kwargs):\n        with ctx_state.use(state):\n            with ctx_host.use(host):\n                return get_fact(state, host, *args, **kwargs)\n\n    greenlet_to_host = {\n        state.pool.spawn(get_fact_with_context, state, host, *args, **kwargs): host\n        for host in state.inventory.iter_active_hosts()\n    }\n\n    results = {}\n\n    with progress_spinner(greenlet_to_host.values()) as progress:\n        for greenlet in gevent.iwait(greenlet_to_host.keys()):\n            host = greenlet_to_host[greenlet]\n            results[host] = greenlet.get()\n            progress(host)\n\n    return results\n"}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)\n"}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        jwk = {\n            \"k\": base64url_encode(force_bytes(key_obj)).decode(),\n            \"kty\": \"oct\",\n        }\n\n        if as_dict:\n            return jwk\n        else:\n            return json.dumps(jwk)\n"}
{"namespace": "discord.utils.time_snowflake", "completion": "    discord_millis = int(dt.timestamp() * 1000 - DISCORD_EPOCH)\n    return (discord_millis << 22) + (2**22 - 1 if high else 0)\n"}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n\n    for d in dicts:\n        if d:\n            for k, v in d.items():\n                # delete cleared key\n                if isinstance(v, ClearedValue) and v.value is None:\n                    result.pop(k, None)\n\n                # just set the value\n                else:\n                    result[k] = _strip_clear_tag(v)\n\n    return result\n"}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    from falcon import app_helpers\n    types_ = app_helpers.prepare_middleware(app._unprepared_middleware, True, app._ASGI)\n\n    type_infos = []\n    for stack in types_:\n        current = []\n        for method in stack:\n            _, name = _get_source_info_and_name(method)\n            cls = type(method.__self__)\n            _, cls_name = _get_source_info_and_name(cls)\n            current.append(MiddlewareTreeItemInfo(name, cls_name))\n        type_infos.append(current)\n    middlewareTree = MiddlewareTreeInfo(*type_infos)\n\n    middlewareClasses = []\n    names = 'Process request', 'Process resource', 'Process response'\n    for m in app._unprepared_middleware:\n        fns = app_helpers.prepare_middleware([m], True, app._ASGI)\n        class_source_info, cls_name = _get_source_info_and_name(type(m))\n        methods = []\n        for method, name in zip(fns, names):\n            if method:\n                real_func = method[0]\n                source_info = _get_source_info(real_func)\n                methods.append(MiddlewareMethodInfo(real_func.__name__, source_info))\n        m_info = MiddlewareClassInfo(cls_name, class_source_info, methods)\n        middlewareClasses.append(m_info)\n\n    return MiddlewareInfo(\n        middlewareTree, middlewareClasses, app._independent_middleware\n    )\n"}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    from .utils import import_attribute\n    if not serializer:\n        return DefaultSerializer\n\n    if isinstance(serializer, str):\n        serializer = import_attribute(serializer)\n\n    default_serializer_methods = ('dumps', 'loads')\n\n    for instance_method in default_serializer_methods:\n        if not hasattr(serializer, instance_method):\n            raise NotImplementedError('Serializer should have (dumps, loads) methods.')\n\n    return serializer\n"}
{"namespace": "alembic.command.upgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def upgrade(rev, context):\n        return script._upgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=upgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()\n"}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if msg_time:\n            time_val = '\"%s\"' % datetime_to_INTERNALDATE(msg_time)\n            time_val = to_unicode(time_val)\n        else:\n            time_val = None\n        return self._command_and_check(\n            \"append\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(flags),\n            time_val,\n            to_bytes(msg),\n            unpack=True,\n        )\n"}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if is_command_key(\"ENTER\", key):\n            self.activate(key)\n            return None\n        else:  # This is in the else clause, to avoid multiple activation\n            return super().keypress(size, key)\n"}
{"namespace": "boltons.strutils.indent", "completion": "    indented_lines = [(margin + line if key(line) else line)\n                      for line in iter_splitlines(text)]\n    return newline.join(indented_lines)\n"}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        L = []\n        for category_name in self.categories():\n            L.append(\n                (\n                    category_name,\n                    self.get_category(category_name, sort_key=sort_key),\n                )\n            )\n        return L\n"}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "    @classmethod\n    def redirect(cls, path, status=302, headers=None):\n        headers = headers or {}\n"}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n\n    # Bail early since we cannot inspect built-in function signatures.\n    if inspect.isbuiltin(obj):\n        raise BuiltinSignatureError(\n            'Built-in function signatures are not inspectable. '\n            'Wrap the function call in a simple, pure Python function.')\n\n    if not (inspect.isfunction(obj) or inspect.ismethod(obj) or isinstance(obj, functools.partial)):\n        return False\n\n    sig = inspect.signature(obj)\n    params = sig.parameters.values()\n    return all(\n        param.kind == param.VAR_POSITIONAL or\n        param.kind == param.VAR_KEYWORD or\n        param.default != param.empty\n        for param in params\n    )\n"}
{"namespace": "falcon.util.uri.decode", "completion": "    decoded_uri = encoded_uri\n\n    # PERF(kgriffs): Don't take the time to instantiate a new\n    # string unless we have to.\n    if '+' in decoded_uri and unquote_plus:\n        decoded_uri = decoded_uri.replace('+', ' ')\n\n    # Short-circuit if we can\n    if '%' not in decoded_uri:\n        return decoded_uri\n\n    # NOTE(kgriffs): Clients should never submit a URI that has\n    # unescaped non-ASCII chars in them, but just in case they\n    # do, let's encode into a non-lossy format.\n    decoded_uri = decoded_uri.encode()\n\n    # PERF(kgriffs): This was found to be faster than using\n    # a regex sub call or list comprehension with a join.\n    tokens = decoded_uri.split(b'%')\n    # PERF(vytas): Just use in-place add for a low number of items:\n    if len(tokens) < 8:\n        decoded_uri = tokens[0]\n        for token in tokens[1:]:\n            token_partial = token[:2]\n            try:\n                decoded_uri += _HEX_TO_BYTE[token_partial] + token[2:]\n            except KeyError:\n                # malformed percentage like \"x=%\" or \"y=%+\"\n                decoded_uri += b'%' + token\n\n        # Convert back to str\n        return decoded_uri.decode('utf-8', 'replace')\n\n    # NOTE(vytas): Decode percent-encoded bytestring fragments and join them\n    # back to a string using the platform-dependent method.\n    return _join_tokens(tokens)\n"}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    from rest_framework.utils.serializer_helpers import ReturnDict\n    from rest_framework.utils.serializer_helpers import ReturnList\n    if isinstance(data, (list, tuple)):\n        ret = [\n            _get_error_details(item, default_code) for item in data\n        ]\n        if isinstance(data, ReturnList):\n            return ReturnList(ret, serializer=data.serializer)\n        return ret\n    elif isinstance(data, dict):\n        ret = {\n            key: _get_error_details(value, default_code)\n            for key, value in data.items()\n        }\n        if isinstance(data, ReturnDict):\n            return ReturnDict(ret, serializer=data.serializer)\n        return ret\n\n    text = force_str(data)\n    code = getattr(data, 'code', default_code)\n    return ErrorDetail(text, code)\n"}
{"namespace": "boto.directconnect.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.directconnect.layer1 import DirectConnectConnection\n    return connect('directconnect', region_name,\n                   connection_cls=DirectConnectConnection, **kw_params)\n"}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    if isinstance(changes, dict):\n        for k, v in changes.items():\n            if isinstance(v, dict):\n                if k not in root:\n                    root[k] = {}\n                recursive_update_dict(root[k], v, ignores)\n            elif v in ignores:\n                if k in root:\n                    root.pop(k)\n            else:\n                root[k] = v\n"}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    lines = []\n    if e.print_usage:\n        lines.append(short_usage)\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    else:\n        lines.append(\"\\n\".join(tb.format_exception_only(type(e), e)))\n    return \"\\n\".join(lines)\n"}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            suggested_facet_sql = \"\"\"\n                select {column} as value, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            distinct_values = None\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                num_distinct_values = len(distinct_values)\n                if (\n                    1 < num_distinct_values < row_count\n                    and num_distinct_values <= facet_size\n                    # And at least one has n > 1\n                    and any(r[\"n\"] > 1 for r in distinct_values)\n                ):\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_added_args(\n                                        self.request, {\"_facet\": column}\n                                    )\n                                ),\n                            ),\n                        }\n                    )\n            except QueryInterrupted:\n                continue\n        return suggested_facets\n"}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        name, fn = self.make_property(callable, name=name, reify=reify)\n        self.properties[name] = fn\n"}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        credentials = extract_http_basic_credentials(request)\n        if credentials:\n            return credentials.username\n"}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        object_permission_principals = self._store.get(permission_key, set())\n        object_permission_principals.add(principal)\n        self._store[permission_key] = object_permission_principals\n"}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())\n"}
{"namespace": "zulipterminal.helper.open_media", "completion": "    from zulipterminal.platform_code import successful_GUI_return_code\n    error = []\n    command = [tool, media_path]\n    try:\n        process = subprocess.run(\n            command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n        )\n        exit_status = process.returncode\n        if exit_status != successful_GUI_return_code():\n            error = [\n                \" The tool \",\n                (\"footer_contrast\", tool),\n                \" did not run successfully\" \". Exited with \",\n                (\"footer_contrast\", str(exit_status)),\n            ]\n    except FileNotFoundError:\n        error = [\" The tool \", (\"footer_contrast\", tool), \" could not be found\"]\n\n    if error:\n        controller.report_error(error)\n"}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(\n                hostname=hostname, protocol=protocol,\n                replace_key=replace_key,\n                replace_key_prefix=replace_key_prefix,\n                http_redirect_code=http_redirect_code)\n        return self\n"}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        try:\n            items = items.items()\n        except AttributeError:\n            pass\n        items = [(self.encode_key(k), self.encode(v)) for k, v in items]\n\n        UPDATE_ITEMS = 'REPLACE INTO \"%s\" (key, value) VALUES (?, ?)' % self.tablename\n        self.conn.executemany(UPDATE_ITEMS, items)\n        if kwds:\n            self.update(kwds)\n        if self.autocommit:\n            self.commit()\n"}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        return [self.stem_word(w) for w in words if w not in self._stop_words]\n"}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        from boto.exception import TooManyRecordsException\n        name = self.route53connection._make_qualified(name)\n        returned = self.route53connection.get_all_rrsets(self.id, name=name,\n                                                         type=type)\n\n        # name/type for get_all_rrsets sets the starting record; they\n        # are not a filter\n        results = []\n        for r in returned:\n            if r.name == name and r.type == type:\n                results.append(r)\n            # Is at the end of the list of matched records. No need to continue\n            # since the records are sorted by name and type.\n            else:\n                break\n\n        weight = None\n        region = None\n        if identifier is not None:\n            try:\n                int(identifier[1])\n                weight = identifier[1]\n            except:\n                region = identifier[1]\n\n        if weight is not None:\n            results = [r for r in results if (r.weight == weight and\n                                              r.identifier == identifier[0])]\n        if region is not None:\n            results = [r for r in results if (r.region == region and\n                                              r.identifier == identifier[0])]\n\n        if ((not all) and (len(results) > desired)):\n            message = \"Search: name %s type %s\" % (name, type)\n            message += \"\\nFound: \"\n            message += \", \".join([\"%s %s %s\" % (r.name, r.type, r.to_print())\n                                  for r in results])\n            raise TooManyRecordsException(message)\n        elif len(results) > 1:\n            return results\n        elif len(results) == 1:\n            return results[0]\n        else:\n            return None\n"}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        identity = self._get_identity(request)\n\n        if identity is None:\n            self.debug and self._log(\n                'repoze.who identity is None, returning None',\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        userid = identity['repoze.who.userid']\n\n        if userid is None:\n            self.debug and self._log(\n                'repoze.who.userid is None, returning None' % userid,\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self._clean_principal(userid) is None:\n            self.debug and self._log(\n                (\n                    'use of userid %r is disallowed by any built-in Pyramid '\n                    'security policy, returning None' % userid\n                ),\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self.callback is None:\n            return userid\n\n        if self.callback(identity, request) is not None:  # is not None!\n            return userid\n"}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        if type is not None:\n            self._check_type(type)\n\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                self.name(path_type, path)\n\n        return dict((name, typed_path[1])\n                    for name, typed_path\n                    in self._name_to_typed_path.items()\n                    if (type is None or typed_path[0] == type))\n"}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    if dep.find(\"@\") > 0 and (\n            (dep.find(\"@\") < dep.find(\"/\") or \"/\" not in dep) and\n            (dep.find(\"@\") < dep.find(\":\") or \":\" not in dep)\n            ):\n        # This should be a 'pkgname @ https://...' style path, or\n        # 'pkname @ /local/file/path'.\n        return parse_as_folder_reference(dep.partition(\"@\")[2].lstrip())\n\n    # Check if this is either not an url, or a file URL:\n    if dep.startswith((\"/\", \"file://\")) or (\n            dep.find(\"/\") > 0 and\n            dep.find(\"://\") < 0) or (dep in [\"\", \".\"]):\n        if dep.startswith(\"file://\"):\n            dep = urlunquote(urlparse(dep).path)\n        return dep\n    return None\n"}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._resolved_dependencies\n        )\n"}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self\n"}
{"namespace": "boto.ec2.address.Address.release", "completion": "        if self.allocation_id:\n            return self.connection.release_address(\n                allocation_id=self.allocation_id,\n                dry_run=dry_run)\n        else:\n            return self.connection.release_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )\n"}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n"}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    return (\n        sorted(\n            sorted(\n                sorted(\n                    ds, key=_attempt_num, reverse=True),\n                key=_container_num),\n            key=_step_sort_key, reverse=True))\n"}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    name = variable.replace('.', '_')\n    if name in os.environ:\n        return os.environ[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(variable, {}).values():\n        name = var.replace('.', '_')\n        if name in os.environ:\n            return os.environ[name]\n\n    return default\n"}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    absolute_binary_path = os.path.normpath(os.path.abspath(binary))\n    if not os.path.exists(absolute_binary_path):\n        for path in os.getenv('PATH', '/bin/:/usr/bin/').split(os.pathsep):\n            absolute_binary_path = os.path.normpath(os.path.abspath(os.path.join(path, binary)))\n            if os.path.exists(absolute_binary_path):\n                break\n        else:\n            raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)\n    return absolute_binary_path\n"}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections[-1]\n        return None\n"}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.relative_uri\n            )\n\n            self._cached_forwarded_uri = value\n\n        return self._cached_forwarded_uri\n"}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n"}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        from pyramid.config.predicates import sort_accept_offers\n        if phash is not None:\n            for i, (s, v, h) in enumerate(list(self.views)):\n                if phash == h:\n                    self.views[i] = (order, view, phash)\n                    return\n\n        if accept is None:\n            self.views.append((order, view, phash))\n            self.views.sort(key=operator.itemgetter(0))\n        else:\n            subset = self.media_views.setdefault(accept, [])\n            for i, (s, v, h) in enumerate(list(subset)):\n                if phash == h:\n                    subset[i] = (order, view, phash)\n                    return\n            else:\n                subset.append((order, view, phash))\n                subset.sort(key=operator.itemgetter(0))\n            # dedupe accepts and sort appropriately\n            accepts = set(self.accepts)\n            accepts.add(accept)\n            if accept_order:\n                accept_order = [v for _, v in accept_order.sorted()]\n            self.accepts = sort_accept_offers(accepts, accept_order)\n"}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._access(item)\n        self._set[item] = None\n        if len(self._set) > self.max_items:\n            self._set.popitem(last=False)\n"}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        path = self.calling_format.build_path_base(bucket, key)\n        auth_path = self.calling_format.build_auth_path(bucket, key)\n        host = self.calling_format.build_host(self.server_name(), bucket)\n\n        # For presigned URLs we should ignore the port if it's HTTPS\n        if host.endswith(':443'):\n            host = host[:-4]\n\n        params = {}\n        if version_id is not None:\n            params['VersionId'] = version_id\n\n        if response_headers is not None:\n            params.update(response_headers)\n\n        http_request = self.build_base_http_request(method, path, auth_path,\n                                                    headers=headers, host=host,\n                                                    params=params)\n\n        return self._auth_handler.presign(http_request, expires_in,\n                                          iso_date=iso_date)\n"}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    body = line.rstrip()\n    newline = line[len(body):]\n    tokens = []\n\n    # add the body of line\n    if body:\n        tokens += _tokenize_str(body)\n\n    # add newlines\n    if newline:\n        if newline in ('\\n', '\\r\\n'):\n            tokens.append(_PrettyToken(_PrettyTokenType.NEWLINE, newline))\n        else:\n            whitespace = newline.rstrip('\\n')\n            newline = newline[len(whitespace):]\n            if whitespace:\n                tokens.append(_PrettyToken(_PrettyTokenType.WHITESPACE, whitespace))\n            tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(trailing whitespace)'))\n            if newline:\n                tokens.append(_PrettyToken(_PrettyTokenType.NEWLINE, newline))\n\n    return tokens\n"}
{"namespace": "telethon.extensions.html.unparse", "completion": "    from ..helpers import within_surrogate\n    if not text:\n        return text\n    elif not entities:\n        return escape(text)\n\n    if isinstance(entities, TLObject):\n        entities = (entities,)\n\n    text = add_surrogate(text)\n    insert_at = []\n    for i, entity in enumerate(entities):\n        s = entity.offset\n        e = entity.offset + entity.length\n        delimiter = ENTITY_TO_FORMATTER.get(type(entity), None)\n        if delimiter:\n            if callable(delimiter):\n                delimiter = delimiter(entity, text[s:e])\n            insert_at.append((s, i, delimiter[0]))\n            insert_at.append((e, len(entities) - i, delimiter[1]))\n\n    insert_at.sort(key=lambda t: (t[0], t[1]))\n    next_escape_bound = len(text)\n    while insert_at:\n        # Same logic as markdown.py\n        at, _, what = insert_at.pop()\n        while within_surrogate(text, at):\n            at += 1\n\n        text = text[:at] + what + escape(text[at:next_escape_bound]) + text[next_escape_bound:]\n        next_escape_bound = at\n\n    text = escape(text[:next_escape_bound]) + text[next_escape_bound:]\n\n    return del_surrogate(text)\n"}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        from boto.glacier.utils import minimum_part_size\n        min_part_size_required = minimum_part_size(total_size)\n        if self._part_size >= min_part_size_required:\n            part_size = self._part_size\n        else:\n            part_size = min_part_size_required\n            log.debug(\"The part size specified (%s) is smaller than \"\n                      \"the minimum required part size.  Using a part \"\n                      \"size of: %s\", self._part_size, part_size)\n        total_parts = int(math.ceil(total_size / float(part_size)))\n        return total_parts, part_size\n"}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        queryset = self.get_queryset()\n        try:\n            return queryset.get(**{self.slug_field: data})\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', slug_name=self.slug_field, value=smart_str(data))\n        except (TypeError, ValueError):\n            self.fail('invalid')\n"}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        try:\n            trans_instrs = self._translate(instruction)\n        except Exception:\n            self._log_translation_exception(instruction)\n\n            raise TranslationError(\"Unknown error\")\n\n        return trans_instrs\n"}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)\n"}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        # avoid VACUUM, as it gives \"OperationalError: database schema has changed\"\n        CLEAR_ALL = 'DELETE FROM \"%s\";' % self.tablename\n        self.conn.commit()\n        self.conn.execute(CLEAR_ALL)\n        self.conn.commit()\n"}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "        try:\n            instr_lower = instr.lower()\n\n            if instr_lower not in self._cache:\n                instr_asm = instruction.parseString(instr_lower)[0]\n\n                self._cache[instr_lower] = instr_asm\n\n            instr_asm = copy.deepcopy(self._cache[instr_lower])\n\n            # self._check_instruction(instr_asm)\n        except Exception:\n            instr_asm = None\n            error_msg = \"Failed to parse instruction: %s\"\n            logger.error(error_msg, instr, exc_info=True)\n#             print(\"Failed to parse instruction: \" + instr)\n#             print(\"Exception: \" + str(e))\n\n        return instr_asm\n"}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        uri = self.settings['logger_class'].get()\n        if uri == \"simple\":\n            # support the default\n            uri = LoggerClass.default\n\n        # if default logger is in use, and statsd is on, automagically switch\n        # to the statsd logger\n        if uri == LoggerClass.default:\n            if 'statsd_host' in self.settings and self.settings['statsd_host'].value is not None:\n                uri = \"gunicorn.instrument.statsd.Statsd\"\n\n        logger_class = util.load_class(\n            uri,\n            default=\"gunicorn.glogging.Logger\",\n            section=\"gunicorn.loggers\")\n\n        if hasattr(logger_class, \"install\"):\n            logger_class.install()\n        return logger_class\n"}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        try:\n            stdout = self.invoke_hadoop(['fs', '-du', path_glob],\n                                        return_stdout=True,\n                                        ok_returncodes=[0, 1, 255])\n        except CalledProcessError:\n            return 0\n\n        try:\n            return sum(int(line.split()[0])\n                       for line in stdout.split(b'\\n')\n                       if line.strip())\n        except (ValueError, TypeError, IndexError):\n            raise IOError(\n                'Unexpected output from hadoop fs -du: %r' % stdout)\n"}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if not pretty:\n            return self.__document.toxml('UTF-8')\n        else:\n            return self.__document.toprettyxml(encoding='UTF-8')\n"}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    from diffprivlib.utils import check_random_state\n    from diffprivlib.utils import PrivacyLeakWarning\n    from diffprivlib.accountant import BudgetAccountant\n    from diffprivlib.validation import clip_to_bounds\n    from diffprivlib.tools.utils import _wrap_axis\n    from diffprivlib.validation import check_bounds\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if bounds is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                      \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                      \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        bounds = (np.min(array), np.max(array))\n\n    quant = np.ravel(quant)\n\n    if np.any(quant < 0) or np.any(quant > 1):\n        raise ValueError(\"Quantiles must be in the unit interval [0, 1].\")\n\n    if len(quant) > 1:\n        return np.array([quantile(array, q_i, epsilon=epsilon / len(quant), bounds=bounds, axis=axis, keepdims=keepdims,\n                                  accountant=accountant, random_state=random_state) for q_i in quant])\n\n    # Dealing with a single quant from now on\n    quant = quant.item()\n\n    if axis is not None or keepdims:\n        return _wrap_axis(quantile, array, quant=quant, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                          random_state=random_state, accountant=accountant)\n\n    # Dealing with a scalar output from now on\n    bounds = check_bounds(bounds, shape=0, min_separation=1e-5)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Let's ravel array to be single-dimensional\n    array = clip_to_bounds(np.ravel(array), bounds)\n\n    k = array.size\n    array = np.append(array, list(bounds))\n    array.sort()\n\n    interval_sizes = np.diff(array)\n\n    # Todo: Need to find a way to do this in a differentially private way, see GH 80\n    if np.isnan(interval_sizes).any():\n        return np.nan\n\n    mech = Exponential(epsilon=epsilon, sensitivity=1, utility=list(-np.abs(np.arange(0, k + 1) - quant * k)),\n                       measure=list(interval_sizes), random_state=random_state)\n    idx = mech.randomise()\n    output = random_state.random() * (array[idx+1] - array[idx]) + array[idx]\n\n    accountant.spend(epsilon, 0)\n\n    return output\n"}
{"namespace": "fs.wildcard.match", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, True)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, True)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None\n"}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        definition = []\n\n        for part in self.parts:\n            definition.append({\n                'AttributeName': part.name,\n                'AttributeType': part.data_type,\n            })\n\n        return definition\n"}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for frmt in formats:\n        try:\n            _t = time.strptime(t, frmt)\n            break\n        except ValueError:\n            continue\n    else:\n        return None\n\n    year = _t.tm_year if _t.tm_year != 1900 else time.localtime().tm_year\n    month = _t.tm_mon\n    day = _t.tm_mday\n    hour = _t.tm_hour\n    minutes = _t.tm_min\n    dt = datetime(year, month, day, hour, minutes, tzinfo=timezone.utc)\n\n    epoch_time = (dt - EPOCH_DT).total_seconds()\n    return epoch_time\n"}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    return [\n        prerequisite_cls()\n        for prerequisite_cls in [\n            HomebrewPrerequisite,\n            AutoconfPrerequisite,\n            AutomakePrerequisite,\n            LibtoolPrerequisite,\n            PkgConfigPrerequisite,\n            CmakePrerequisite,\n            OpenSSLPrerequisite,\n            JDKPrerequisite,\n        ] if prerequisite_cls.mandatory.get(platform, False)\n    ]\n"}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    import mopidy\n    parts = [\n        f\"Mopidy/{mopidy.__version__}\",\n        f\"{platform.python_implementation()}/{platform.python_version()}\",\n    ]\n    if name:\n        parts.insert(0, name)\n    return \" \".join(parts)\n"}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n        # Use a mapper if:\n        #   - the user writes one\n        #   - it is the first step and we don't want to mess up protocols\n        #   - there are only combiners and we don't want to mess up protocols\n        if (step_num == 0 or\n                self.has_explicit_mapper or\n                self.has_explicit_combiner):\n            desc['mapper'] = self.render_mapper()\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n        # TODO: verify this is a dict, convert booleans to strings\n        if self._steps['jobconf']:\n            desc['jobconf'] = self._steps['jobconf']\n\n        return desc\n"}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        status = self.connection.create_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        if self.tags is None:\n            self.tags = TagSet()\n        self.tags.update(tags)\n"}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        result = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.params:\n            result[\"params\"] = self.params\n        if self.resource_id:\n            result[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            result[\"resourceUri\"] = self.resource_uri\n        if self.expiration:\n            result[\"expiration\"] = self.expiration\n\n        return result\n"}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    from kinto.core.utils import encode64\n    credentials = f\"{user}:{password}\"\n    authorization = f\"Basic {encode64(credentials)}\"\n    return {\"Authorization\": authorization}\n"}
{"namespace": "boto.sqs.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.sqs.connection import SQSConnection\n    return connect('sqs', region_name, region_cls=SQSRegionInfo,\n                   connection_cls=SQSConnection, **kw_params)\n"}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        environ: dict[str, t.Any] = {}\n        if resource_request is None:\n            resource_request = system_resources()\n        # use nvidia gpu\n        nvidia_gpus: list[int] | None = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            if isinstance(workers_per_resource, float):\n                # NOTE: We hit this branch when workers_per_resource is set to\n                # float, for example 0.5 or 0.25\n                if workers_per_resource > 1:\n                    raise ValueError(\n                        \"Currently, the default strategy doesn't support workers_per_resource > 1. It is recommended that one should implement a custom strategy in this case.\"\n                    )\n                # We are round the assigned resource here. This means if workers_per_resource=.4\n                # then it will round down to 2. If workers_per_source=0.6, then it will also round up to 2.\n                assigned_resource_per_worker = round(1 / workers_per_resource)\n                if len(nvidia_gpus) < assigned_resource_per_worker:\n                    logger.warning(\n                        \"Failed to allocate %s GPUs for %s (number of available GPUs < assigned workers per resource [%s])\",\n                        nvidia_gpus,\n                        worker_index,\n                        assigned_resource_per_worker,\n                    )\n                    raise IndexError(\n                        f\"There aren't enough assigned GPU(s) for given worker id '{worker_index}' [required: {assigned_resource_per_worker}].\"\n                    )\n                assigned_gpu = nvidia_gpus[\n                    assigned_resource_per_worker\n                    * worker_index : assigned_resource_per_worker\n                    * (worker_index + 1)\n                ]\n                dev = \",\".join(map(str, assigned_gpu))\n            else:\n                dev = str(nvidia_gpus[worker_index // workers_per_resource])\n            environ[\"CUDA_VISIBLE_DEVICES\"] = dev\n            logger.info(\n                \"Environ for worker %s: set CUDA_VISIBLE_DEVICES to %s\",\n                worker_index,\n                dev,\n            )\n            return environ\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # disable gpu\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                thread_count = math.ceil(cpus)\n                for thread_env in THREAD_ENVS:\n                    environ[thread_env] = str(thread_count)\n                logger.info(\n                    \"Environ for worker %d: set CPU thread count to %d\",\n                    worker_index,\n                    thread_count,\n                )\n                return environ\n            else:\n                for thread_env in THREAD_ENVS:\n                    environ[thread_env] = \"1\"\n                return environ\n\n        return environ\n"}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    try:\n        with open(join(ndk_dir, 'source.properties')) as fileh:\n            ndk_data = fileh.read()\n    except IOError:\n        info(UNKNOWN_NDK_MESSAGE)\n        return\n\n    for line in ndk_data.split('\\n'):\n        if line.startswith('Pkg.Revision'):\n            break\n    else:\n        info(PARSE_ERROR_NDK_MESSAGE)\n        return\n\n    # Line should have the form \"Pkg.Revision = ...\"\n    ndk_version = LooseVersion(line.split('=')[-1].strip())\n\n    return ndk_version\n"}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        narrow = parsed_link[\"narrow\"]\n        if \"stream\" == narrow:\n            self.controller.narrow_to_stream(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n            )\n        elif \"stream:near\" == narrow:\n            self.controller.narrow_to_stream(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                contextual_message_id=parsed_link[\"message_id\"],\n            )\n        elif \"stream:topic\" == narrow:\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n            )\n        elif \"stream:topic:near\" == narrow:\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n                contextual_message_id=parsed_link[\"message_id\"],\n            )\n"}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        obj = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(obj)\n\n        # Retreive the last_modified information from a querystring if present.\n        last_modified = self.request.validated[\"querystring\"].get(\"last_modified\")\n\n        # If less or equal than current object. Ignore it.\n        if last_modified and last_modified <= obj[self.model.modified_field]:\n            last_modified = None\n\n        try:\n            deleted = self.model.delete_object(obj, last_modified=last_modified)\n        except storage_exceptions.ObjectNotFoundError:\n            # Delete might fail if the object was deleted since we\n            # fetched it from the storage (ref Kinto/kinto#1407). This\n            # is one of a larger class of issues where another request\n            # could modify the object between our fetch and our\n            # delete, which could e.g. invalidate our precondition\n            # checking. Fixing this correctly is a larger\n            # problem. However, let's punt on fixing it correctly and\n            # just handle this one important case for now (see #1557).\n            #\n            # Raise a 404 vs. a 409 or 412 because that's what we\n            # would have done if the other thread's delete had\n            # happened a little earlier. (The client doesn't need to\n            # know that we did a bunch of work fetching the existing\n            # object for nothing.)\n            raise self._404_for_object(self.object_id)\n\n        timestamp = deleted[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(deleted, action=ACTIONS.DELETE, old=obj)\n"}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n\n            return BudgetAccountant._default\n\n        if not isinstance(accountant, BudgetAccountant):\n            raise TypeError(f\"Accountant must be of type BudgetAccountant, got {type(accountant)}\")\n\n        return accountant\n"}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for p in self._plugins:\n            if p.name == name:\n                return p\n        return None\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n    if len(likelihoods) == 0:\n        return [], np.nan\n    min_lik = min(likelihoods)\n    ind = likelihoods.index(min_lik)\n    return session[ind : ind + window_len], min_lik  # noqa E203\n"}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for key in sorted(d.keys()):\n        value = d[key]\n        if isinstance(value, dict) and value:\n            for k, v in iterate_flattened(d[key]):\n                yield join_paths(key, k), v\n        else:\n            yield key, value\n"}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if locales is not None:\n            if not isinstance(locales, (list, tuple)):\n                locales = [locales]\n            locales = [str(locale) for locale in locales]\n        if not domain:\n            domain = cls.DEFAULT_DOMAIN\n        filename = gettext.find(domain, dirname, locales)\n        if not filename:\n            return gettext.NullTranslations()\n        with open(filename, 'rb') as fp:\n            return cls(fileobj=fp, domain=domain)\n"}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        oldkw = self.kw.copy()\n        oldkw.update(kw)\n        inst = self.__class__(self.__name__, self.__parent__, **oldkw)\n        inst.subs = copy.deepcopy(self.subs)\n        if __name__ is not _marker:\n            inst.__name__ = __name__\n        if __parent__ is not _marker:\n            inst.__parent__ = __parent__\n        return inst\n"}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        from mrjob.cat import decompress\n        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        fs_path = m.group('filesystem_path')\n\n        p = self._ssh_launch(addr, ['cat', fs_path])\n\n        for chunk in decompress(p.stdout, fs_path):\n            yield chunk\n\n        self._ssh_finish_run(p)\n"}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what, unpack=True\n        )\n"}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        self.nest(verb)\n        return self\n"}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        self.last_page += 1\n        return self.last_page\n"}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        config_scope = ConfigScope(func)\n        self._add_named_config(func.__name__, config_scope)\n        return config_scope\n"}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    if len(evaluated_sentences) <= 0:\n        raise (ValueError(\"Collections must contain at least 1 sentence.\"))\n\n    lcs_union = set()\n    reference_words = _split_into_words([reference_sentence])\n    combined_lcs_length = 0\n    for eval_s in evaluated_sentences:\n        evaluated_words = _split_into_words([eval_s])\n        lcs = set(_recon_lcs(reference_words, evaluated_words))\n        combined_lcs_length += len(lcs)\n        lcs_union = lcs_union.union(lcs)\n\n    union_lcs_count = len(lcs_union)\n    union_lcs_value = union_lcs_count / combined_lcs_length\n    return union_lcs_value\n"}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "\n    async def asgi_send(self, send):\n        headers = {}\n        headers.update(self.headers)\n        headers[\"content-type\"] = self.content_type\n        raw_headers = [\n            [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n            for key, value in headers.items()\n        ]\n        for set_cookie in self._set_cookie_headers:\n            raw_headers.append([b\"set-cookie\", set_cookie.encode(\"utf-8\")])\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": raw_headers,\n            }\n        )\n        body = self.body\n        if not isinstance(body, bytes):\n"}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(\n            self.id,\n            description,\n            dry_run=dry_run\n        )\n"}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    if _local_modules and os.path.dirname(_local_modules[0][1]) == directory:\n        return _local_modules\n\n    if not os.path.isdir(directory):\n        # example/import_test_project/A.py -> example/import_test_project\n        directory = os.path.dirname(directory)\n\n    if directory == '':\n        return _local_modules\n\n    for path in os.listdir(directory):\n        if _is_python_file(path):\n            # A.py -> A\n            module_name = os.path.splitext(path)[0]\n            _local_modules.append((module_name, os.path.join(directory, path)))\n\n    return _local_modules\n"}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    if netcode not in THREAD_LOCALS.providers:\n        THREAD_LOCALS.providers[netcode] = providers_for_netcode_from_env(netcode)\n    return THREAD_LOCALS.providers[netcode]\n"}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file(self.args.input_path)\n\n        try:\n            with warnings.catch_warnings():\n                if getattr(self.args, 'no_header_row', None):\n                    warnings.filterwarnings(action='ignore', message='Column names not specified', module='agate')\n\n                self.main()\n        finally:\n            if 'f' not in self.override_flags:\n                self.input_file.close()\n"}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.seek(0, os.SEEK_END)\n        self.buf.write(data)\n"}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        from .util import chunk\n        folder_data = [item for item in folder_data if item not in (b\"\", None)]\n\n        ret = []\n        parsed = parse_response(folder_data)\n        for flags, delim, name in chunk(parsed, size=3):\n            if isinstance(name, int):\n                # Some IMAP implementations return integer folder names\n                # with quotes. These get parsed to ints so convert them\n                # back to strings.\n                name = str(name)\n            elif self.folder_encode:\n                name = decode_utf7(name)\n\n            ret.append((flags, delim, name))\n        return ret\n"}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)\n"}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        samesite=\"lax\",\n    ):\n        assert samesite in SAMESITE_VALUES, \"samesite should be one of {}\".format(\n            SAMESITE_VALUES\n        )\n        cookie = SimpleCookie()\n        cookie[key] = value\n        for prop_name, prop_value in (\n            (\"max_age\", max_age),\n            (\"expires\", expires),\n            (\"path\", path),\n            (\"domain\", domain),\n            (\"samesite\", samesite),\n        ):\n            if prop_value is not None:\n                cookie[key][prop_name.replace(\"_\", \"-\")] = prop_value\n        for prop_name, prop_value in ((\"secure\", secure), (\"httponly\", httponly)):\n            if prop_value:\n"}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens, text = _decode_with_recovery(content)\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens\n"}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        result = self.cookie.identify(request)\n        if result:\n            return result['userid']\n"}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return bool(glob.glob(path_glob))\n"}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        result_as_txt = [cls._format_title(\"Certificates Information\")]\n\n        # SNI\n        server_name_indication = result.hostname_used_for_server_name_indication\n        result_as_txt.append(cls._format_field(\"Hostname sent for SNI:\", server_name_indication))\n\n        # Display each certificate deployment\n        result_as_txt.append(\n            cls._format_field(\"Number of certificates detected:\", str(len(result.certificate_deployments)))\n        )\n        for index, cert_deployment in enumerate(result.certificate_deployments):\n            result_as_txt.append(\"\\n\")\n            result_as_txt.extend(cls._cert_deployment_to_console_output(index, cert_deployment))\n\n        return result_as_txt\n"}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        try:\n            return self._get_apparent_index(self.item_index_map[val])\n        except KeyError:\n            cn = self.__class__.__name__\n            raise ValueError('%r is not in %s' % (val, cn))\n"}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    check_digit = luhn_checksum(int(partial_number) * 10)\n    return check_digit if check_digit == 0 else 10 - check_digit\n"}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  t1, t2 = options.cmp\n  d1 = db.GetNet(t1)\n  d2 = db.GetNet(t2)\n  union = list(set(d1 + d2))\n  meta = (t1, t2, union)\n  results = []\n  for el in set(d1 + d2):\n    el = nacaddr.IP(el)\n    if el in d1 and el in d2:\n      results.append(str(el))\n    elif el in d1:\n      results.append(str(el))\n    elif el in d2:\n      results.append(str(el))\n  return meta, results\n"}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if path is None:\n        return None\n    if (\n        requests_pathname != \"/\" and not path.startswith(requests_pathname.rstrip(\"/\"))\n    ) or (requests_pathname == \"/\" and not path.startswith(\"/\")):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with requests_pathname_prefix are not supported.\n            You supplied: {path} and requests_pathname_prefix was {requests_pathname}\n            \"\"\"\n        )\n    if requests_pathname != \"/\" and path.startswith(requests_pathname.rstrip(\"/\")):\n        path = path.replace(\n            # handle the case where the path might be `/my-dash-app`\n            # but the requests_pathname_prefix is `/my-dash-app/`\n            requests_pathname.rstrip(\"/\"),\n            \"\",\n            1,\n        )\n    return path.strip(\"/\")\n"}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        return self.helper.principals_allowed_by_permission(\n            context, permission\n        )\n"}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        return os.path.join(\n            self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name),\n            \"lib/pkgconfig\",\n        )\n"}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    metadata_default = metadata_col.server_default\n    conn_col_default = conn_col.server_default\n    if conn_col_default is None and metadata_default is None:\n        return False\n\n    if sqla_compat._server_default_is_computed(metadata_default):\n        # return False in case of a computed column as the server\n        # default. Note that DDL for adding or removing \"GENERATED AS\" from\n        # an existing column is not currently known for any backend.\n        # Once SQLAlchemy can reflect \"GENERATED\" as the \"computed\" element,\n        # we would also want to ignore and/or warn for changes vs. the\n        # metadata (or support backend specific DDL if applicable).\n        if not sqla_compat.has_computed_reflection:\n            return False\n\n        else:\n            return (\n                _compare_computed_default(  # type:ignore[func-returns-value]\n                    autogen_context,\n                    alter_column_op,\n                    schema,\n                    tname,\n                    cname,\n                    conn_col,\n                    metadata_col,\n                )\n            )\n    if sqla_compat._server_default_is_computed(conn_col_default):\n        _warn_computed_not_supported(tname, cname)\n        return False\n\n    if sqla_compat._server_default_is_identity(\n        metadata_default, conn_col_default\n    ):\n        alter_column_op.existing_server_default = conn_col_default\n        diff, is_alter = _compare_identity_default(\n            autogen_context,\n            alter_column_op,\n            schema,\n            tname,\n            cname,\n            conn_col,\n            metadata_col,\n        )\n        if is_alter:\n            alter_column_op.modify_server_default = metadata_default\n            if diff:\n                log.info(\n                    \"Detected server default on column '%s.%s': \"\n                    \"identity options attributes %s\",\n                    tname,\n                    cname,\n                    sorted(diff),\n                )\n    else:\n        rendered_metadata_default = _render_server_default_for_compare(\n            metadata_default, autogen_context\n        )\n\n        rendered_conn_default = (\n            cast(Any, conn_col_default).arg.text if conn_col_default else None\n        )\n\n        alter_column_op.existing_server_default = conn_col_default\n\n        is_diff = autogen_context.migration_context._compare_server_default(\n            conn_col,\n            metadata_col,\n            rendered_metadata_default,\n            rendered_conn_default,\n        )\n        if is_diff:\n            alter_column_op.modify_server_default = metadata_default\n            log.info(\"Detected server default on column '%s.%s'\", tname, cname)\n\n    return None\n"}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key, original_value in original.items():\n        modified_value = modified.get(key, None)\n        if modified_value is None:\n            # Use None to signal that the element is deleted\n            patch[key] = None\n        elif original_value != modified_value:\n            if type(original_value) == type({}):\n                # Recursively descend objects\n                patch[key] = makepatch(original_value, modified_value)\n            else:\n                # In the case of simple types or arrays we just replace\n                patch[key] = modified_value\n        else:\n            # Don't add anything to patch if there's no change\n            pass\n    for key in modified:\n        if key not in original:\n            patch[key] = modified[key]\n\n    return patch\n"}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        from boto.dynamodb2.results import BatchGetResultSet\n        results = BatchGetResultSet(keys=keys, max_batch_get=self.max_batch_get)\n        results.to_call(self._batch_get, consistent=consistent, attributes=attributes)\n        return results\n"}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        if TYPE_CHECKING:\n            assert self.lex.current_source is not None\n        return self.lex.current_source.literal\n"}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    warn_unused_args(unused_args)\n\n    quant = np.asarray(percent) / 100\n\n    if np.any(quant < 0) or np.any(quant > 1):\n        raise ValueError(\"Percentiles must be between 0 and 100 inclusive\")\n\n    return quantile(array, quant, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                    random_state=random_state, accountant=accountant)\n"}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        from mrjob.py2 import integer_types\n        if not isinstance(amount, integer_types):\n            raise TypeError('amount must be an integer, not %r' % (amount,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(group, string_types):\n            group = str(group)\n        if not isinstance(counter, string_types):\n            counter = str(counter)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n"}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return connect('cognito-sync', region_name,\n                   connection_cls=CognitoSyncConnection, **kw_params)\n"}
{"namespace": "fs._ftp_parse.parse", "completion": "    info = []\n    for line in lines:\n        if not line.strip():\n            continue\n        raw_info = parse_line(line)\n        if raw_info is not None:\n            info.append(raw_info)\n    return info\n"}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint_table = sqla_compat._table_for_constraint(constraint)\n            constraint_table.name = self.table_name\n            constraint_table.schema = self.schema\n\n            return constraint\n        else:\n            raise ValueError(\n                \"constraint cannot be produced; \"\n                \"original constraint is not present\"\n            )\n"}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)\n"}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if not pages_folder:\n        return None\n    is_custom_folder = str(pages_folder) != \"pages\"\n    pages_folder_path = os.path.join(flask.helpers.get_root_path(name), pages_folder)\n    if (use_pages or is_custom_folder) and not os.path.isdir(pages_folder_path):\n        error_msg = f\"\"\"\n        A folder called `{pages_folder}` does not exist. If a folder for pages is not\n        required in your application, set `pages_folder=\"\"`. For example:\n        `app = Dash(__name__,  pages_folder=\"\")`\n        \"\"\"\n        raise exceptions.InvalidConfig(error_msg)\n    return pages_folder_path\n"}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    out = \"%02d-%s-%d\" % (dt.day, _SHORT_MONTHS[dt.month], dt.year)\n    return out.encode(\"ascii\")\n"}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    streaming = bool(output)\n\n    if not streaming:\n        output = StringIO()\n\n    try:\n        encoding = kwargs['encoding']\n    except KeyError:\n        encoding = None\n\n    if isinstance(skip_lines, int):\n        while skip_lines > 0:\n            f.readline()\n            skip_lines -= 1\n    else:\n        raise ValueError('skip_lines argument must be an int')\n\n    writer = agate.csv.writer(output)\n\n    reader = FixedWidthReader(f, schema, encoding=encoding)\n    writer.writerows(reader)\n\n    if not streaming:\n        data = output.getvalue()\n        output.close()\n\n        return data\n\n    # Return empty string when streaming\n    return ''\n"}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None\n"}
{"namespace": "boltons.strutils.multi_replace", "completion": "    m = MultiReplace(sub_map, **kwargs)\n    return m.sub(text)\n"}
{"namespace": "mistune.create_markdown", "completion": "    if renderer == 'ast':\n        # explicit and more similar to 2.x's API\n        renderer = None\n    elif renderer == 'html':\n        renderer = HTMLRenderer(escape=escape)\n\n    inline = InlineParser(hard_wrap=hard_wrap)\n    if plugins is not None:\n        plugins = [import_plugin(n) for n in plugins]\n    return Markdown(renderer=renderer, inline=inline, plugins=plugins)\n"}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    copy_string = string_to_check_against\n\n    if condition_keys is not None:\n        for k, v in condition_keys.items():\n            if isinstance(v, list):\n                v = str(v)  # TODO: how would a multi-valued context value be handled in resource fields?\n            full_key = '${' + k + '}'\n            copy_string = copy_string.replace(full_key, v)\n\n    pattern = _compose_pattern(copy_string)\n    return pattern.match(string_to_check) is not None\n"}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    return (version_gte(version, '2') or\n            version_gte(version, '0.23') and not version_gte(version, '1'))\n"}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        new_databases = self.databases.copy()\n        if name is None:\n            # Pick a unique name for this database\n            suggestion = db.suggest_name()\n            name = suggestion\n        else:\n            suggestion = name\n        i = 2\n        while name in self.databases:\n            name = \"{}_{}\".format(suggestion, i)\n            i += 1\n        db.name = name\n        db.route = route or name\n        new_databases[name] = db\n        # don't mutate! that causes race conditions with live import\n        self.databases = new_databases\n        return db\n"}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    for column_name in df.columns:\n        # All\n        non_nested_enumeration = (\n            df[column_name]\n            .dropna()\n            .map(lambda x: type(x) in types and not any(type(y) in types for y in x))\n        )\n\n        if non_nested_enumeration.all():\n            # Expand and prefix\n            expanded = pd.DataFrame(df[column_name].dropna().tolist())\n            expanded = expanded.add_prefix(column_name + \"_\")\n\n            # Add recursion\n            expanded = expand_mixed(expanded)\n\n            # Drop the expanded\n            df.drop(columns=[column_name], inplace=True)\n\n            df = pd.concat([df, expanded], axis=1)\n    return df\n"}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, bool) or not isinstance(data, (str, int, float,)):\n            self.fail('invalid')\n        value = str(data)\n        return value.strip() if self.trim_whitespace else value\n"}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class is None:\n            try:\n                m = sys.modules[self.module]\n            except KeyError:\n                if import_module:\n                    import importlib\n\n                    m = importlib.import_module(self.module)\n                else:\n                    raise ValueError(f\"Module {self.module} not imported\")\n\n            self._runtime_class = t.cast(\"t.Type[T]\", getattr(m, self.qualname))\n\n        return self._runtime_class\n"}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    if six.PY3 and hasattr(fileobj, 'mode') and 'b' not in fileobj.mode:\n        raise ValueError('File-like object must be opened in binary mode!')\n\n    linear_hash = hashlib.sha256()\n    chunks = []\n    chunk = fileobj.read(chunk_size)\n    while chunk:\n        # It's possible to get a file-like object that has no mode (checked\n        # above) and returns something other than bytes (e.g. str). So here\n        # we try to catch that and encode to bytes.\n        if not isinstance(chunk, bytes):\n            chunk = chunk.encode(getattr(fileobj, 'encoding', '') or 'utf-8')\n        linear_hash.update(chunk)\n        chunks.append(hashlib.sha256(chunk).digest())\n        chunk = fileobj.read(chunk_size)\n    if not chunks:\n        chunks = [hashlib.sha256(b'').digest()]\n    return linear_hash.hexdigest(), bytes_to_hex(tree_hash(chunks))\n"}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        from .utils import display_actor\n        from .utils import format_bytes\n        if not self._startup_invoked:\n            raise Exception(\"render_template() called before await ds.invoke_startup()\")\n        context = context or {}\n        if isinstance(templates, Template):\n            template = templates\n        else:\n            if isinstance(templates, str):\n                templates = [templates]\n            template = self.jinja_env.select_template(templates)\n        body_scripts = []\n        # pylint: disable=no-member\n        for extra_script in pm.hook.extra_body_script(\n            template=template.name,\n            database=context.get(\"database\"),\n            table=context.get(\"table\"),\n            columns=context.get(\"columns\"),\n            view_name=view_name,\n            request=request,\n            datasette=self,\n        ):\n            extra_script = await await_me_maybe(extra_script)\n            if isinstance(extra_script, dict):\n                script = extra_script[\"script\"]\n                module = bool(extra_script.get(\"module\"))\n            else:\n                script = extra_script\n                module = False\n            body_scripts.append({\"script\": Markup(script), \"module\": module})\n\n        extra_template_vars = {}\n        # pylint: disable=no-member\n        for extra_vars in pm.hook.extra_template_vars(\n            template=template.name,\n            database=context.get(\"database\"),\n            table=context.get(\"table\"),\n            columns=context.get(\"columns\"),\n            view_name=view_name,\n            request=request,\n            datasette=self,\n        ):\n            extra_vars = await await_me_maybe(extra_vars)\n            assert isinstance(extra_vars, dict), \"extra_vars is of type {}\".format(\n                type(extra_vars)\n            )\n            extra_template_vars.update(extra_vars)\n\n        async def menu_links():\n            links = []\n            for hook in pm.hook.menu_links(\n                datasette=self,\n                actor=request.actor if request else None,\n                request=request or None,\n            ):\n                extra_links = await await_me_maybe(hook)\n                if extra_links:\n                    links.extend(extra_links)\n            return links\n\n        template_context = {\n            **context,\n            **{\n                \"request\": request,\n                \"crumb_items\": self._crumb_items,\n                \"urls\": self.urls,\n                \"actor\": request.actor if request else None,\n                \"menu_links\": menu_links,\n                \"display_actor\": display_actor,\n                \"show_logout\": request is not None\n                and \"ds_actor\" in request.cookies\n                and request.actor,\n                \"app_css_hash\": self.app_css_hash(),\n                \"zip\": zip,\n                \"body_scripts\": body_scripts,\n                \"format_bytes\": format_bytes,\n                \"show_messages\": lambda: self._show_messages(request),\n                \"extra_css_urls\": await self._asset_urls(\n                    \"extra_css_urls\", template, context, request, view_name\n                ),\n                \"extra_js_urls\": await self._asset_urls(\n                    \"extra_js_urls\", template, context, request, view_name\n                ),\n                \"base_url\": self.setting(\"base_url\"),\n                \"csrftoken\": request.scope[\"csrftoken\"] if request else lambda: \"\",\n            },\n            **extra_template_vars,\n        }\n        if request and request.args.get(\"_context\") and self.setting(\"template_debug\"):\n            return \"<pre>{}</pre>\".format(\n                escape(json.dumps(template_context, default=repr, indent=4))\n            )\n\n        return await template.render_async(template_context)\n"}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        try:\n            instr_lower = instr.lower()\n\n            if instr_lower not in self._cache:\n                instr_asm = instruction.parseString(instr_lower)[0]\n\n                self._cache[instr_lower] = instr_asm\n\n            instr_asm = copy.deepcopy(self._cache[instr_lower])\n\n            # self._check_instruction(instr_asm)\n        except Exception:\n            instr_asm = None\n\n            error_msg = \"Failed to parse instruction: %s\"\n\n            logger.error(error_msg, instr, exc_info=True)\n\n        return instr_asm\n"}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        size = len(payload)\n        if size > self.maxsize:\n            raise NetstringMessageTooLong(size, self.maxsize)\n        data = str(size).encode('ascii') + b':' + payload + b','\n        self.bsock.send(data)\n"}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        with open(self.path, 'rb') as f:\n            return hashlib.sha256(f.read()).hexdigest()\n"}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw={\n                \"comment\": table.comment,\n                \"info\": dict(table.info),\n                \"prefixes\": list(table._prefixes),\n                **table.kwargs,\n            },\n            _reverse=CreateTableOp.from_table(\n                table, _namespace_metadata=_namespace_metadata\n            ),\n        )\n"}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    from .constants import LOREM_IPSUM_WORDS\n\n    words = LOREM_IPSUM_WORDS.split()\n    result = []\n\n    for _ in range(n):\n        next_capitalized = True\n        last_comma = last_fullstop = 0\n        word = None\n        last = None\n        p = []\n\n        # each paragraph contains out of 20 to 100 words.\n        for idx, _ in enumerate(range(randrange(min, max))):\n            while True:\n                word = choice(words)\n                if word != last:\n                    last = word\n                    break\n            if next_capitalized:\n                word = word.capitalize()\n                next_capitalized = False\n            # add commas\n            if idx - randrange(3, 8) > last_comma:\n                last_comma = idx\n                last_fullstop += 2\n                word += \",\"\n            # add end of sentences\n            if idx - randrange(10, 20) > last_fullstop:\n                last_comma = last_fullstop = idx\n                word += \".\"\n                next_capitalized = True\n            p.append(word)\n\n        # ensure that the paragraph ends with a dot.\n        p_str = \" \".join(p)\n\n        if p_str.endswith(\",\"):\n            p_str = p_str[:-1] + \".\"\n        elif not p_str.endswith(\".\"):\n            p_str += \".\"\n\n        result.append(p_str)\n\n    if not html:\n        return \"\\n\\n\".join(result)\n    return markupsafe.Markup(\n        \"\\n\".join(f\"<p>{markupsafe.escape(x)}</p>\" for x in result)\n    )\n"}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        try:\n            file = self.file_factory(path, entry_point=entry_point, chroot=self.chroot)\n        except UnexpectedDirectoryError:\n            assert entry_point is None, \"Directories can't have entry points.\"\n            for root, directories, files in os.walk(path):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    self.add_file(file_path)\n            return\n\n        self.files.add(file)\n        if file.elf:\n            if file.elf.linker_file:\n                self.linker_files.add(file.elf.linker_file)\n                self.files |= file.elf.dependencies\n            else:\n                # Manually set the linker if there isn't one in the program header,\n                # and we've only seen one in all of the files that have been added.\n                if len(self.linker_files) == 1:\n                    [file.elf.linker_file] = self.linker_files\n                    self.files |= file.elf.dependencies\n                    # We definitely don't want a launcher for this file, so clear the linker.\n                    file.elf.linker_file = None\n                else:\n                    logger.warning((\n                        'An ELF binary without a suitable linker candidate was encountered. '\n                        'Either no linker was found or there are multiple conflicting linkers.'\n                    ))\n\n        return file\n"}
{"namespace": "pyinfra.operations.files.rsync", "completion": "    show_rsync_warning()\n\n    try:\n        host.check_can_rsync()\n    except NotImplementedError as e:\n        raise OperationError(*e.args)\n\n    yield RsyncCommand(src, dest, flags)\n"}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n"}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    js_type_name = type_object[\"name\"]\n    js_to_py_types = (\n        map_js_to_py_types_flow_types(type_object=type_object)\n        if is_flow_type\n        else map_js_to_py_types_prop_types(\n            type_object=type_object, indent_num=indent_num\n        )\n    )\n\n    if (\n        \"computed\" in type_object\n        and type_object[\"computed\"]\n        or type_object.get(\"type\", \"\") == \"function\"\n    ):\n        return \"\"\n    if js_type_name in js_to_py_types:\n        if js_type_name == \"signature\":  # This is a Flow object w/ signature\n            return js_to_py_types[js_type_name](indent_num)\n        # All other types\n        return js_to_py_types[js_type_name]()\n    return \"\"\n"}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        value = (\n            log((float(hertz) * 1024) / standard_pitch, 2) + 1.0 / 24\n        ) * 12 + 9  # notes.note_to_int(\"A\")\n        self.name = notes.int_to_note(int(value) % 12)\n        self.octave = int(value / 12) - 6\n        return self\n"}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        for entry in self._ca_entries if ca else self._user_entries:\n            if (entry.key == key and\n                    entry.match_options(client_host, client_addr,\n                                        cert_principals)):\n                return entry.options\n\n        return None\n"}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value.pk)\n        return value.pk\n"}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        metrics = MultiProcessCollector._read_metrics(files)\n        return MultiProcessCollector._accumulate_metrics(metrics, accumulate)\n"}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        if self._architecture == ARCH_X86:\n            candidates = self._find_x86_candidates(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            candidates = self._find_arm_candidates(start_address, end_address)\n        else:\n            raise Exception(\"Architecture not supported.\")\n\n        # Sort and return.\n        return sorted(candidates, key=lambda g: g.address)\n"}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        res = []\n        for n in self.notes:\n            if n.name not in res:\n                res.append(n.name)\n        return res\n"}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])\n"}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n"}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return []\n        return sections\n"}
{"namespace": "alembic.command.ensure_version", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def do_ensure_version(rev, context):\n        context._ensure_version_table()\n        return []\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_ensure_version,\n        as_sql=sql,\n    ):\n        script.run_env()\n"}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    from .env import _sqlite_file_db\n    from sqlalchemy import event\n\n    buf = io.StringIO()\n\n    eng = _sqlite_file_db()\n\n    conn = eng.connect()\n\n    @event.listens_for(conn, \"before_cursor_execute\")\n    def bce(conn, cursor, statement, parameters, context, executemany):\n        buf.write(statement + \"\\n\")\n\n    kw.update({\"connection\": conn})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf\n"}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    try:\n        san_ext = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n        san_ext_value = cast(SubjectAlternativeName, san_ext.value)\n    except ExtensionNotFound:\n        return SubjectAlternativeNameExtension(dns_names=[], ip_addresses=[])\n    except DuplicateExtension:\n        # Fix for https://github.com/nabla-c0d3/sslyze/issues/420\n        # Not sure how browsers behave in this case but having a duplicate extension makes the certificate invalid\n        # so we just return no SANs (likely to make hostname validation fail, which is fine)\n        return SubjectAlternativeNameExtension(dns_names=[], ip_addresses=[])\n\n    dns_names = []\n    ip_addresses = []\n    for san_value in san_ext_value:\n        if isinstance(san_value, IPAddress):\n            ip_addresses.append(str(san_value.value))\n        elif isinstance(san_value, DNSName):\n            dns_names.append(san_value.value)\n        else:\n            pass\n\n    return SubjectAlternativeNameExtension(dns_names=dns_names, ip_addresses=ip_addresses)\n"}
{"namespace": "jinja2.environment.Template.render", "completion": "        if self.environment.is_async:\n            import asyncio\n\n            close = False\n\n            try:\n                loop = asyncio.get_running_loop()\n            except RuntimeError:\n                loop = asyncio.new_event_loop()\n                close = True\n\n            try:\n                return loop.run_until_complete(self.render_async(*args, **kwargs))\n            finally:\n                if close:\n                    loop.close()\n\n        ctx = self.new_context(dict(*args, **kwargs))\n\n        try:\n            return self.environment.concat(self.root_render_func(ctx))  # type: ignore\n        except Exception:\n            self.environment.handle_exception()\n"}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default\n"}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result\n"}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if len(tokens) < 2:\n            return None\n        if tokens[-2] == REDIRECTION_APPEND_SYM:\n            return (RedirectionType.append, tokens[-1])\n        if tokens[-2] == REDIRECTION_SYM:\n            return (RedirectionType.truncate, tokens[-1])\n        if (\n            tokens[-1] == REDIRECTION_APPEND_SYM\n            or tokens[-1] == REDIRECTION_SYM\n        ):\n            return (RedirectionType.quiet, None)\n        return None\n"}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.trigger_word in node.label:\n            yield TriggerNode(trigger, node)\n"}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    if \"[\" in key and key.endswith(\"]\"):\n        keys = []\n        while True:\n            matches = re.findall(KEY_INDEX_RE, key)\n            if matches:\n                key = re.sub(KEY_INDEX_RE, \"\", key)\n                index = int(matches[0])\n                keys.insert(0, index)\n                # keys.insert(0, { keylist_util.INDEX_KEY:index })\n                continue\n            keys.insert(0, key)\n            break\n        return keys\n    return [key]\n"}
{"namespace": "boto.support.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.support.layer1 import SupportConnection\n    return connect('support', region_name,\n                   connection_cls=SupportConnection, **kw_params)\n"}
{"namespace": "imapclient.imap_utf7.decode", "completion": "    if not isinstance(s, bytes):\n        return s\n\n    res = []\n    # Store base64 substring that will be decoded once stepping on end shift character\n    b64_buffer = bytearray()\n    for c in s:\n        # Shift character without anything in buffer -> starts storing base64 substring\n        if c == AMPERSAND_ORD and not b64_buffer:\n            b64_buffer.append(c)\n        # End shift char. -> append the decoded buffer to the result and reset it\n        elif c == DASH_ORD and b64_buffer:\n            # Special case &-, representing \"&\" escaped\n            if len(b64_buffer) == 1:\n                res.append(\"&\")\n            else:\n                res.append(base64_utf7_decode(b64_buffer[1:]))\n            b64_buffer = bytearray()\n        # Still buffering between the shift character and the shift back to ASCII\n        elif b64_buffer:\n            b64_buffer.append(c)\n        # No buffer initialized yet, should be an ASCII printable char\n        else:\n            res.append(chr(c))\n\n    # Decode the remaining buffer if any\n    if b64_buffer:\n        res.append(base64_utf7_decode(b64_buffer[1:]))\n\n    return \"\".join(res)\n"}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = (\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\")\n        else:\n            what = normalise_text_list(what)\n        what_ = \"(%s)\" % (\" \".join(what))\n\n        fname = self._normalise_folder(folder)\n        data = self._command_and_check(\"status\", fname, what_)\n        response = parse_response(data)\n        status_items = response[-1]\n        return dict(as_pairs(status_items))\n"}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package is CALLER_PACKAGE:\n            package_name = caller_package().__name__\n        else:\n            package_name = self.package.__name__\n        return package_name\n"}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        tag = Tag.from_taglike(name)\n        if tag.version is None:\n            tag = tag.make_new_version()\n        labels = {} if labels is None else labels\n        metadata = {} if metadata is None else metadata\n        options = ModelOptions() if options is None else options\n\n        model_fs = fs.open_fs(f\"temp://bentoml_model_{tag.name}\")\n\n        return cls(\n            tag,\n            model_fs,\n            ModelInfo(\n                tag=tag,\n                module=module,\n                api_version=api_version,\n                signatures=signatures,\n                labels=labels,\n                options=options,\n                metadata=metadata,\n                context=context,\n            ),\n            custom_objects=custom_objects,\n            _internal=True,\n        )\n"}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        logger.debug(\"< UNSELECT\")\n        # IMAP4 class has no `unselect` method so we can't use `_command_and_check` there\n        _typ, data = self._imap._simple_command(\"UNSELECT\")\n        return data[0]\n"}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for alternative in _JOBCONF_MAP.get(name, {}).values():\n        if alternative in jobconf:\n            return jobconf[alternative]\n\n    return default\n"}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        from pyramid.config import Configurator\n\n        config = Configurator(registry=registry)\n        return config.get_routes_mapper()\n"}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    parsed = sqlparse.parse(sql)\n    if not parsed:\n        return ()\n\n    # INSERT statements must stop looking for tables at the sign of first\n    # Punctuation. eg: INSERT INTO abc (col1, col2) VALUES (1, 2)\n    # abc is the table name, but if we don't stop at the first lparen, then\n    # we'll identify abc, col1 and col2 as table names.\n    insert_stmt = parsed[0].token_first().value.lower() == 'insert'\n    stream = extract_from_part(parsed[0], stop_at_punctuation=insert_stmt)\n\n    # Kludge: sqlparse mistakenly identifies insert statements as\n    # function calls due to the parenthesized column list, e.g. interprets\n    # \"insert into foo (bar, baz)\" as a function call to foo with arguments\n    # (bar, baz). So don't allow any identifiers in insert statements\n    # to have is_function=True\n    identifiers = extract_table_identifiers(stream,\n                                            allow_functions=not insert_stmt)\n    # In the case 'sche.<cursor>', we get an empty TableReference; remove that\n    return tuple(i for i in identifiers if i.name)\n"}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n"}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [b\"X-GM-LABELS\"])\n        response = self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n        return {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n"}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    sql = \"\\n\".join(\n        line for line in sql.split(\"\\n\") if not line.strip().startswith(\"--\")\n    )\n    sql = sql.strip().lower()\n    if not any(r.match(sql) for r in allowed_sql_res):\n        raise InvalidSql(\"Statement must be a SELECT\")\n    for r, msg in disallawed_sql_res:\n        if r.search(sql):\n            raise InvalidSql(msg)\n"}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        return cls(Condition(key_prefix=key_prefix,\n                             http_error_code=http_error_code), None)\n"}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        from mingus.core import progressions\n        self.empty()\n        chords = progressions.to_chords(shorthand, key)\n        # warning Throw error, not a valid shorthand\n\n        if chords == []:\n            return False\n        notes = chords[0]\n        self.add_notes(notes)\n        return self\n"}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    from .ids import _time_sort_key\n    attempt_to_container_id = attempt_to_container_id or {}\n\n    key_to_error = {}\n\n    for error in errors:\n        # merge by container_id if we know it\n        container_id = error.get('container_id') or (\n            attempt_to_container_id.get(error.get('attempt_id')))\n\n        if container_id:\n            key = ('container_id', container_id)\n        else:\n            key = ('time_key', _time_sort_key(error))\n\n        key_to_error.setdefault(key, {})\n        # assume redundant fields will match\n        key_to_error[key].update(error)\n\n    # wrap sort key to prioritize task errors. See #1429\n    def sort_key(key_and_error):\n        key, error = key_and_error\n\n        # key[0] is 'container_id' or 'time_key'\n        return (bool(error.get('task_error')),\n                key[0] == 'container_id',\n                key[1:])\n\n    return [error for _, error in\n            sorted(key_to_error.items(), key=sort_key, reverse=True)]\n"}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        token = self.get('_csrft_', None)\n        if token is None:\n            token = self.new_csrf_token()\n        return token\n"}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            pass\n"}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    part_size = _MEGABYTE\n    if (default_part_size * MAXIMUM_NUMBER_OF_PARTS) < size_in_bytes:\n        if size_in_bytes > (4096 * _MEGABYTE * 10000):\n            raise ValueError(\"File size too large: %s\" % size_in_bytes)\n        min_part_size = size_in_bytes / 10000\n        power = 3\n        while part_size < min_part_size:\n            part_size = math.ldexp(_MEGABYTE, power)\n            power += 1\n        part_size = int(part_size)\n    else:\n        part_size = default_part_size\n    return part_size\n"}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n"}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    return connect('cloudsearchdomain', region_name,\n                   connection_cls=CloudSearchDomainConnection, **kw_params)\n"}
{"namespace": "lux.action.default.register_default_actions", "completion": "    import lux\n    from lux.action.custom import custom\n    from lux.action.correlation import correlation\n    from lux.action.univariate import univariate\n    from lux.action.enhance import enhance\n    from lux.action.filter import add_filter\n    from lux.action.generalize import generalize\n    from lux.action.temporal import temporal\n\n    # display conditions for default actions\n    no_vis = lambda ldf: (ldf.current_vis is None) or (\n        ldf.current_vis is not None and len(ldf.current_vis) == 0\n    )\n    one_current_vis = lambda ldf: ldf.current_vis is not None and len(ldf.current_vis) == 1\n    multiple_current_vis = lambda ldf: ldf.current_vis is not None and len(ldf.current_vis) > 1\n\n    # globally register default actions\n    lux.config.register_action(\"correlation\", correlation, no_vis)\n    lux.config.register_action(\"distribution\", univariate, no_vis, \"quantitative\")\n    lux.config.register_action(\"occurrence\", univariate, no_vis, \"nominal\")\n    lux.config.register_action(\"temporal\", temporal, no_vis)\n    lux.config.register_action(\"geographical\", univariate, no_vis, \"geographical\")\n\n    lux.config.register_action(\"Enhance\", enhance, one_current_vis)\n    lux.config.register_action(\"Filter\", add_filter, one_current_vis)\n    lux.config.register_action(\"Generalize\", generalize, one_current_vis)\n\n    lux.config.register_action(\"Custom\", custom, multiple_current_vis)\n"}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        lists = self.lists\n        if len(lists) == 1 and not a:\n            return self.lists[0].pop()\n        index = a and a[0]\n        if index == () or index is None or index == -1:\n            ret = lists[-1].pop()\n            if len(lists) > 1 and not lists[-1]:\n                lists.pop()\n        else:\n            list_idx, rel_idx = self._translate_index(index)\n            if list_idx is None:\n                raise IndexError()\n            ret = lists[list_idx].pop(rel_idx)\n            self._balance_list(list_idx)\n        return ret\n"}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for k, v in d.items():\n        if isinstance(v, t.MutableMapping):\n            expand_env_var_in_values(v)\n        elif isinstance(v, str):\n            d[k] = expand_env_var(v)\n        elif isinstance(v, t.Sequence):\n            d[k] = [expand_env_var(i) for i in v]\n"}
{"namespace": "telethon.utils.get_inner_text", "completion": "    text = add_surrogate(text)\n    result = []\n    for e in entities:\n        start = e.offset\n        end = e.offset + e.length\n        result.append(del_surrogate(text[start:end]))\n\n    return result\n"}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        columns = await self.get_columns(self.sql, self.params)\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            # Is every value in this column either null or a JSON array?\n            suggested_facet_sql = \"\"\"\n                select distinct json_type({column})\n                from ({sql})\n                where {column} is not null and {column} != ''\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql\n            )\n            try:\n                results = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                    log_sql_errors=False,\n                )\n                types = tuple(r[0] for r in results.rows)\n                if types in ((\"array\",), (\"array\", None)):\n                    # Now check that first 100 arrays contain only strings\n                    first_100 = [\n                        v[0]\n                        for v in await self.ds.execute(\n                            self.database,\n                            (\n                                \"select {column} from ({sql}) \"\n                                \"where {column} is not null \"\n                                \"and {column} != '' \"\n                                \"and json_array_length({column}) > 0 \"\n                                \"limit 100\"\n                            ).format(column=escape_sqlite(column), sql=self.sql),\n                            self.params,\n                            truncate=False,\n                            custom_time_limit=self.ds.setting(\n                                \"facet_suggest_time_limit_ms\"\n                            ),\n                            log_sql_errors=False,\n                        )\n                    ]\n                    if first_100 and all(\n                        self._is_json_array_of_strings(r) for r in first_100\n                    ):\n                        suggested_facets.append(\n                            {\n                                \"name\": column,\n                                \"type\": \"array\",\n                                \"toggle_url\": self.ds.absolute_url(\n                                    self.request,\n                                    self.ds.urls.path(\n                                        path_with_added_args(\n                                            self.request, {\"_facet_array\": column}\n                                        )\n                                    ),\n                                ),\n                            }\n                        )\n            except (QueryInterrupted, sqlite3.OperationalError):\n                continue\n        return suggested_facets\n"}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print(\n        'The \"falcon-print-routes\" command is deprecated. '\n        'Please use \"falcon-inspect-app\"'\n    )\n    main()\n"}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    languages = languages if (not languages is None) else LANGUAGES\n    return gettext.translation(domain, localedir, languages, fallback=True)\n"}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "        try:\n            from ruamel.yaml import YAML, representer\n            YAML_INSTALLED = True\n        except Exception:\n            YAML_INSTALLED = False\n\n        if YAML_INSTALLED:\n            y_string_buf = io.BytesIO()\n\n            # monkey patch to disable plugins since we don't use them and in\n            # ruamel.yaml versions prior to 0.17.0 the use of __file__ in the\n            # plugin code is incompatible with the pyoxidizer packager\n            YAML.official_plug_ins = lambda a: []  # type: ignore\n\n            # monkey patch to disable aliases\n            representer.RoundTripRepresenter.ignore_aliases = lambda x, y: True  # type: ignore\n\n            yaml = YAML()\n            yaml.default_flow_style = False\n            yaml.explicit_start = True  # type: ignore\n            yaml.allow_unicode = not self.ascii_only\n            yaml.encoding = 'utf-8'\n            yaml.dump(self.data_out, y_string_buf)\n            y_string = y_string_buf.getvalue().decode('utf-8')[:-1]\n\n            if not self.mono:\n                class JcStyle(Style):\n                    styles: CustomColorType = self.custom_colors\n\n                return str(highlight(y_string, YamlLexer(), Terminal256Formatter(style=JcStyle))[0:-1])\n\n            return y_string\n\n        utils.warning_message(['YAML Library not installed. Reverting to JSON output.'])\n        return self.json_out()\n"}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        from kinto.core.utils import merge_dicts\n        if cstruct is not colander.null:\n            defaults = cstruct.get(\"defaults\")\n            requests = cstruct.get(\"requests\")\n            if isinstance(defaults, dict) and isinstance(requests, list):\n                for request in requests:\n                    if isinstance(request, dict):\n                        merge_dicts(request, defaults)\n        return super().deserialize(cstruct)\n"}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        assert actor is None or isinstance(actor, dict), \"actor must be None or a dict\"\n        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            elif isinstance(permission, (tuple, list)) and len(permission) == 2:\n                action, resource = permission\n            else:\n                assert (\n                    False\n                ), \"permission should be string or tuple of two items: {}\".format(\n                    repr(permission)\n                )\n            ok = await self.permission_allowed(\n                actor,\n                action,\n                resource=resource,\n                default=None,\n            )\n            if ok is not None:\n                if ok:\n                    return\n                else:\n                    raise Forbidden(action)\n"}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        result = self.connection.describe_table(self.table_name)\n\n        # Blindly update throughput, since what's on DynamoDB's end is likely\n        # more correct.\n        raw_throughput = result['Table']['ProvisionedThroughput']\n        self.throughput['read'] = int(raw_throughput['ReadCapacityUnits'])\n        self.throughput['write'] = int(raw_throughput['WriteCapacityUnits'])\n\n        if not self.schema:\n            # Since we have the data, build the schema.\n            raw_schema = result['Table'].get('KeySchema', [])\n            raw_attributes = result['Table'].get('AttributeDefinitions', [])\n            self.schema = self._introspect_schema(raw_schema, raw_attributes)\n\n        if not self.indexes:\n            # Build the index information as well.\n            raw_indexes = result['Table'].get('LocalSecondaryIndexes', [])\n            self.indexes = self._introspect_indexes(raw_indexes)\n\n        # Build the global index information as well.\n        raw_global_indexes = result['Table'].get('GlobalSecondaryIndexes', [])\n        self.global_indexes = self._introspect_global_indexes(raw_global_indexes)\n\n        # This is leaky.\n        return result\n"}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d.get(path)\n\n    # the challenge is to identify what is the root key, as dict keys may\n    # contain dot characters themselves\n    parts = path.split(\".\")\n\n    # build a list of all possible root keys from all the path parts\n    candidates = [\".\".join(parts[: i + 1]) for i in range(len(parts))]\n\n    # we start with the longest candidate paths as they're most likely to be the\n    # ones we want if they match\n    root = next((key for key in reversed(candidates) if key in d), None)\n\n    # if no valid root candidates were found, the path is invalid; abandon\n    if root is None or not isinstance(d.get(root), dict):\n        return default\n\n    # we have our root key, extract the new subpath and recur\n    subpath = path.replace(root + \".\", \"\", 1)\n    return find_nested_value(d.get(root), subpath, default=default)\n"}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        types = {\n            \"unique_constraint\": \"unique\",\n            \"foreign_key_constraint\": \"foreignkey\",\n            \"primary_key_constraint\": \"primary\",\n            \"check_constraint\": \"check\",\n            \"column_check_constraint\": \"check\",\n            \"table_or_column_check_constraint\": \"check\",\n        }\n\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(constraint.name),\n            constraint_table.name,\n            schema=constraint_table.schema,\n            type_=types.get(constraint.__visit_name__),\n            _reverse=AddConstraintOp.from_constraint(constraint),\n        )\n"}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "        content = ['']\n        try:\n            while (not self.needs_more_data or self.read_next_chunk()):\n                # We should have all the data we need to form a message in the buffer.\n                # If we need more data to form the next message, this flag will\n                # be reset by a attempt to form a header or content.\n                self.needs_more_data = False\n                # If we can't read a header, read the next chunk.\n                if self.read_state is ReadState.Header and not self.try_read_headers():\n                    self.needs_more_data = True\n                    continue\n                # If we read the header, try the content. If that fails, read\n                # the next chunk.\n                if self.read_state is ReadState.Content and not self.try_read_content(\n                        content):\n                    self.needs_more_data = True\n                    continue\n                # We have the  content\n                break\n\n            # Resize buffer and remove bytes we have read\n            self.trim_buffer_and_resize(self.read_offset)\n            return json.loads(content[0])\n        except ValueError as ex:\n            # response has invalid json object.\n            logger.debug(u'JSON RPC Reader on read_response() encountered exception: %s', ex)\n            raise\n"}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    paths = path.split(\"/\")\n    while len(paths):\n        path = int(paths.pop())\n        secret_exponent = ascend_bip32(bip32_pub_node.subkey_for_path(\"/\".join(paths)), secret_exponent, path)\n    return bip32_pub_node.__class__(\n        bip32_pub_node._chain_code, bip32_pub_node._depth, bip32_pub_node._parent_fingerprint,\n        bip32_pub_node._child_index, secret_exponent=secret_exponent)\n"}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n"}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        key_schema = []\n\n        for part in self.parts:\n            key_schema.append(part.schema())\n\n        return {\n            'IndexName': self.name,\n            'KeySchema': key_schema,\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            }\n        }\n"}
{"namespace": "twtxt.config.Config.create_config", "completion": "        cfgfile_dir = os.path.dirname(cfgfile)\n        if not os.path.exists(cfgfile_dir):\n            os.makedirs(cfgfile_dir)\n\n        cfg = configparser.ConfigParser()\n\n        cfg.add_section(\"twtxt\")\n        cfg.set(\"twtxt\", \"nick\", nick)\n        cfg.set(\"twtxt\", \"twtfile\", twtfile)\n        cfg.set(\"twtxt\", \"twturl\", twturl)\n        cfg.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n        cfg.set(\"twtxt\", \"character_limit\", \"140\")\n        cfg.set(\"twtxt\", \"character_warning\", \"140\")\n\n        cfg.add_section(\"following\")\n        if add_news:\n            cfg.set(\"following\", \"twtxt\", \"https://buckket.org/twtxt_news.txt\")\n\n        conf = cls(cfgfile, cfg)\n        conf.write_config()\n        return conf\n"}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if file is None:\n        file = sys.stderr\n    if tb:\n        tbi = TracebackInfo.from_traceback(tb, limit)\n        print(str(tbi), end='', file=file)\n\n    for line in format_exception_only(etype, value):\n        print(line, end='', file=file)\n"}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        if size < self.buf.tell():\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while size > self.buf.tell():\n            data = self.reader.read(1024)\n            if not data:\n                break\n            self.buf.write(data)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n"}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        self._checkClosed()\n        for line in lines:\n            self.write(line)\n"}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result\n"}
{"namespace": "falcon.request.Request.subdomain", "completion": "        subdomain, sep, remainder = self.host.partition('.')\n        return subdomain if sep else None\n"}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        t = sa_schema.Table(\n            tablename or \"no_table\",\n            self.metadata(),\n            schema=schema,\n        )\n        kw[\"_table\"] = t\n        idx = sa_schema.Index(\n            name,\n            *[util.sqla_compat._textual_index_column(t, n) for n in columns],\n            **kw,\n        )\n        return idx\n"}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    if m == values.unset:\n        return {}\n\n    def flatten_dict(d, result=None, prv_keys=None):\n        if result is None:\n            result = {}\n\n        if prv_keys is None:\n            prv_keys = []\n\n        for k, v in d.items():\n            if isinstance(v, dict):\n                flatten_dict(v, result, prv_keys + [k])\n            else:\n                result[\".\".join(prv_keys + [k])] = v\n\n        return result\n\n    if isinstance(m, dict):\n        flattened = flatten_dict(m)\n        return {\"{}.{}\".format(prefix, k): v for k, v in flattened.items()}\n\n    return {}\n"}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    request = event.request\n    settings = request.registry.settings\n    if not settings.get(\"account_validation\", False):\n        return\n\n    for impacted_object in event.impacted_objects:\n        old_account = impacted_object[\"old\"]\n        account = impacted_object[\"new\"]\n        if old_account.get(\"validated\", True) or not account.get(\"validated\", False):\n            # It's not an account activation, bail.\n            continue\n\n        # Send a confirmation email.\n        Emailer(request, account).send_confirmation()\n"}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        result = await self._read_from(\n            self._iter_delimited(delimiter, size_hint=size or 0), size\n        )\n\n        if consume_delimiter:\n            await self._consume_delimiter(delimiter)\n\n        return result\n"}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        users_list = self.view.users\n        recipients = text.rsplit(\",\", 1)\n\n        # Use the most recent recipient for autocomplete.\n        previous_recipients = f\"{recipients[0]}, \" if len(recipients) > 1 else \"\"\n        latest_text = recipients[-1].strip()\n\n        matching_users = [\n            user for user in users_list if match_user_name_and_email(user, latest_text)\n        ]\n\n        # Append the potential autocompleted recipients to the string\n        # containing the previous recipients.\n        updated_recipients = [\n            f\"{previous_recipients}{user['full_name']} <{user['email']}>\"\n            for user in matching_users\n        ]\n\n        user_names = [user[\"full_name\"] for user in matching_users]\n\n        return self._process_typeaheads(updated_recipients, state, user_names)\n"}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            if self.query_string:\n                self._cached_relative_uri = (\n                    self.app + self.path + '?' + self.query_string\n                )\n            else:\n                self._cached_relative_uri = self.app + self.path\n\n        return self._cached_relative_uri\n"}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        self.enable = True\n        self.parsed = False\n        if self.log_print:\n            self.overload_print()\n        if self.include_files is not None and self.exclude_files is not None:\n            raise Exception(\"include_files and exclude_files can't be both specified!\")\n        self.config()\n        self._tracer.start()\n"}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        from boto.s3.tagging import Tags\n        response = self.get_xml_tags(headers)\n        tags = Tags()\n        h = handler.XmlHandler(tags, self)\n        if not isinstance(response, bytes):\n            response = response.encode('utf-8')\n        xml.sax.parseString(response, h)\n        return tags\n"}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        package = self.maybe_dotted(package)\n        if package is None:  # pragma: no cover\n            package = caller_package()\n\n        ctorkw = {'config': self}\n        ctorkw.update(kw)\n\n        scanner = self.venusian.Scanner(**ctorkw)\n\n        scanner.scan(\n            package, categories=categories, onerror=onerror, ignore=ignore\n        )\n"}
{"namespace": "pyinfra.operations.files.get", "completion": "    if add_deploy_dir and state.cwd:\n        dest = os.path.join(state.cwd, dest)\n\n    if create_local_dir:\n        local_pathname = os.path.dirname(dest)\n        if not os.path.exists(local_pathname):\n            os.makedirs(local_pathname)\n\n    remote_file = host.get_fact(File, path=src)\n\n    # No remote file, so assume exists and download it \"blind\"\n    if not remote_file or force:\n        yield FileDownloadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))\n\n    # No local file, so always download\n    elif not os.path.exists(dest):\n        yield FileDownloadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))\n\n    # Remote file exists - check if it matches our local\n    else:\n        local_sum = get_file_sha1(dest)\n        remote_sum = host.get_fact(Sha1File, path=src)\n\n        # Check sha1sum, upload if needed\n        if local_sum != remote_sum:\n            yield FileDownloadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))\n"}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        from ydata_profiling.report.presentation.flavours.html import templates\n        return templates.template(\"diagram.html\").render(**self.content)\n"}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        if self._imap.state != \"AUTH\":\n            raise exceptions.IllegalStateError(\n                \"ENABLE command illegal in state %s\" % self._imap.state\n            )\n\n        resp = self._raw_command_untagged(\n            b\"ENABLE\",\n            [to_bytes(c) for c in capabilities],\n            uid=False,\n            response_name=\"ENABLED\",\n            unpack=True,\n        )\n        if not resp:\n            return []\n        return resp.split()\n"}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        from diffprivlib.utils import BudgetError\n        check_epsilon_delta(epsilon, delta)\n        if self.epsilon == float(\"inf\") and self.delta == 1:\n            return True\n\n        if 0 < epsilon < self.__min_epsilon:\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon} if non-zero, got {epsilon}.\")\n\n        spent_budget = self.spent_budget + [(epsilon, delta)]\n\n        if Budget(self.epsilon, self.delta) >= self.total(spent_budget=spent_budget):\n            return True\n\n        raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                          f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n"}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    from .datetime_util import format_criteria_date\n    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"no criteria specified\")\n    if not charset:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [to_bytes(criteria, charset)]\n\n    out = []\n    for item in criteria:\n        if isinstance(item, int):\n            out.append(str(item).encode(\"ascii\"))\n        elif isinstance(item, (datetime, date)):\n            out.append(format_criteria_date(item))\n        elif isinstance(item, (list, tuple)):\n            # Process nested criteria list and wrap in parens.\n            inner = _normalise_search_criteria(item)\n            inner[0] = b\"(\" + inner[0]\n            inner[-1] = inner[-1] + b\")\"\n            out.extend(inner)  # flatten\n        else:\n            out.append(_quoted.maybe(to_bytes(item, charset)))\n    return out\n"}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "    from pyinfra.progress import progress_spinner\n    hosts = [\n        host\n        for host in state.inventory\n        if state.is_host_in_limit(host)  # these are the hosts to activate (\"initially connect to\")\n    ]\n\n    greenlet_to_host = {state.pool.spawn(host.connect): host for host in hosts}\n\n    with progress_spinner(greenlet_to_host.values()) as progress:\n        for greenlet in gevent.iwait(greenlet_to_host.keys()):\n            host = greenlet_to_host[greenlet]\n            progress(host)\n\n    # Get/set the results\n    failed_hosts = set()\n\n    for greenlet, host in greenlet_to_host.items():\n        # Raise any unexpected exception\n        greenlet.get()\n\n        if host.connected:\n            state.activate_host(host)\n        else:\n            failed_hosts.add(host)\n\n    # Remove those that failed, triggering FAIL_PERCENT check\n    state.fail_hosts(failed_hosts, activated_count=len(hosts))\n"}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    for couple in keys:\n        if key in couple:\n            accidentals = keys.index(couple) - 7\n            return accidentals\n"}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '%s(%s)' % (\n            self.__class__.__name__,\n            ', '.join(('%s=%r' % (k, getattr(self, k))\n                       for k in self._FIELDS\n                       if getattr(self, k) is not None)))\n"}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    temp_acc = BudgetAccountant()\n\n    # old = stats until now\n    # new = the current increment\n    # updated = the aggregated stats\n    last_sum = last_mean * last_sample_count\n\n    new_mean = nanmean(X, epsilon=epsilon, axis=0, bounds=bounds, random_state=random_state, accountant=temp_acc)\n    new_sample_count = np.sum(~np.isnan(X), axis=0)\n    new_sum = new_mean * new_sample_count\n    updated_sample_count = last_sample_count + new_sample_count\n\n    updated_mean = (last_sum + new_sum) / updated_sample_count\n\n    if last_variance is None:\n        updated_variance = None\n    else:\n        new_unnormalized_variance = nanvar(X, epsilon=epsilon, axis=0, bounds=bounds, random_state=random_state,\n                                           accountant=temp_acc) * new_sample_count\n        last_unnormalized_variance = last_variance * last_sample_count\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            last_over_new_count = last_sample_count / new_sample_count\n            updated_unnormalized_variance = (\n                last_unnormalized_variance + new_unnormalized_variance +\n                last_over_new_count / updated_sample_count *\n                (last_sum / last_over_new_count - new_sum) ** 2)\n\n        zeros = last_sample_count == 0\n        updated_unnormalized_variance[zeros] = new_unnormalized_variance[zeros]\n        updated_variance = updated_unnormalized_variance / updated_sample_count\n\n    return updated_mean, updated_variance, updated_sample_count\n"}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    results = []\n\n    for path in combine_lists(*path_seqs):\n        expanded = expand_path(path)\n        # if we can't expand a glob, leave as-is (maybe it refers to\n        # S3 or HDFS)\n        paths = sorted(glob.glob(expanded)) or [expanded]\n\n        results.extend(paths)\n\n    return results\n"}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        if name in self.routes:\n            oldroute = self.routes[name]\n            if oldroute in self.routelist:\n                self.routelist.remove(oldroute)\n\n        route = Route(name, pattern, factory, predicates, pregenerator)\n        if not static:\n            self.routelist.append(route)\n        else:\n            self.static_routes.append(route)\n\n        self.routes[name] = route\n        return route\n"}
{"namespace": "feedparser.api._open_resource", "completion": "    from . import http\n    if hasattr(url_file_stream_or_string, 'read'):\n        return url_file_stream_or_string.read()\n\n    if isinstance(url_file_stream_or_string, str) \\\n       and urllib.parse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp', 'file', 'feed'):\n        return http.get(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result)\n\n    # try to open with native open function (if url_file_stream_or_string is a filename)\n    try:\n        with open(url_file_stream_or_string, 'rb') as f:\n            data = f.read()\n    except (IOError, UnicodeEncodeError, TypeError, ValueError):\n        # if url_file_stream_or_string is a str object that\n        # cannot be converted to the encoding returned by\n        # sys.getfilesystemencoding(), a UnicodeEncodeError\n        # will be thrown\n        # If url_file_stream_or_string is a string that contains NULL\n        # (such as an XML document encoded in UTF-32), TypeError will\n        # be thrown.\n        pass\n    else:\n        return data\n\n    # treat url_file_stream_or_string as string\n    if not isinstance(url_file_stream_or_string, bytes):\n        return url_file_stream_or_string.encode('utf-8')\n    return url_file_stream_or_string\n"}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    migration_script = produce_migrations(context, metadata)\n    return migration_script.upgrade_ops.as_diffs()\n"}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        all_dependencies = set()\n        unprocessed_dependencies = set(self.direct_dependencies)\n        while len(unprocessed_dependencies):\n            all_dependencies |= unprocessed_dependencies\n            new_dependencies = set()\n            for dependency in unprocessed_dependencies:\n                if dependency.elf:\n                    new_dependencies |= set(\n                        dependency.elf.find_direct_dependencies(self.linker_file))\n            unprocessed_dependencies = new_dependencies - all_dependencies\n        return all_dependencies\n"}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        result = self.render(value, system_values, request=request)\n        return self._make_response(result, request)\n"}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        expected = self._build_filters(expected, using=FILTER_OPERATORS)\n        raw_key = self._encode_keys(kwargs)\n\n        try:\n            self.connection.delete_item(self.table_name, raw_key,\n                                        expected=expected,\n                                        conditional_operator=conditional_operator)\n        except exceptions.ConditionalCheckFailedException:\n            return False\n\n        return True\n"}
{"namespace": "threatingestor.state.State.save_state", "completion": "        logger.debug(f\"Updating state for '{name}' to '{state}'\")\n        self.cursor.execute('INSERT OR REPLACE INTO states (name, state) values (?, ?)', (name, state))\n        self.conn.commit()\n"}
{"namespace": "boto.awslambda.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.awslambda.layer1 import AWSLambdaConnection\n    return connect('awslambda', region_name,\n                   connection_cls=AWSLambdaConnection, **kw_params)\n"}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        # Enqueue None to optimistically unblock background threads so\n        # they can check for the cancellation flag.\n        self.request_queue.put(None)\n\n        # Wait for request thread to finish with a timeout in seconds.\n        self.request_thread.join(1)\n\n        # close the underlying writer.\n        self.writer.close()\n        logger.info('Shutting down Json rpc client.')\n"}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        for record in self.entries:\n            data.extend(record.dump())\n\n        used_page_length = len(data) + 4\n        assert 0 <= used_page_length < self._tree_conf.page_size\n        next_page = 0 if self.next_page is None else self.next_page\n        header = (\n            self._node_type_int.to_bytes(1, ENDIAN) +\n            used_page_length.to_bytes(3, ENDIAN) +\n            next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n\n        data = bytearray(header) + data\n\n        padding = self._tree_conf.page_size - len(data)\n        assert padding >= 0\n        data.extend(bytearray(padding))\n        assert len(data) == self._tree_conf.page_size\n\n        return data\n"}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n"}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    from sslyze.plugins.certificate_info._certificate_utils import get_common_names\n    from sslyze.plugins.certificate_info._certificate_utils import parse_subject_alternative_name_extension\n    try:\n        cert_subject = certificate.subject\n    except ValueError:\n        # Cryptography could not parse the certificate https://github.com/nabla-c0d3/sslyze/issues/495\n        return False\n\n    subj_alt_name_ext = parse_subject_alternative_name_extension(certificate)\n    subj_alt_name_as_list = [(\"DNS\", name) for name in subj_alt_name_ext.dns_names]\n    subj_alt_name_as_list.extend([(\"IP Address\", ip) for ip in subj_alt_name_ext.ip_addresses])\n\n    certificate_names = {\n        \"subject\": (tuple([(\"commonName\", name) for name in get_common_names(cert_subject)]),),\n        \"subjectAltName\": tuple(subj_alt_name_as_list),\n    }\n    # CertificateError is raised on failure\n    try:\n        match_hostname(certificate_names, server_hostname)  # type: ignore\n        return True\n    except CertificateError:\n        return False\n"}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        repeaters = itertools.starmap(itertools.repeat, self.iteritems())\n        return itertools.chain.from_iterable(repeaters)\n"}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package\n"}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    if not os.path.exists(filename):\n        raise MissingFileError('The \"%s\" file was not found.' % filename)\n\n    with open(filename, 'rb') as f:\n        first_four_bytes = f.read(4)\n\n    return first_four_bytes == b'\\x7fELF'\n"}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(\n            Enqueue(\n                name=name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )\n"}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        fragments = urlparse(link.rstrip(\"/\")).fragment.split(\"/\")\n        len_fragments = len(fragments)\n        parsed_link = ParsedNarrowLink()\n\n        if len_fragments == 3 and fragments[1] == \"stream\":\n            stream_data = cls._decode_stream_data(fragments[2])\n            parsed_link = dict(narrow=\"stream\", stream=stream_data)\n\n        elif (\n            len_fragments == 5 and fragments[1] == \"stream\" and fragments[3] == \"topic\"\n        ):\n            stream_data = cls._decode_stream_data(fragments[2])\n            topic_name = hash_util_decode(fragments[4])\n            parsed_link = dict(\n                narrow=\"stream:topic\", stream=stream_data, topic_name=topic_name\n            )\n\n        elif len_fragments == 5 and fragments[1] == \"stream\" and fragments[3] == \"near\":\n            stream_data = cls._decode_stream_data(fragments[2])\n            message_id = cls._decode_message_id(fragments[4])\n            parsed_link = dict(\n                narrow=\"stream:near\", stream=stream_data, message_id=message_id\n            )\n\n        elif (\n            len_fragments == 7\n            and fragments[1] == \"stream\"\n            and fragments[3] == \"topic\"\n            and fragments[5] == \"near\"\n        ):\n            stream_data = cls._decode_stream_data(fragments[2])\n            topic_name = hash_util_decode(fragments[4])\n            message_id = cls._decode_message_id(fragments[6])\n            parsed_link = dict(\n                narrow=\"stream:topic:near\",\n                stream=stream_data,\n                topic_name=topic_name,\n                message_id=message_id,\n            )\n\n        return parsed_link\n"}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        from pyramid.response import _get_response_factory\n        f = _get_response_factory(self.registry)\n        return f(self)\n"}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "    from awesome_autodl.data_cls import BibAbbreviations\n\n    xfile = str(get_bib_abbrv_file())\n    return BibAbbreviations(xfile)\n"}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    if module_id:\n        config_path = get_data_path(module_id)\n    else:\n        profile = coordinator.profile\n        config_path = get_base_path() / 'profiles' / profile\n    if not config_path.exists():\n        config_path.mkdir(parents=True)\n    return config_path / \"config.{}\".format(ext)\n"}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start > 0.0\n    assert start < end\n    assert step > 0.0\n\n    bound = start\n    buckets: list[float] = []\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)\n"}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.cookies[self.cookie_name] = token\n\n        def set_cookie(request, response):\n            self.cookie_profile.set_cookies(response, token)\n\n        request.add_response_callback(set_cookie)\n        return token\n"}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        from pyramid.config.actions import action_method\n        directives = getattr(self.registry, '_directives', {})\n        c = directives.get(name)\n        if c is None:\n            raise AttributeError(name)\n        c, action_wrap = c\n        if action_wrap:\n            c = action_method(c)\n        # Create a bound method (works on both Py2 and Py3)\n        # http://stackoverflow.com/a/1015405/209039\n        m = c.__get__(self, self.__class__)\n        return m\n"}
{"namespace": "fs.path.recursepath", "completion": "    if path in \"/\":\n        return [\"/\"]\n\n    path = abspath(normpath(path)) + \"/\"\n\n    paths = [\"/\"]\n    find = path.find\n    append = paths.append\n    pos = 1\n    len_path = len(path)\n\n    while pos < len_path:\n        pos = find(\"/\", pos)\n        append(path[:pos])\n        pos += 1\n\n    if reverse:\n        return paths[::-1]\n    return paths\n"}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )\n"}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    assert (len(sentences) > 0)\n    assert (n > 0)\n\n    words = set()\n    for sentence in sentences:\n        words.update(_get_ngrams(n, _split_into_words([sentence])))\n\n    return words\n"}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        if not key_name:\n            raise ValueError('Empty key names are not allowed')\n        return self.key_class(self, key_name)\n"}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    warnings.warn(\n        \"The host_info_getter is deprecated. \"\n        \"Please use the `additional_host_info` argument\"\n        \" in the Experiment constructor.\",\n        DeprecationWarning,\n    )\n    name = name or func.__name__\n    host_info_gatherers[name] = func\n    return func\n"}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        candi_prob = {i: self.probability(i) for i in self.candidates(word)}\n        sort_candi_prob = sorted(candi_prob.items(), key=operator.itemgetter(1))\n        return sort_candi_prob[-1][0]\n"}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for config_name, config in ingredient.named_configs.items():\n                config_name = join_paths(ingredient.path, config_name)\n                config_name = self.post_process_name(config_name, ingredient)\n                yield config_name, config\n"}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        if key not in self.data:\n            self.data[key] = set()\n        self.data[key].add(val)\n        if val not in self.inv.data:\n            self.inv.data[val] = set()\n        self.inv.data[val].add(key)\n"}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    # Split off optional timezone\n    dt_match = DTM_TZ_RE.match(value)\n    if not dt_match:\n        raise ValueError(\"Malformed HL7 datetime {0}\".format(value))\n    dtm = dt_match.group(1)\n    tzh = dt_match.group(2)\n    tzm = dt_match.group(3)\n    if tzh and tzm:\n        minutes = int(tzh) * 60\n        minutes += math.copysign(int(tzm), minutes)\n        tzinfo = _UTCOffset(minutes)\n    else:\n        tzinfo = None\n\n    precision = len(dtm)\n\n    if precision >= 4:\n        year = int(dtm[0:4])\n    else:\n        raise ValueError(\"Malformed HL7 datetime {0}\".format(value))\n\n    if precision >= 6:\n        month = int(dtm[4:6])\n    else:\n        month = 1\n\n    if precision >= 8:\n        day = int(dtm[6:8])\n    else:\n        day = 1\n\n    if precision >= 10:\n        hour = int(dtm[8:10])\n    else:\n        hour = 0\n\n    if precision >= 12:\n        minute = int(dtm[10:12])\n    else:\n        minute = 0\n\n    if precision >= 14:\n        delta = datetime.timedelta(seconds=float(dtm[12:]))\n        second = delta.seconds\n        microsecond = delta.microseconds\n    else:\n        second = 0\n        microsecond = 0\n\n    return datetime.datetime(\n        year, month, day, hour, minute, second, microsecond, tzinfo=tzinfo\n    )\n"}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        item_bytes = spendable.tx_hash + struct.pack(\"<L\", spendable.tx_out_index)\n        self.add_item(item_bytes)\n"}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.soft_miss_count += 1\n                self[key] = default\n                return default\n"}
{"namespace": "kinto.core.errors.http_error", "completion": "    errno = errno or ERRORS.UNDEFINED\n\n    if isinstance(errno, Enum):\n        errno = errno.value\n\n    body = {\n        \"code\": code or httpexception.code,\n        \"errno\": errno,\n        \"error\": error or httpexception.title,\n        \"message\": message,\n        \"info\": info,\n        \"details\": details or colander.drop,\n    }\n\n    response = httpexception\n    response.errno = errno\n    response.json = ErrorSchema().deserialize(body)\n    response.content_type = \"application/json\"\n    return response\n"}
{"namespace": "mingus.core.keys.relative_major", "completion": "    for couple in keys:\n        if key == couple[1]:\n            return couple[0]\n    raise NoteFormatError(\"'%s' is not a minor key\" % key)\n"}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for k, v in iterate_flattened(dotted_dict):\n        set_by_dotted_path(nested_dict, k, v)\n    return nested_dict\n"}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        configurator = self.__class__(\n            registry=self.registry,\n            package=package,\n            root_package=self.root_package,\n            autocommit=self.autocommit,\n            route_prefix=self.route_prefix,\n            introspection=self.introspection,\n        )\n        configurator.basepath = self.basepath\n        configurator.includepath = self.includepath\n        configurator.info = self.info\n        return configurator\n"}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        m = _SSH_URI_RE.match(path_glob)\n        addr = m.group('hostname')\n        path_to_ls = m.group('filesystem_path')\n\n        p = self._ssh_launch(\n            addr, ['find', '-L', path_to_ls, '-type', 'f'])\n\n        for line in p.stdout:\n            path = to_unicode(line).rstrip('\\n')\n            yield 'ssh://%s%s' % (addr, path)\n\n        self._ssh_finish_run(p)\n"}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f, default_flow_style=False)\n    else:\n        json.dump(conf, f, indent=2)\n    f.flush()\n"}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        try:\n            code = self.client.get(self.prefix + bucket.key)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n        else:\n            bucket.bytecode_from_string(code)\n"}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        from rest_framework.utils.serializer_helpers import BindingDict\n        fields = BindingDict(self)\n        for key, value in self.get_fields().items():\n            fields[key] = value\n        return fields\n"}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        response = self.fetch(messages, [\"FLAGS\"])\n        return self._filter_fetch_dict(response, b\"FLAGS\")\n"}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, \"w\") as fp:\n            fp.write(authorizer.refresh_token)\n"}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    def _perform_make_grouping_like(value, next_values):\n        if isinstance(value, (tuple, list)):\n            return list(\n                _perform_make_grouping_like(el, next_values)\n                for i, el in enumerate(value)\n            )\n\n        if isinstance(value, dict):\n            return {\n                k: _perform_make_grouping_like(v, next_values)\n                for i, (k, v) in enumerate(value.items())\n            }\n\n        return next_values.pop(0)\n\n    if not isinstance(flat_values, list):\n        raise ValueError(\n            \"The flat_values argument must be a list. \"\n            f\"Received value of type {type(flat_values)}\"\n        )\n\n    expected_length = len(flatten_grouping(schema))\n    if len(flat_values) != expected_length:\n        raise ValueError(\n            f\"The specified grouping pattern requires {expected_length} \"\n            f\"elements but received {len(flat_values)}\\n\"\n            f\"    Grouping pattern: {repr(schema)}\\n\"\n            f\"    Values: {flat_values}\"\n        )\n\n    return _perform_make_grouping_like(schema, list(flat_values))\n"}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "    with open(service_definition_file(servicename), encoding=\"UTF-8\") as definition_file:\n        service_definition = json.loads(definition_file.read())\n        return service_definition['operations'][operationname]\n"}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "    @classmethod\n    def text(cls, body, status=200, headers=None):\n        return cls(\n            str(body),\n            status=status,\n            headers=headers,\n"}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    cfg = _testing_config()\n    with open(cfg.config_file_name, \"w\") as f:\n        f.write(text)\n    return cfg\n"}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for name, regex in _regexen.items():\n        for rx_match in regex.finditer(password):\n            matches.append({\n                'pattern': 'regex',\n                'token': rx_match.group(0),\n                'i': rx_match.start(),\n                'j': rx_match.end()-1,\n                'regex_name': name,\n                'regex_match': rx_match,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n"}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self\n"}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    from boto.regioninfo import connect\n    return connect('cloudwatch', region_name,\n                   connection_cls=CloudWatchConnection, **kw_params)\n"}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        known_web_packages = {\"flask\"}  # to pick webview over service_only\n        recipes_with_deps_lists = expand_dependencies(recipes, ctx)\n        acceptable_bootstraps = cls.get_usable_bootstraps_for_recipes(\n            recipes, ctx\n        )\n\n        def have_dependency_in_recipes(dep):\n            for dep_list in recipes_with_deps_lists:\n                if dep in dep_list:\n                    return True\n            return False\n\n        # Special rule: return SDL2 bootstrap if there's an sdl2 dep:\n        if (have_dependency_in_recipes(\"sdl2\") and\n                \"sdl2\" in [b.name for b in acceptable_bootstraps]\n                ):\n            info('Using sdl2 bootstrap since it is in dependencies')\n            return cls.get_bootstrap(\"sdl2\", ctx)\n\n        # Special rule: return \"webview\" if we depend on common web recipe:\n        for possible_web_dep in known_web_packages:\n            if have_dependency_in_recipes(possible_web_dep):\n                # We have a web package dep!\n                if \"webview\" in [b.name for b in acceptable_bootstraps]:\n                    info('Using webview bootstrap since common web packages '\n                         'were found {}'.format(\n                             known_web_packages.intersection(recipes)\n                         ))\n                    return cls.get_bootstrap(\"webview\", ctx)\n\n        prioritized_acceptable_bootstraps = sorted(\n            list(acceptable_bootstraps),\n            key=functools.cmp_to_key(_cmp_bootstraps_by_priority)\n        )\n\n        if prioritized_acceptable_bootstraps:\n            info('Using the highest ranked/first of these: {}'\n                 .format(prioritized_acceptable_bootstraps[0].name))\n            return prioritized_acceptable_bootstraps[0]\n        return None\n"}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    translated_jobconf = jobconf.copy()\n    translation_warnings = {}\n\n    for variable, value in jobconf.items():\n        if hadoop_version:\n            variants = [translate_jobconf(variable, hadoop_version)]\n        else:\n            variants = translate_jobconf_for_all_versions(variable)\n\n        for variant in variants:\n            if variant in jobconf:\n                # this happens if variant == variable or\n                # if the variant was in jobconf to start with\n                continue\n\n            translated_jobconf[variant] = value\n\n            if hadoop_version:\n                translation_warnings[variable] = variant\n\n    if translation_warnings:\n        log.warning(\"Detected hadoop configuration property names that\"\n                    \" do not match hadoop version %s:\"\n                    \"\\nThe have been translated as follows\\n %s\",\n                    hadoop_version,\n                    '\\n'.join([\n                        \"%s: %s\" % (variable, variant) for variable, variant\n                        in sorted(translation_warnings.items())]))\n\n    return translated_jobconf\n"}
{"namespace": "boto.configservice.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.configservice.layer1 import ConfigServiceConnection\n    return connect('configservice', region_name,\n                   connection_cls=ConfigServiceConnection, **kw_params)\n"}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if name in self.names:\n            self.remove(name)\n        self.names.append(name)\n        self.name2val[name] = val\n        if after is None and before is None:\n            before = self.default_before\n            after = self.default_after\n        if after is not None:\n            if not is_nonstr_iter(after):\n                after = (after,)\n            self.name2after[name] = after\n            self.order += [(u, name) for u in after]\n            self.req_after.add(name)\n        if before is not None:\n            if not is_nonstr_iter(before):\n                before = (before,)\n            self.name2before[name] = before\n            self.order += [(name, o) for o in before]\n            self.req_before.add(name)\n"}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            sqla_compat._get_index_expressions(index),\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )\n"}
{"namespace": "boto.ses.connect_to_region", "completion": "    from boto.regioninfo import connect\n    return connect('ses', region_name, connection_cls=SESConnection,\n                   **kw_params)\n"}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if not self.snapshot:\n            raise Exception('Table name requires snapshot')\n        if not self.snapshot.hash:\n            raise Exception('Snapshot hash is empty.')\n\n        if old:\n            return 'stellar_%s_%s_%s' % (\n                self.table_name,\n                self.snapshot.hash,\n                postfix\n            )\n        else:\n            return 'stellar_%s' % hashlib.md5(\n                ('%s|%s|%s' % (\n                    self.table_name,\n                    self.snapshot.hash,\n                    postfix\n                )).encode('utf-8')\n            ).hexdigest()[0:16]\n"}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n            uid = uid or id\n        member = ChatMember(self, name=name, alias=alias, uid=uid,\n                            vendor_specific=vendor_specific, description=description,\n                            middleware=middleware)\n        self.members.append(member)\n        return member\n"}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "        id_to_rev = self._revision_map\n\n        def get_ancestors(rev_id):\n            return {\n                r.revision\n                for r in self._get_ancestor_nodes([id_to_rev[rev_id]])\n            }\n\n        todo = {d.revision for d in revisions}\n\n        # Use revision map (ordered dict) key order to pre-sort.\n        inserted_order = list(self._revision_map)\n\n        current_heads = list(\n            sorted(\n                {d.revision for d in heads if d.revision in todo},\n                key=inserted_order.index,\n            )\n        )\n        ancestors_by_idx = [get_ancestors(rev_id) for rev_id in current_heads]\n\n        output = []\n\n        current_candidate_idx = 0\n        while current_heads:\n            candidate = current_heads[current_candidate_idx]\n\n            for check_head_index, ancestors in enumerate(ancestors_by_idx):\n                # scan all the heads.  see if we can continue walking\n                # down the current branch indicated by current_candidate_idx.\n                if (\n                    check_head_index != current_candidate_idx\n                    and candidate in ancestors\n                ):\n                    current_candidate_idx = check_head_index\n                    # nope, another head is dependent on us, they have\n                    # to be traversed first\n                    break\n            else:\n                # yup, we can emit\n                if candidate in todo:\n                    output.append(candidate)\n                    todo.remove(candidate)\n\n                # now update the heads with our ancestors.\n\n                candidate_rev = id_to_rev[candidate]\n                assert candidate_rev is not None\n\n                heads_to_add = [\n                    r\n                    for r in candidate_rev._normalized_down_revisions\n                    if r in todo and r not in current_heads\n                ]\n\n                if not heads_to_add:\n                    # no ancestors, so remove this head from the list\n                    del current_heads[current_candidate_idx]\n                    del ancestors_by_idx[current_candidate_idx]\n                    current_candidate_idx = max(current_candidate_idx - 1, 0)\n                else:\n                    if (\n                        not candidate_rev._normalized_resolved_dependencies\n                        and len(candidate_rev._versioned_down_revisions) == 1\n                    ):\n                        current_heads[current_candidate_idx] = heads_to_add[0]\n\n                        # for plain movement down a revision line without\n                        # any mergepoints, branchpoints, or deps, we\n                        # can update the ancestors collection directly\n                        # by popping out the candidate we just emitted\n                        ancestors_by_idx[current_candidate_idx].discard(\n                            candidate\n                        )\n\n                    else:\n                        # otherwise recalculate it again, things get\n                        # complicated otherwise.  This can possibly be\n                        # improved to not run the whole ancestor thing\n                        # each time but it was getting complicated\n                        current_heads[current_candidate_idx] = heads_to_add[0]\n                        current_heads.extend(heads_to_add[1:])\n                        ancestors_by_idx[\n                            current_candidate_idx\n                        ] = get_ancestors(heads_to_add[0])\n                        ancestors_by_idx.extend(\n                            get_ancestors(head) for head in heads_to_add[1:]\n                        )\n\n        assert not todo\n        return output\n"}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        error_str = self.error_type\n        if self.message:\n            error_str += f\": {self.message!r}\"\n        if self.field:\n            error_str += f\" on field {self.field!r}\"\n        return error_str\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "    from .model import Model\n    from ...common.exceptions import MsticpyException\n    if not isinstance(data, pd.DataFrame):\n        raise MsticpyException(\"`data` should be a pandas dataframe\")\n    if session_column not in data.columns:\n        raise MsticpyException(f'\"{session_column}\" should be a column in the `data`')\n\n    sessions_df = data.copy()\n    sessions = sessions_df[session_column].values.tolist()\n\n    model = Model(sessions=sessions)\n    model.train()\n    model.compute_rarest_windows(\n        window_len=window_length, use_geo_mean=False, use_start_end_tokens=True\n    )\n\n    sessions_df[\n        f\"rarest_window{window_length}_likelihood\"\n    ] = model.rare_window_likelihoods[window_length]\n    sessions_df[f\"rarest_window{window_length}\"] = model.rare_windows[window_length]\n\n    return sessions_df\n"}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try decode instance to unicode\n        return instance_to_unicode(object)\n"}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        X, y = get_train_data(path)\n    if len(X) < folds:\n        raise ValueError('There are not enough samples ({}). Need at least {}.'.format(len(X), folds))\n    clf = clf or get_model(path)\n    tot = 0\n    print(\"KFold folds={}, running {} times\".format(folds, n))\n    for i in range(n):\n        res = cross_val_score(clf, X, y, cv=folds).mean()\n        tot += res\n        print(\"{}/{}: {}\".format(i + 1, n, res))\n    print(\"-------- total --------\")\n    print(tot / n)\n    return tot / n\n"}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = set()\n\n    for provider_path in providers:\n        provider_module = import_module(provider_path)\n        if getattr(provider_module, \"localized\", False):\n            langs = list_module(provider_module)\n            available_locales.update(langs)\n    return sorted(available_locales)\n"}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    lfpp = math.log(false_positive_probability)\n    return min(36000, int(((-1 / pow(LOG_2, 2) * element_count * lfpp)+7) // 8))\n"}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    manually_sorted_keys = manually_sorted_keys or []\n\n    def get_order(key_and_value):\n        key, value = key_and_value\n        if key in manually_sorted_keys:\n            return 0, manually_sorted_keys.index(key)\n        elif not is_non_empty_dict(value):\n            return 1, key\n        else:\n            return 2, key\n\n    for key, value in sorted(dictionary.items(), key=get_order):\n        if is_non_empty_dict(value):\n            yield key, PATHCHANGE\n            for k, val in iterate_flattened_separately(value, manually_sorted_keys):\n                yield join_paths(key, k), val\n        else:\n            yield key, value\n"}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        if not hadoop_streaming_jar:\n            raise Exception('no Hadoop streaming jar')\n\n        return (self.get_hadoop_bin() + ['jar', hadoop_streaming_jar] +\n                self._hadoop_streaming_jar_args(step_num))\n"}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    dhalf = measure(note1, note2)\n    return dhalf in [0, 7] or include_fourths and dhalf == 5\n"}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        try:\n            return self.stack[-1]\n        except IndexError:\n            return self.default()\n"}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.handle is None:\n            raise IOError('read on closed handle')\n        if self.pos >= self.size:\n            # print('ZipSubFile: read fake at end')\n            return b''   # fake being at the end, even if we are not\n        data = self.handle.read(size)\n        self.pos += len(data)\n        # print('ZipSubFile: read {} bytes, pos now {}'.format(size, self.pos))\n        return data\n"}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    request = Request.blank(path=\"\")\n    request.registry = registry\n    return instance_uri(request, resource_name, **params)\n"}
{"namespace": "ydata_profiling.controller.console.main", "completion": "    from ydata_profiling.utils.dataframe import read_pandas\n    parsed_args = parse_args(args)\n    kwargs = vars(parsed_args)\n\n    input_file = Path(kwargs.pop(\"input_file\"))\n    output_file = kwargs.pop(\"output_file\")\n    if output_file is None:\n        output_file = str(input_file.with_suffix(\".html\"))\n\n    silent = kwargs.pop(\"silent\")\n\n    # read the DataFrame\n    df = read_pandas(input_file)\n\n    # Generate the profiling report\n    p = ProfileReport(\n        df,\n        **kwargs,\n    )\n    p.to_file(Path(output_file), silent=silent)\n"}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    from .ids import _to_job_id\n    m = _PRE_YARN_TASK_LOG_PATH_RE.match(path)\n    if m:\n        if job_id and job_id != _to_job_id(m.group('attempt_id')):\n            return None  # matches, but wrong job_id\n\n        return dict(\n            attempt_id=m.group('attempt_id'),\n            log_type=m.group('log_type'))\n\n    m = _YARN_TASK_LOG_PATH_RE.match(path)\n    if m:\n        if application_id and application_id != m.group('application_id'):\n            return None  # matches, but wrong application_id\n\n        return dict(\n            application_id=m.group('application_id'),\n            container_id=m.group('container_id'),\n            log_type=m.group('log_type'))\n\n    return None\n"}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        from .listing import ModNoteListing\n        if isinstance(listing, list):\n            return listing[1]  # for submission duplicates\n        elif isinstance(listing, dict):\n            classes = [FlairListing, ModNoteListing]\n\n            for listing_type in classes:\n                if listing_type.CHILD_ATTRIBUTE in listing:\n                    return listing_type(self._reddit, listing)\n            else:\n                raise ValueError(\n                    \"The generator returned a dictionary PRAW didn't recognize.\"\n                    \" File a bug report at PRAW.\"\n                )\n        return listing\n"}
{"namespace": "boltons.setutils.complement", "completion": "    if type(wrapped) is _ComplementSet:\n        return wrapped.complemented()\n    if type(wrapped) is frozenset:\n        return _ComplementSet(excluded=wrapped)\n    return _ComplementSet(excluded=set(wrapped))\n"}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        try:\n            value = self.env['REMOTE_ADDR']\n        except KeyError:\n            value = '127.0.0.1'\n\n        return value\n"}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        logger.debug(\"< DONE\")\n        self._imap.send(b\"DONE\\r\\n\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n"}
{"namespace": "rest_framework.fields.Field.root", "completion": "        root = self\n        while root.parent is not None:\n            root = root.parent\n        return root\n"}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    host_db_path = os.path.join(os.environ[\"HOME\"], \".dropbox/host.db\")\n    try:\n        with open(host_db_path, \"r\") as f_hostdb:\n            data = f_hostdb.read().split()\n    except IOError:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Dropbox install\"))\n    dropbox_home = base64.b64decode(data[1]).decode()\n\n    return dropbox_home\n"}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "        typed_gadgets = []\n\n        for g_type, g_classifier in self._classifiers.items():\n            try:\n                typed_gadgets += self._classify(gadget, g_classifier, g_type, self._emu_iters)\n            except:\n                import traceback\n\n                print(\"[-] Error classifying gadgets :\")\n                print(gadget)\n                print(\"\")\n                print(traceback.format_exc())\n\n        # Sort and return.\n        return sorted(typed_gadgets, key=lambda g: str(g))\n"}
{"namespace": "tools.cgrep.group_diff", "completion": "  nested_rvals = []\n  for ip in options.gmp:\n    nested_rvals.append(get_ip_parents(ip, db))\n  # get just the list of groups, stripping out the networks.\n  group1 = [x[0] for x in nested_rvals[0]]\n  group2 = [x[0] for x in nested_rvals[1]]\n  common = sorted(list(set(group1) & set(group2)))\n  diff1 = sorted(list(set(group1) - set(group2)))\n  diff2 = sorted(list(set(group2) - set(group1)))\n  return common, diff1, diff2\n"}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        else:\n            return item.dumps()\n    elif not isinstance(item, str):\n        item = str(item)\n\n    if escape:\n        item = escape_latex(item)\n\n    return item\n"}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers\n"}
{"namespace": "parsel.utils.extract_regex", "completion": "    if isinstance(regex, str):\n        regex = re.compile(regex, re.UNICODE)\n\n    if \"extract\" in regex.groupindex:\n        # named group\n        try:\n            extracted = cast(Match[str], regex.search(text)).group(\"extract\")\n        except AttributeError:\n            strings = []\n        else:\n            strings = [extracted] if extracted is not None else []\n    else:\n        # full regex or numbered groups\n        strings = regex.findall(text)\n\n    strings = flatten(strings)\n    if not replace_entities:\n        return strings\n    return [w3lib_replace_entities(s, keep=[\"lt\", \"amp\"]) for s in strings]\n"}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        self.name = notes.int_to_note(integer % 12)\n        self.octave = integer // 12\n        return self\n"}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if self._num_digits is not None and len(value) != self._num_digits:\n            return None\n\n        # NOTE(kgriffs): int() will accept numbers with preceding or\n        # trailing whitespace, so we need to do our own check. Using\n        # strip() is faster than either a regex or a series of or'd\n        # membership checks via \"in\", esp. as the length of contiguous\n        # numbers in the value grows.\n        if value.strip() != value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and value < self._min:\n            return None\n\n        if self._max is not None and value > self._max:\n            return None\n\n        return value\n"}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    call_with = _gather_arguments(fn, kwargs)\n    return fn(*call_with)\n"}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        msg_parts = ['MSG: ' + msg]\n\n        if detail is not None:\n            msg_parts.append('DETAIL: ' + detail)\n        if hint is not None:\n            msg_parts.append('HINT: ' + hint)\n\n        # Initialize a fresh dictionary if structured is not passed,\n        # because keyword arguments are not re-evaluated when calling\n        # the function and it's okay for callees to mutate their\n        # passed dictionary.\n        if structured is None:\n            structured = {}\n\n        msg_parts.append('STRUCTURED: ' +\n                         WalELogger._fmt_structured(structured))\n\n        return '\\n'.join(msg_parts)\n"}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        try:\n            if isinstance(jwk, str):\n                obj: JWKDict = json.loads(jwk)\n            elif isinstance(jwk, dict):\n                obj = jwk\n            else:\n                raise ValueError\n        except ValueError:\n            raise InvalidKeyError(\"Key is not valid JSON\")\n\n        if obj.get(\"kty\") != \"oct\":\n            raise InvalidKeyError(\"Not an HMAC key\")\n\n        return base64url_decode(obj[\"k\"])\n"}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        if self.properties:\n            self.apply_properties(target, self.properties)\n"}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n"}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    from rest_framework.utils.urls import replace_query_param\n    iri = request.get_full_path()\n    uri = iri_to_uri(iri)\n    return escape(replace_query_param(uri, key, val))\n"}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    match['base_guesses'] = match['rank']\n    match['uppercase_variations'] = uppercase_variations(match)\n    match['l33t_variations'] = l33t_variations(match)\n    reversed_variations = match.get('reversed', False) and 2 or 1\n\n    return match['base_guesses'] * match['uppercase_variations'] * \\\n        match['l33t_variations'] * reversed_variations\n"}
{"namespace": "mrjob.job.MRJob.execute", "completion": "        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n\n        else:\n            self.run_job()\n"}
{"namespace": "falcon.inspect.inspect_app", "completion": "    routes = inspect_routes(app)\n    static = inspect_static_routes(app)\n    sinks = inspect_sinks(app)\n    error_handlers = inspect_error_handlers(app)\n    middleware = inspect_middleware(app)\n    return AppInfo(routes, middleware, static, sinks, error_handlers, app._ASGI)\n"}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    table = _lcs(x, y)\n\n    def _recon(i, j):\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n        elif table[i - 1, j] > table[i, j - 1]:\n            return _recon(i - 1, j)\n        else:\n            return _recon(i, j - 1)\n\n    i, j = _get_index_of_lcs(x, y)\n    recon_tuple = tuple(map(lambda r: r[0], _recon(i, j)))\n    return recon_tuple\n"}
{"namespace": "datasette.utils.escape_css_string", "completion": "    return _css_re.sub(\n        lambda m: \"\\\\\" + (f\"{ord(m.group()):X}\".zfill(6)),\n        s.replace(\"\\r\\n\", \"\\n\"),\n    )\n"}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type,\n        }\n"}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        # need a dummy column name here since SQLAlchemy\n        # 0.7.6 and further raises on Index with no columns\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self._reverse.columns if self._reverse else [\"x\"],\n            schema=self.schema,\n            **self.kw,\n        )\n"}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self:\n            raise PacketDecodeError('Unexpected data at end of packet')\n"}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        prop = cls.make_property(callable, name=name, reify=reify)\n        cls.apply_properties(target, [prop])\n"}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), log.COLORS)\n        return value.lower()\n"}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "        try:\n            with item_fs.open(MODEL_YAML_FILENAME, \"r\") as model_yaml:\n                info = ModelInfo.from_yaml_file(model_yaml)\n        except fs.errors.ResourceNotFound:\n            raise BentoMLException(\n                f\"Failed to load bento model because it does not contain a '{MODEL_YAML_FILENAME}'\"\n            )\n\n        res = Model(tag=info.tag, model_fs=item_fs, info=info, _internal=True)\n        try:\n            res.validate()\n        except BentoMLException as e:\n            raise BentoMLException(f\"Failed to load {res!s}: {e}\") from None\n\n        return res\n"}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    trd = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, trd, 3)\n"}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        if request_id in self.response_map:\n            if not self.response_map[request_id].empty():\n                return self.response_map[request_id].get()\n\n        if owner_uri in self.response_map:\n            if not self.response_map[owner_uri].empty():\n                return self.response_map[owner_uri].get()\n\n        if not self.response_map[0].empty():\n            return self.response_map[0].get()\n\n        if not self.exception_queue.empty():\n            raise self.exception_queue.get()\n\n        return None\n"}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        t = sa_schema.Table(\n            source,\n            self.metadata(),\n            *[sa_schema.Column(n, NULLTYPE) for n in local_cols],\n            schema=schema,\n        )\n        kw[\"name\"] = name\n        uq = sa_schema.UniqueConstraint(*[t.c[n] for n in local_cols], **kw)\n        # TODO: need event tests to ensure the event\n        # is fired off here\n        t.append_constraint(uq)\n        return uq\n"}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaTypeValidationError.check(grouping, full_schema, path, (tuple, list))\n        SchemaLengthValidationError.check(grouping, full_schema, path, len(schema))\n\n        for i, (g, s) in enumerate(zip(grouping, schema)):\n            validate_grouping(g, s, full_schema=full_schema, path=path + (i,))\n    elif isinstance(schema, dict):\n        SchemaTypeValidationError.check(grouping, full_schema, path, dict)\n        SchemaKeysValidationError.check(grouping, full_schema, path, set(schema))\n\n        for k in schema:\n            validate_grouping(\n                grouping[k], schema[k], full_schema=full_schema, path=path + (k,)\n            )\n    else:\n        pass\n"}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        self.socket.send(data)\n        # wait for the ACK/NACK\n        return self.socket.recv(RECV_BUFFER)\n"}
{"namespace": "dash._grouping.map_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return [map_grouping(fn, g) for g in grouping]\n\n    if isinstance(grouping, dict):\n        return AttributeDict({k: map_grouping(fn, g) for k, g in grouping.items()})\n\n    return fn(grouping)\n"}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "        _, _, summary = self.handle(str(dtype), config, series, {\"type\": str(dtype)})\n        return summary\n"}
{"namespace": "alembic.command.merge", "completion": "    script = ScriptDirectory.from_config(config)\n    template_args = {\n        \"config\": config  # Let templates use config for\n        # e.g. multiple databases\n    }\n\n    environment = util.asbool(config.get_main_option(\"revision_environment\"))\n\n    if environment:\n\n        def nothing(rev, context):\n            return []\n\n        with EnvironmentContext(\n            config,\n            script,\n            fn=nothing,\n            as_sql=False,\n            template_args=template_args,\n        ):\n            script.run_env()\n\n    return script.generate_revision(\n        rev_id or util.rev_id(),\n        message,\n        refresh=True,\n        head=revisions,\n        branch_labels=branch_label,\n        **template_args,  # type:ignore[arg-type]\n    )\n"}
{"namespace": "boto.logs.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.logs.layer1 import CloudWatchLogsConnection\n    return connect('logs', region_name,\n                   connection_cls=CloudWatchLogsConnection, **kw_params)\n"}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        show_warning()\n\n        if not inventory_filename:\n            raise InventoryError(\"No Ansible inventory filename provided!\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(\n                (\"Could not find Ansible inventory file: {0}\").format(inventory_filename),\n            )\n\n        return parse_inventory(inventory_filename)\n"}
{"namespace": "pyinfra.operations.files.put", "completion": "    if hasattr(src, \"read\"):\n        local_file = src\n        local_sum = get_file_sha1(src)\n\n    # Assume string filename\n    else:\n        # Add deploy directory?\n        if add_deploy_dir and state.cwd:\n            src = os.path.join(state.cwd, src)\n\n        local_file = src\n\n        if os.path.isfile(local_file):\n            local_sum = get_file_sha1(local_file)\n        elif assume_exists:\n            local_sum = None\n        else:\n            raise IOError(\"No such file: {0}\".format(local_file))\n\n    if mode is True:\n        if os.path.isfile(local_file):\n            mode = get_path_permissions_mode(local_file)\n        else:\n            logger.warning(\n                (\"No local file exists to get permissions from with `mode=True` ({0})\").format(\n                    get_call_location(),\n                ),\n            )\n    else:\n        mode = ensure_mode_int(mode)\n\n    remote_file = host.get_fact(File, path=dest)\n\n    if not remote_file and host.get_fact(Directory, path=dest):\n        dest = unix_path_join(dest, os.path.basename(src))\n        remote_file = host.get_fact(File, path=dest)\n\n    if create_remote_dir:\n        yield from _create_remote_dir(state, host, dest, user, group)\n\n    # No remote file, always upload and user/group/mode if supplied\n    if not remote_file or force:\n        yield FileUploadCommand(\n            local_file,\n            dest,\n            remote_temp_filename=state.get_temp_filename(dest),\n        )\n\n        if user or group:\n            yield file_utils.chown(dest, user, group)\n\n        if mode:\n            yield file_utils.chmod(dest, mode)\n\n    # File exists, check sum and check user/group/mode if supplied\n    else:\n        remote_sum = host.get_fact(Sha1File, path=dest)\n\n        # Check sha1sum, upload if needed\n        if local_sum != remote_sum:\n            yield FileUploadCommand(\n                local_file,\n                dest,\n                remote_temp_filename=state.get_temp_filename(dest),\n            )\n\n            if user or group:\n                yield file_utils.chown(dest, user, group)\n\n            if mode:\n                yield file_utils.chmod(dest, mode)\n\n        else:\n            changed = False\n\n            # Check mode\n            if mode and remote_file[\"mode\"] != mode:\n                yield file_utils.chmod(dest, mode)\n                changed = True\n\n            # Check user/group\n            if (user and remote_file[\"user\"] != user) or (group and remote_file[\"group\"] != group):\n                yield file_utils.chown(dest, user, group)\n                changed = True\n\n            if not changed:\n                host.noop(\"file {0} is already uploaded\".format(dest))\n"}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        from mrjob.step import StepFailedException\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\n\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=log_stream)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except StepFailedException as e:\n                # no need for a runner stacktrace if step failed; runners will\n                # log more useful information anyway\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                for chunk in runner.cat_output():\n                    self.stdout.write(chunk)\n                self.stdout.flush()\n"}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n        cls.security_roles[method_name] = definition.get(\"scopes\", {}).keys()\n"}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        version = self.get_installed_version()\n        if not version:\n            self.create_schema(dry_run)\n            return\n\n        logger.info(f\"Detected PostgreSQL {self.name} schema version {version}.\")\n        if version == self.schema_version:\n            logger.info(f\"PostgreSQL {self.name} schema is up-to-date.\")\n            return\n\n        self.migrate_schema(version, dry_run)\n"}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info[0] == 3:\n        return anything\n\n    if isinstance(anything, str):\n        return unicode(anything, \"utf-8\")\n    if isinstance(anything, dict):\n        cast_dict = dict()\n        for key in anything:\n            cast_key = cast_to_unicode(key)\n            cast_value = cast_to_unicode(anything[key])\n            cast_dict[cast_key] = cast_value\n        return cast_dict\n    if isinstance(anything, list):\n        cast_list = []\n        for e in anything:\n            cast_list.append(cast_to_unicode(e))\n        return cast_list\n    return anything\n"}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n"}
{"namespace": "fs.path.normpath", "completion": "    from .errors import IllegalBackReference\n    if path in \"/\":\n        return path\n\n    # An early out if there is no need to normalize this path\n    if not _requires_normalization(path):\n        return path.rstrip(\"/\")\n\n    prefix = \"/\" if path.startswith(\"/\") else \"\"\n    components = []  # type: List[Text]\n    try:\n        for component in path.split(\"/\"):\n            if component in \"..\":  # True for '..', '.', and ''\n                if component == \"..\":\n                    components.pop()\n            else:\n                components.append(component)\n    except IndexError:\n        # FIXME (@althonos): should be raised from the IndexError\n        raise IllegalBackReference(path)\n    return prefix + \"/\".join(components)\n"}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        identity = self._get_identity(request)\n        return identifier.forget(request.environ, identity)\n"}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        # try encode instance to bytes\n        return instance_to_bytes(object)\n"}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        post_filter = {\"ActivitySid\": {\"required\": True}}\n        self._make_policy(self.resource_url, \"POST\", True, post_filter=post_filter)\n"}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    components = urlparse(uri)\n    if (components.scheme not in ('s3', 's3n', 's3a') or\n        '/' not in components.path):  # noqa\n\n        raise ValueError('Invalid S3 URI: %s' % uri)\n\n    return components.netloc, components.path[1:]\n"}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()\n"}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return os.path.abspath(\n            self.pkg_resources.resource_filename(self.pkg_name, self.path)\n        )\n"}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n <= 0:\n            return []\n        ret = sorted(self.iteritems(), key=lambda x: x[1], reverse=True)\n        if n is None or n >= len(ret):\n            return ret\n        return ret[:n]\n"}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.route53.domains.layer1 import Route53DomainsConnection\n    return connect('route53domains', region_name,\n                   connection_cls=Route53DomainsConnection, **kw_params)\n"}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        raw_value = decode(value).strip()\n        validators.validate_required(raw_value, self._required)\n        if not raw_value:\n            return None\n\n        if self._separator in raw_value:\n            values = raw_value.split(self._separator, 1)\n        elif self._optional_pair:\n            values = (raw_value, raw_value)\n        else:\n            raise ValueError(\n                f\"Config value must include {self._separator!r} separator: {raw_value}\"\n            )\n\n        return (\n            self._subtypes[0].deserialize(encode(values[0])),\n            self._subtypes[1].deserialize(encode(values[1])),\n        )\n"}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return '<%s instance at %s with msg %r>' % (\n            self.__class__.__name__,\n            id(self),\n            self.msg,\n        )\n"}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    explain = \"explain {}\".format(sql.strip().rstrip(\";\"))\n    possible_params = _re_named_parameter.findall(sql)\n    try:\n        results = await db.execute(explain, {p: None for p in possible_params})\n        return [row[\"p4\"].lstrip(\":\") for row in results if row[\"opcode\"] == \"Variable\"]\n    except sqlite3.DatabaseError:\n        return possible_params\n"}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    boto_service_definition_files()\n    service_definitions_for_service = fnmatch.filter(boto_service_definition_files(),\n                                                     \"**/\" + servicename + \"/*/service-*.json\")\n\n    service_definitions_for_service.sort()\n\n    return service_definitions_for_service[-1]\n"}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        if isinstance(batch, pd.Series):\n            batch = pd.DataFrame([batch])\n\n        meta: dict[str, bool | int | float | str | list[int]] = {\"format\": \"pickle5\"}\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            data = concat_buffer_bs\n            meta[\"pickle_bytes_str\"] = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta[\"with_buffer\"] = False\n            data = bs\n\n        return cls.create_payload(\n            data,\n            batch.shape[0],\n            meta=meta,\n        )\n"}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        view = self.match(context, request)\n        view = getattr(view, '__call_permissive__', view)\n        return view(context, request)\n"}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        members = self._store.get(permission_key, set())\n        return members\n"}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    from mrjob.util import file_ext\n    name_with_ext = posixpath.basename(input_uri)\n    name = name_with_ext[:-len(file_ext(name_with_ext))]\n\n    parts = name.split('-')\n\n    doc_id = parts[0]\n    cats = {}\n\n    for part in parts[1:]:\n        if part.startswith('not_'):\n            cats[part[4:]] = False\n        else:\n            cats[part] = True\n\n    return dict(id=doc_id, cats=cats)\n"}
{"namespace": "music_dl.utils.colorize", "completion": "    string = str(string)\n    if color not in colors:\n        return string\n    if platform.system() == \"Windows\":\n        return string\n    return colors[color] + string + \"\\033[0m\"\n"}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    return _parse_step_syslog_from_log4j_records(\n        _parse_hadoop_log4j_records(lines))\n"}
{"namespace": "mrjob.util.safeeval", "completion": "    from mrjob.py2 import PY2\n    safe_globals = {\n        'False': False,\n        'None': None,\n        'True': True,\n        '__builtin__': None,\n        '__builtins__': None,\n        'set': set\n    }\n\n    # xrange is range in Python 3\n    if PY2:\n        safe_globals['xrange'] = xrange\n    else:\n        safe_globals['range'] = range\n\n    # PyPy needs special magic\n    def open(*args, **kwargs):\n        raise NameError(\"name 'open' is not defined\")\n    safe_globals['open'] = open\n\n    # add the user-specified global variables\n    if globals:\n        safe_globals.update(globals)\n\n    return eval(expr, safe_globals, locals)\n"}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        d = {}\n        for batch in self:\n            b = batch.to_dict()\n            if b['Keys']:\n                d[batch.table.name] = b\n        return d\n"}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.machinelearning.layer1 import MachineLearningConnection\n    return connect('machinelearning', region_name,\n                   connection_cls=MachineLearningConnection, **kw_params)\n"}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        n = Note(startnote.name, startnote.octave, startnote.dynamics)\n        n.transpose(shorthand, up)\n        self.add_notes([startnote, n])\n        return self\n"}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            if not self.exists(self.manifest_path):\n                return {}\n            mtime = self.getmtime(self.manifest_path)\n            if self._mtime is None or mtime > self._mtime:\n                self._manifest = self.get_manifest()\n                self._mtime = mtime\n        return self._manifest\n"}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None\n"}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        if self._startup_invoked:\n            return\n        for hook in pm.hook.prepare_jinja2_environment(\n            env=self.jinja_env, datasette=self\n        ):\n            await await_me_maybe(hook)\n        for hook in pm.hook.startup(datasette=self):\n            await await_me_maybe(hook)\n        self._startup_invoked = True\n"}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        with self._lock:\n            if E is self:\n                return\n            setitem = self.__setitem__\n            if callable(getattr(E, 'keys', None)):\n                for k in E.keys():\n                    setitem(k, E[k])\n            else:\n                for k, v in E:\n                    setitem(k, v)\n            for k in F:\n                setitem(k, F[k])\n            return\n"}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )\n"}
{"namespace": "fs.path.relativefrom", "completion": "    base_parts = list(iteratepath(base))\n    path_parts = list(iteratepath(path))\n\n    common = 0\n    for component_a, component_b in zip(base_parts, path_parts):\n        if component_a != component_b:\n            break\n        common += 1\n\n    return \"/\".join([\"..\"] * (len(base_parts) - common) + path_parts[common:])\n"}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n"}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    if search_environment_path:\n        path = resolve_binary(path)\n    if not os.path.exists(path):\n        raise MissingFileError('The \"%s\" file was not found.' % path)\n    if os.path.isdir(path):\n        raise UnexpectedDirectoryError('\"%s\" is a directory, not a file.' % path)\n    return os.path.normpath(os.path.abspath(path))\n"}
{"namespace": "fs.path.iteratepath", "completion": "    path = relpath(normpath(path))\n    if not path:\n        return []\n    return path.split(\"/\")\n"}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "    have_current_user = False\n    have_user_edit = False\n    have_persist_dir = False\n    have_document_container = False\n    ppt_file = None\n    try:\n        ppt_file = PptFile(filename)\n        for stream in ppt_file.iter_streams():\n            if stream.name == 'Current User':\n                for record in stream.iter_records():\n                    if isinstance(record, PptRecordCurrentUser):\n                        have_current_user = True\n                        if have_current_user and have_user_edit and \\\n                                have_persist_dir and have_document_container:\n                            return True\n            elif stream.name == 'PowerPoint Document':\n                for record in stream.iter_records():\n                    if record.type == 0x0ff5:     # UserEditAtom\n                        have_user_edit = True\n                    elif record.type == 0x1772:   # PersistDirectoryAtom\n                        have_persist_dir = True\n                    elif record.type == 0x03e8:   # DocumentContainer\n                        have_document_container = True\n                    else:\n                        continue\n                    if have_current_user and have_user_edit and \\\n                            have_persist_dir and have_document_container:\n                        return True\n            else:   # ignore other streams/storages since they are optional\n                continue\n    except Exception as exc:\n        logging.debug('Ignoring exception in is_ppt, assume is not ppt',\n                      exc_info=True)\n    finally:\n        if ppt_file is not None:\n            ppt_file.close()\n    return False\n"}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, 'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, dict(filters))\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, [('item', VpcPeeringConnection)])\n"}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(\n            self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag\n        )\n"}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.main_type == '*':\n            return 0\n        elif self.sub_type == '*':\n            return 1\n        elif not self.params or list(self.params) == ['q']:\n            return 2\n        return 3\n"}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        from .plot.plot import plot as plotter\n\n        if data is None:\n            d = copy.copy(self.data)\n            transform = copy.copy(self.xform_data)\n            if any([k in kwargs for k in ['reduce', 'align', 'normalize',\n                                          'semantic', 'vectorizer', 'corpus']]):\n                d = copy.copy(self.data)\n                transform = None\n        else:\n            d = data\n            transform = None\n\n        # get kwargs and update with new kwargs\n        new_kwargs = copy.copy(self.kwargs)\n        update_kwargs = dict(transform=transform, reduce=self.reduce,\n                       align=self.align, normalize=self.normalize,\n                       semantic=self.semantic, vectorizer=self.vectorizer,\n                       corpus=self.corpus)\n        new_kwargs.update(update_kwargs)\n        for key in kwargs:\n            new_kwargs.update({key : kwargs[key]})\n        return plotter(d, **new_kwargs)\n"}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        col_diff = []\n        schema, tname, cname = self.schema, self.table_name, self.column_name\n\n        if self.modify_type is not None:\n            col_diff.append(\n                (\n                    \"modify_type\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_type,\n                    self.modify_type,\n                )\n            )\n\n        if self.modify_nullable is not None:\n            col_diff.append(\n                (\n                    \"modify_nullable\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_nullable,\n                    self.modify_nullable,\n                )\n            )\n\n        if self.modify_server_default is not False:\n            col_diff.append(\n                (\n                    \"modify_default\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_server_default,\n                    self.modify_server_default,\n                )\n            )\n\n        if self.modify_comment is not False:\n            col_diff.append(\n                (\n                    \"modify_comment\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                    },\n                    self.existing_comment,\n                    self.modify_comment,\n                )\n            )\n\n        return col_diff\n"}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n"}
{"namespace": "faker.decode.unidecode", "completion": "    chars = \"\"\n    for ch in txt:\n        codepoint = ord(ch)\n\n        try:\n            chars += codes[codepoint]\n        except IndexError:\n            pass\n    return chars\n"}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    nacaddr.IP(arg)\n  except:\n    raise argparse.ArgumentTypeError('%s is an invalid ip address' % arg)\n  return arg\n"}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        ret = \"%s/%s\" % (self.main_type, self.sub_type)\n        for key, val in self.params.items():\n            ret += \"; %s=%s\" % (key, val)\n        return ret\n"}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind not in _RESOURCE_REGISTRY:\n        raise BentoMLConfigException(f\"Unknown resource kind '{resource_kind}'.\")\n\n    resource: t.Type[Resource[t.Any]] = _RESOURCE_REGISTRY[resource_kind]\n\n    if resource_kind in resources:\n        if resources[resource_kind] == \"system\":\n            return resource.from_system()\n        else:\n            res = resource.from_spec(resources[resource_kind])\n            if validate:\n                resource.validate(res)\n            return res\n    else:\n        return None\n"}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    auth = headers.get('Authorization')\n    if not auth or ' ' not in auth:\n        return None, None\n\n    auth_type, auth_token = auth.split(None, 1)\n    if auth_type.lower() != 'basic':\n        return None, None\n\n    try:\n        query = to_unicode(base64.b64decode(auth_token))\n    except (binascii.Error, TypeError):\n        return None, None\n    if ':' in query:\n        username, password = query.split(':', 1)\n        return username, password\n    return query, None\n"}
{"namespace": "twtxt.config.Config.options", "completion": "        try:\n            return dict(self.cfg.items(\"twtxt\"))\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n            return {}\n"}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 5)\n        a = interval_diff(roman, n, 9) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res\n"}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n    for providers_mod in modules:\n        if providers_mod.__package__:\n            providers = [\n                \".\".join([providers_mod.__package__, mod]) for mod in list_module(providers_mod) if mod != \"__pycache__\"\n            ]\n            available_providers.update(providers)\n    return sorted(available_providers)\n"}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n        self._data.extend(data)\n        self._set_width()\n        self._fill()\n"}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)\n"}
{"namespace": "boto.glacier.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.glacier.layer2 import Layer2\n    return connect('glacier', region_name, connection_cls=Layer2, **kw_params)\n"}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if arg:\n            output = str(text) % arg\n        else:\n            output = str(text)\n\n        util.write_outstream(self.stdout, output, \"\\n\", **self.messaging_opts)\n"}
{"namespace": "falcon.request.Request.get_header", "completion": "        wsgi_name = name.upper().replace('-', '_')\n\n        # Use try..except to optimize for the header existing in most cases\n        try:\n            # Don't take the time to cache beforehand, using HTTP naming.\n            # This will be faster, assuming that most headers are looked\n            # up only once, and not all headers will be requested.\n            return self.env['HTTP_' + wsgi_name]\n\n        except KeyError:\n            # NOTE(kgriffs): There are a couple headers that do not\n            # use the HTTP prefix in the env, so try those. We expect\n            # people to usually just use the relevant helper properties\n            # to access these instead of .get_header.\n            if wsgi_name in WSGI_CONTENT_HEADERS:\n                try:\n                    return self.env[wsgi_name]\n                except KeyError:\n                    pass\n\n            if not required:\n                return default\n\n            raise errors.HTTPMissingHeader(name)\n"}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        ret = self.buffer.reader.read(n, n)\n        self._tell = self.tell() + len(ret)\n        return ret\n"}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        for key, value in tags.items():\n            if key in self.tags:\n                if value is None or value == self.tags[key]:\n                    del self.tags[key]\n"}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        instrs_reil = []\n\n        try:\n            for instr in instrs:\n                instr_lower = instr.lower()\n\n                # If the instruction to parsed is not in the cache,\n                # parse it and add it to the cache.\n                if instr_lower not in self._cache:\n                    self._cache[instr_lower] = instruction.parseString(\n                        instr_lower)[0]\n\n                # Retrieve parsed instruction from the cache and clone\n                # it.\n                instrs_reil += [copy.deepcopy(self._cache[instr_lower])]\n        except:\n            error_msg = \"Failed to parse instruction: %s\"\n\n            logger.error(error_msg, instr, exc_info=True)\n\n        return instrs_reil\n"}
{"namespace": "kinto.core.utils.native_value", "completion": "    if isinstance(value, str):\n        try:\n            value = json.loads(value)\n        except ValueError:\n            return value\n    return value\n"}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    return _write_config_file(\n        \"\"\"\n[alembic]\nscript_location = %s\nsqlalchemy.url = %s://\n%s\n\n[loggers]\nkeys = root\n\n[handlers]\nkeys = console\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname =\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatters]\nkeys = generic\n\n[formatter_generic]\nformat = %%(levelname)-5.5s [%%(name)s] %%(message)s\ndatefmt = %%H:%%M:%%S\n\n\"\"\"\n        % (dir_, dialect, directives)\n    )\n"}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        for i, filename in enumerate(self.ls(path_glob)):\n            if i > 0:\n                yield b''  # mark end of previous file\n\n            for line in self._cat_file(filename):\n                yield line\n"}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.dynamodb2.layer1 import DynamoDBConnection\n    return connect('dynamodb', region_name, connection_cls=DynamoDBConnection,\n                   **kw_params)\n"}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.setdefault(category_name, {})\n        intr = category.get(discriminator, default)\n        return intr\n"}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    stats = {}\n    hist_config = config.plot.histogram\n    bins_arg = \"auto\" if hist_config.bins == 0 else min(hist_config.bins, n_unique)\n    bins = np.histogram_bin_edges(finite_values, bins=bins_arg)\n    if len(bins) > hist_config.max_bins:\n        bins = np.histogram_bin_edges(finite_values, bins=hist_config.max_bins)\n        weights = weights if weights and len(weights) == hist_config.max_bins else None\n\n    stats[name] = np.histogram(\n        finite_values, bins=bins, weights=weights, density=config.plot.histogram.density\n    )\n\n    return stats\n"}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n"}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        params = []\n        if self.epsilon != float(\"inf\"):\n            params.append(f\"epsilon={self.epsilon}\")\n\n        if self.delta != 1:\n            params.append(f\"delta={self.delta}\")\n\n        if self.slack > 0:\n            params.append(f\"slack={self.slack}\")\n\n        if self.spent_budget:\n            if len(self.spent_budget) > n_budget_max:\n                params.append(\"spent_budget=\" + str(self.spent_budget[:n_budget_max] + [\"...\"]).replace(\"'\", \"\"))\n            else:\n                params.append(\"spent_budget=\" + str(self.spent_budget))\n\n        return \"BudgetAccountant(\" + \", \".join(params) + \")\"\n"}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n"}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        p1 = self.maximum_path(h1, path_cache)\n        p2 = self.maximum_path(h2, path_cache)\n        if p1[-1] != p2[-1]:\n            return [], []\n\n        shorter_len = min(len(p1), len(p2))\n        i1 = len(p1) - shorter_len\n        i2 = len(p2) - shorter_len\n        while 1:\n            if p1[i1] == p2[i2]:\n                return p1[:i1+1], p2[:i2+1]\n            i1 += 1\n            i2 += 1\n"}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        tag = self._imap._command(\"NOOP\")\n        return self._consume_until_tagged_response(tag, \"NOOP\")\n"}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if self.ssl or self._starttls_done:\n            raise exceptions.IMAPClientAbortError(\"TLS session already established\")\n\n        typ, data = self._imap._simple_command(\"STARTTLS\")\n        self._checkok(\"starttls\", typ, data)\n\n        self._starttls_done = True\n\n        self._imap.sock = tls.wrap_socket(self._imap.sock, ssl_context, self.host)\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n        return data[0]\n"}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        if key not in self.data:\n            return\n        self.data[newkey] = fwdset = self.data.pop(key)\n        for val in fwdset:\n            revset = self.inv.data[val]\n            revset.remove(key)\n            revset.add(newkey)\n"}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    _check_iterable(arg, msg, name=cls.__name__)\n    if not all(isinstance(instance, cls) for instance in arg):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n"}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return b''.join(self.sbuf)\n"}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    next_line = next_lines.pop()\n    result = re.match(_screen_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    raw_matches = result.groupdict()\n\n    screen: Screen = {\"devices\": []}\n    for k, v in raw_matches.items():\n        screen[k] = int(v)\n\n    while next_lines:\n        device: Optional[Device] = _parse_device(next_lines)\n        if not device:\n            break\n        else:\n            screen[\"devices\"].append(device)\n\n    return screen\n"}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        words = map(self.normalize_word, document.words)\n        unique_words = frozenset(self.stem_word(w) for w in words if w not in self._stop_words)\n\n        return dict((w, i) for i, w in enumerate(unique_words))\n"}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        system = {\n            'view': view,\n            'renderer_name': self.name,  # b/c\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'req': request,\n            'get_csrf_token': partial(get_csrf_token, request),\n        }\n        return self.render_to_response(response, system, request=request)\n"}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if not os.path.exists(file):\n            raise ValueError(\"Config file not found.\")\n\n        try:\n            config_parser = configparser.ConfigParser()\n            config_parser.read(file)\n\n            configuration = cls(file, config_parser)\n            if not configuration.check_config_sanity():\n                raise ValueError(\"Error in config file.\")\n            else:\n                return configuration\n        except configparser.Error:\n            raise ValueError(\"Config file is invalid.\")\n"}
{"namespace": "discord.utils.snowflake_time", "completion": "    timestamp = ((id >> 22) + DISCORD_EPOCH) / 1000\n    return datetime.datetime.fromtimestamp(timestamp, tz=datetime.timezone.utc)\n"}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                if password_lower[i:j + 1] in ranked_dict:\n                    word = password_lower[i:j + 1]\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': False,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n"}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    channel_path = get_base_path() / \"modules\"\n    if not channel_path.exists():\n        channel_path.mkdir(parents=True)\n    return channel_path\n"}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.step_desc:\n            step_desc = self.step_desc\n        else:\n            if self.step_num is not None:\n                # 1-index step numbers\n                if self.last_step_num is not None:\n                    step_name = 'Steps %d-%d' % (\n                        self.step_num + 1, self.last_step_num + 1)\n                else:\n                    step_name = 'Step %d' % (self.step_num + 1)\n\n                if self.num_steps:\n                    step_desc = '%s of %d' % (step_name, self.num_steps)\n                else:\n                    step_desc = step_name\n            else:\n                step_desc = 'Step'\n\n        if self.reason:\n            return '%s failed: %s' % (step_desc, self.reason)\n        else:\n            return '%s failed' % step_desc\n"}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.abc.Mapping):\n            r = recursive_update(d.get(k, {}), v)\n            d[k] = r\n        else:\n            d[k] = u[k]\n    return d\n"}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        if self.userid_key in request.session:\n            del request.session[self.userid_key]\n        return []\n"}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        if not quotas:\n            return\n\n        quota_root = None\n        set_quota_args = []\n\n        for quota in quotas:\n            if quota_root is None:\n                quota_root = quota.quota_root\n            elif quota_root != quota.quota_root:\n                raise ValueError(\"set_quota only accepts a single quota root\")\n\n            set_quota_args.append(\"{} {}\".format(quota.resource, quota.limit))\n\n        set_quota_args = \" \".join(set_quota_args)\n        args = [to_bytes(_quote(quota_root)), to_bytes(\"({})\".format(set_quota_args))]\n\n        response = self._raw_command_untagged(\n            b\"SETQUOTA\", args, uid=False, response_name=\"QUOTA\"\n        )\n        return _parse_quota(response)\n"}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n"}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)\n"}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if not patterns:\n        return lambda name: True\n    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)\n"}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path\n"}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "    prohibited = (stringprep.in_table_c12, stringprep.in_table_c21_c22,\n                  stringprep.in_table_c3, stringprep.in_table_c4,\n                  stringprep.in_table_c5, stringprep.in_table_c6,\n                  stringprep.in_table_c7, stringprep.in_table_c8,\n                  stringprep.in_table_c9)\n\n    return _stringprep(s, True, _map_saslprep, 'NFKC', prohibited, True)\n"}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        if len(self.lists) == 1:\n            self.lists[0].sort()\n        else:\n            for li in self.lists:\n                li.sort()\n            tmp_sorted = sorted(chain.from_iterable(self.lists))\n            del self.lists[:]\n            self.lists[0] = tmp_sorted\n            self._balance_list(0)\n"}
{"namespace": "boto.opsworks.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.opsworks.layer1 import OpsWorksConnection\n    return connect('opsworks', region_name,\n                   connection_cls=OpsWorksConnection, **kw_params)\n"}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        if not path or path.endswith('/'):\n            override = DirectoryOverride(path, source)\n        else:\n            override = FileOverride(path, source)\n        self.overrides.insert(0, override)\n        return override\n"}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        args = [\n            _normalise_sort_criteria(sort_criteria),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"SORT\", args, unpack=True)\n        return [int(i) for i in ids.split()]\n"}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}  # used for successful tasks in failed jobs\n\n    for record in _parse_pre_yarn_history_records(lines):\n        fields = record['fields']\n\n        # if job is successful, we get counters for the entire job at the end\n        if record['type'] == 'Job' and 'COUNTERS' in fields:\n            result['counters'] = _parse_pre_yarn_counters(fields['COUNTERS'])\n\n        # otherwise, compile counters for each successful task\n        #\n        # Note: this apparently records a higher total than the task tracker\n        # (possibly some tasks are duplicates?). Couldn't figure out the logic\n        # behind this while looking at the history file\n        elif (record['type'] == 'Task' and\n              'COUNTERS' in fields and 'TASKID' in fields):\n            task_id = fields['TASKID']\n            counters = _parse_pre_yarn_counters(fields['COUNTERS'])\n\n            task_to_counters[task_id] = counters\n\n        # only want FAILED (not KILLED) tasks with non-blank errors\n        elif (record['type'] in ('MapAttempt', 'ReduceAttempt') and\n              'TASK_ATTEMPT_ID' in fields and\n              fields.get('TASK_STATUS') == 'FAILED' and\n              fields.get('ERROR')):\n            result.setdefault('errors', [])\n            result['errors'].append(dict(\n                hadoop_error=dict(\n                    message=fields['ERROR'],\n                    start_line=record['start_line'],\n                    num_lines=record['num_lines']),\n                attempt_id=fields['TASK_ATTEMPT_ID']))\n\n    # if job failed, patch together counters from successful tasks\n    if 'counters' not in result and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result\n"}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    nodes = nodes or []\n\n    for prop_name, value in metadata.items():\n        # Support for recursive shapes, the type is directly in the field.\n        t_value = value.get(\"type\", value)\n        p_type = t_value.get(\"name\")\n\n        if base:\n            key = f\"{base}.{prop_name}\"\n        else:\n            key = prop_name\n        if is_node(p_type):\n            nodes.append(key)\n        elif p_type == \"arrayOf\":\n            a_value = t_value.get(\"value\", t_value)\n            nodes = collect_array(a_value, key, nodes)\n        elif is_shape(p_type):\n            nodes = collect_nodes(t_value[\"value\"], key, nodes)\n        elif p_type == \"union\":\n            nodes = collect_union(t_value[\"value\"], key, nodes)\n        elif p_type == \"objectOf\":\n            o_value = t_value.get(\"value\", {})\n            nodes = collect_object(o_value, key, nodes)\n\n    return nodes\n"}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        scan_offset = self.read_offset\n        while (scan_offset + 3 < self.buffer_end_offset and\n               (self.buffer[scan_offset] != self.CR or\n                self.buffer[scan_offset + 1] != self.LF or\n                self.buffer[scan_offset + 2] != self.CR or\n                self.buffer[scan_offset + 3] != self.LF)):\n            scan_offset += 1\n\n        # if we reached the end\n        if scan_offset + 3 >= self.buffer_end_offset:\n            return False\n\n        # Split the headers by new line\n        try:\n            headers_read = self.buffer[self.read_offset:scan_offset].decode(\n                u'ascii')\n            for header in headers_read.split(u'\\n'):\n                colon_index = header.find(u':')\n\n                if colon_index == -1:\n                    logger.debug(\n                        u'JSON RPC Reader encountered missing colons in try_read_headers()')\n                    raise KeyError(\n                        u'Colon missing from Header: {}.'.format(header))\n\n                # Case insensitive.\n                header_key = header[:colon_index].lower()\n                header_value = header[colon_index + 1:]\n\n                self.headers[header_key] = header_value\n\n            # Was content-length header found?\n            if 'content-length' not in self.headers:\n                logger.debug(\n                    u'JSON RPC Reader did not find Content-Length in the headers')\n                raise LookupError(\n                    u'Content-Length was not found in headers received.')\n\n            self.expected_content_length = int(self.headers[u'content-length'])\n\n        except ValueError:\n            # Content-length contained invalid literal for int.\n            self.trim_buffer_and_resize(scan_offset + 4)\n            raise\n\n        # Pushing read pointer past the newline characters.\n        self.read_offset = scan_offset + 4\n        self.read_state = ReadState.Content\n\n        return True\n"}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)\n"}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        boto.log.info(\n            \"Re-sending %s unprocessed items.\" % len(self._unprocessed)\n        )\n\n        while len(self._unprocessed):\n            # Again, do 25 at a time.\n            to_resend = self._unprocessed[:25]\n            # Remove them from the list.\n            self._unprocessed = self._unprocessed[25:]\n            batch_data = {\n                self.table.table_name: to_resend\n            }\n            boto.log.info(\"Sending %s items\" % len(to_resend))\n            resp = self.table.connection.batch_write_item(batch_data)\n            self.handle_unprocessed(resp)\n            boto.log.info(\n                \"%s unprocessed items left\" % len(self._unprocessed)\n            )\n"}
{"namespace": "folium.utilities.get_bounds", "completion": "    bounds = [[None, None], [None, None]]\n    for point in iter_coords(locations):\n        bounds = [\n            [\n                none_min(bounds[0][0], point[0]),\n                none_min(bounds[0][1], point[1]),\n            ],\n            [\n                none_max(bounds[1][0], point[0]),\n                none_max(bounds[1][1], point[1]),\n            ],\n        ]\n    if lonlat:\n        bounds = _locations_mirror(bounds)\n    return bounds\n"}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "    threshold = config.categorical_maximum_correlation_distinct\n\n    # `index` and `columns` must not be a set since Pandas 1.5,\n    # so convert it to a list. The order of the list is arbitrary.\n    categoricals = list(\n        {\n            key\n            for key, value in summary.items()\n            if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n            and 1 < value[\"n_distinct\"] <= threshold\n        }\n    )\n\n    if len(categoricals) <= 1:\n        return None\n\n    matrix = np.zeros((len(categoricals), len(categoricals)))\n    np.fill_diagonal(matrix, 1.0)\n    correlation_matrix = pd.DataFrame(\n        matrix,\n        index=categoricals,\n        columns=categoricals,\n    )\n\n    for name1, name2 in itertools.combinations(categoricals, 2):\n        confusion_matrix = pd.crosstab(df[name1], df[name2])\n        if confusion_matrix.empty:\n            correlation_matrix.loc[name2, name1] = np.nan\n        else:\n            correlation_matrix.loc[name2, name1] = _cramers_corrected_stat(\n                confusion_matrix, correction=True\n            )\n        correlation_matrix.loc[name1, name2] = correlation_matrix.loc[name2, name1]\n    return correlation_matrix\n"}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        props = {\n            p: getattr(self, p)\n            for p in self._prop_names  # pylint: disable=no-member\n            if hasattr(self, p)\n        }\n        # Add the wildcard properties data-* and aria-*\n        props.update(\n            {\n                k: getattr(self, k)\n                for k in self.__dict__\n                if any(\n                    k.startswith(w)\n                    # pylint:disable=no-member\n                    for w in self._valid_wildcard_attributes\n                )\n            }\n        )\n        as_json = {\n            \"props\": props,\n            \"type\": self._type,  # pylint: disable=no-member\n            \"namespace\": self._namespace,  # pylint: disable=no-member\n        }\n\n        return as_json\n"}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    from . import exceptions\n    if not condition:\n        msg = \"Server replied with a response that violates the IMAP protocol\"\n        if message:\n            # FIXME(jlvillal): This looks wrong as it repeats `msg` twice\n            msg += \"{}: {}\".format(\n                msg, message.decode(encoding=\"ascii\", errors=\"ignore\")\n            )\n        raise exceptions.ProtocolError(msg)\n"}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        records = []\n        for logfile in self._valid_log_files():\n            if logfile.contains_events_for_timeframe(from_date, to_date):\n                records.extend(logfile.records())\n\n        return records\n"}
{"namespace": "alembic.command.history", "completion": "    base: Optional[str]\n    head: Optional[str]\n    script = ScriptDirectory.from_config(config)\n    if rev_range is not None:\n        if \":\" not in rev_range:\n            raise util.CommandError(\n                \"History range requires [start]:[end], \" \"[start]:, or :[end]\"\n            )\n        base, head = rev_range.strip().split(\":\")\n    else:\n        base = head = None\n\n    environment = (\n        util.asbool(config.get_main_option(\"revision_environment\"))\n        or indicate_current\n    )\n\n    def _display_history(config, script, base, head, currents=()):\n        for sc in script.walk_revisions(\n            base=base or \"base\", head=head or \"heads\"\n        ):\n            if indicate_current:\n                sc._db_current_indicator = sc.revision in currents\n\n            config.print_stdout(\n                sc.cmd_format(\n                    verbose=verbose,\n                    include_branches=True,\n                    include_doc=True,\n                    include_parents=True,\n                )\n            )\n\n    def _display_history_w_current(config, script, base, head):\n        def _display_current_history(rev, context):\n            if head == \"current\":\n                _display_history(config, script, base, rev, rev)\n            elif base == \"current\":\n                _display_history(config, script, rev, head, rev)\n            else:\n                _display_history(config, script, base, head, rev)\n            return []\n\n        with EnvironmentContext(config, script, fn=_display_current_history):\n            script.run_env()\n\n    if base == \"current\" or head == \"current\" or environment:\n        _display_history_w_current(config, script, base, head)\n    else:\n        _display_history(config, script, base, head)\n"}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        path = _from_file_uri(path)\n        shutil.copyfile(src, path)\n"}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        with self._recv_lock:\n            if timeout is _UNSET:\n                timeout = self.timeout\n            if flags:\n                raise ValueError(\"non-zero flags not supported: %r\" % flags)\n            if len(self.rbuf) >= size:\n                data, self.rbuf = self.rbuf[:size], self.rbuf[size:]\n                return data\n            if self.rbuf:\n                ret, self.rbuf = self.rbuf, b''\n                return ret\n            self.sock.settimeout(timeout)\n            try:\n                data = self.sock.recv(self._recvsize)\n            except socket.timeout:\n                raise Timeout(timeout)  # check the rbuf attr for more\n            if len(data) > size:\n                data, self.rbuf = data[:size], data[size:]\n        return data\n"}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if psutil.POSIX:\n            return query_cgroup_cpu_count()\n        else:\n            return float(query_os_cpu_count())\n"}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"\n"}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        elements = []\n        for rootdir, dirs, files in os.walk(self.path):\n            filenames = [f for f in files if f.endswith(self.extension)]\n            for filename in filenames:\n                docpath = os.path.join(rootdir, filename)\n                with open(docpath) as f:\n                    elements.append(yaml.load(f.read()))\n\n        return {\"_default\":\n                {idx: elem for idx, elem in enumerate(elements, start=1)}}\n"}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        max_jitter = self._base / 16.0\n        value = self._base + random.random() * max_jitter - max_jitter / 2\n        self._base = min(self._base * 2, self._max)\n        return value\n"}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "        self.stream_write_box = urwid.Text(caption)\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        self.edit_mode_button = EditModeButton(\n            controller=self.model.controller,\n            width=20,\n        )\n        self.header_write_box.widget_list.append(self.edit_mode_button)\n\n        # Use callback to set stream marker - it shouldn't change, so don't need signal\n        self._set_stream_write_box_style(None, caption)\n"}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    lower, upper = check_bounds(bounds, np.size(bounds[0]), min_separation=0)\n    clipped_array = array.copy()\n\n    if np.allclose(lower, np.min(lower)) and np.allclose(upper, np.max(upper)):\n        clipped_array = np.clip(clipped_array, np.min(lower), np.max(upper))\n    else:\n        if array.ndim != 2:\n            raise ValueError(f\"For non-scalar bounds, input array must be 2-dimensional. Got {array.ndim} dimensions.\")\n\n        for feature in range(array.shape[1]):\n            clipped_array[:, feature] = np.clip(array[:, feature], lower[feature], upper[feature])\n\n    return clipped_array\n"}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dict!\")\n\n    for key, val in metadata.items():\n        if not isinstance(key, (str, int, float)):\n            raise ValueError(\"metadata keys must be strings\")\n\n        metadata[key] = _validate_metadata_entry(val)\n"}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "    @classmethod\n    def html(cls, body, status=200, headers=None):\n        return cls(\n            body,\n            status=status,\n            headers=headers,\n"}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        from twitter.twitter_utils import is_url\n        if not self._config:\n            self.GetHelpConfiguration()\n\n        tweets = []\n        line = []\n        line_length = 0\n        words = re.split(r'\\s', status)\n\n        if len(words) == 1 and not is_url(words[0]):\n            if len(words[0]) > CHARACTER_LIMIT:\n                raise TwitterError(\"Unable to split status into tweetable parts. Word was: {0}/{1}\".format(len(words[0]), char_lim))\n            else:\n                tweets.append(words[0])\n                return tweets\n\n        for word in words:\n            if len(word) > char_lim:\n                raise TwitterError(\"Unable to split status into tweetable parts. Word was: {0}/{1}\".format(len(word), char_lim))\n            new_len = line_length\n\n            if is_url(word):\n                new_len = line_length + self._config['short_url_length_https'] + 1\n            else:\n                new_len += len(word) + 1\n\n            if new_len > CHARACTER_LIMIT:\n                tweets.append(' '.join(line))\n                line = [word]\n                line_length = new_len - line_length\n            else:\n                line.append(word)\n                line_length = new_len\n\n        tweets.append(' '.join(line))\n        return tweets\n"}
{"namespace": "pyinfra.operations.python.call", "completion": "    from pyinfra import logger\n    from pyinfra.api.util import get_call_location\n    argspec = getfullargspec(function)\n    if \"state\" in argspec.args and \"host\" in argspec.args:\n        logger.warning(\n            \"Callback functions used in `python.call` operations no \"\n            f\"longer take `state` and `host` arguments: {get_call_location(frame_offset=3)}\",\n        )\n\n    kwargs.pop(\"state\", None)\n    kwargs.pop(\"host\", None)\n    yield FunctionCommand(function, args, kwargs)\n"}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    ret = []\n    if resolve_pos:\n        fstr = infer_positional_format_args(fstr)\n    formatter = Formatter()\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if lit:\n            ret.append(lit)\n        if fname is None:\n            continue\n        ret.append(BaseFormatField(fname, fspec, conv))\n    return ret\n"}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "    from pyinfra.context import ctx_state\n    state.is_executing = True\n\n    with ctx_state.use(state):\n        # Run all ops, but server by server\n        if serial:\n            _run_serial_ops(state)\n        # Run all the ops on each server in parallel (not waiting at each operation)\n        elif no_wait:\n            _run_no_wait_ops(state)\n        # Default: run all ops in order, waiting at each for all servers to complete\n        else:\n            for op_hash in state.get_op_order():\n                _run_single_op(state, op_hash)\n"}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while not isinstance(node, (LonelyRootNode, LeafNode)):\n            node = self._mem.get_node(node.smallest_entry.before)\n        return node\n"}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        topic_names = self.model.topics_in_stream(self.stream_id)\n\n        topic_typeaheads = match_topics(topic_names, text)\n\n        # Typeaheads and suggestions are the same.\n        return self._process_typeaheads(topic_typeaheads, state, topic_typeaheads)\n"}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    for prefix in ['openat(AT_FDCWD, \"', 'open(\"']:\n        if line.startswith(prefix):\n            parts = line[len(prefix):].split('\", ')\n            if len(parts) != 2:\n                continue\n            if 'ENOENT' in parts[1]:\n                continue\n            if 'O_RDONLY' not in parts[1]:\n                continue\n            if 'O_DIRECTORY' in parts[1]:\n                continue\n            return parts[0]\n    return None\n"}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    next_location_path = None\n    if 300 <= http_response.status < 400:\n        location_header = _extract_first_header_value(http_response, \"Location\")\n        if location_header:\n            parsed_location = urlsplit(location_header)\n            is_relative_url = False if parsed_location.hostname else True\n            if is_relative_url:\n                # Yes, to a relative URL; follow the redirection\n                next_location_path = location_header\n            else:\n                is_absolute_url_to_same_hostname = parsed_location.hostname == server_host_name\n                absolute_url_port = 443 if parsed_location.port is None else parsed_location.port\n                is_absolute_url_to_same_port = absolute_url_port == server_port\n                if is_absolute_url_to_same_hostname and is_absolute_url_to_same_port:\n                    # Yes, to an absolute URL to the same server; follow the redirection\n                    next_location_path = f\"{parsed_location.path}\"\n                    if parsed_location.query:\n                        next_location_path += f\"?{parsed_location.query}\"\n\n    return next_location_path\n"}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n\n        size = min(self.length, size)\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        data = self.unreader.read()\n        while data:\n            buf.write(data)\n            if buf.tell() >= size:\n                break\n            data = self.unreader.read()\n\n        buf = buf.getvalue()\n        ret, rest = buf[:size], buf[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret\n"}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        cn = self.__class__.__name__\n        val_map = super(LRI, self).__repr__()\n        return ('%s(max_size=%r, on_miss=%r, values=%s)'\n                % (cn, self.max_size, self.on_miss, val_map))\n"}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        flattened_list = []\n        link = self._anchor\n        while True:\n            flattened_list.append((link[KEY], link[VALUE]))\n            link = link[NEXT]\n            if link is self._anchor:\n                break\n        return flattened_list\n"}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        raw_key, raw_value = line.split(b'\\t', 1)\n\n        if raw_key != self._last_key_encoded:\n            self._last_key_encoded = raw_key\n            self._last_key_decoded = self._loads(raw_key)\n        return (self._last_key_decoded, self._loads(raw_value))\n"}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        return (self._opts['task_python_bin'] or\n                self._python_bin())\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    total_cmds = sum(seq1_counts.values())\n\n    prior_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    trans_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    # compute prior probs\n    for cmd in seq1_counts:\n        prior_probs[cmd] = seq1_counts[cmd] / total_cmds\n\n    # compute trans probs\n    for prev, currents in seq2_counts.items():\n        for current in currents:\n            trans_probs[prev][current] = seq2_counts[prev][current] / sum(\n                seq2_counts[prev].values()\n            )\n\n    prior_probs_sm = StateMatrix(states=prior_probs, unk_token=unk_token)\n    trans_probs_sm = StateMatrix(states=trans_probs, unk_token=unk_token)\n\n    return prior_probs_sm, trans_probs_sm\n"}
{"namespace": "OpenSSL.rand.add", "completion": "    if not isinstance(buffer, bytes):\n        raise TypeError(\"buffer must be a byte string\")\n\n    if not isinstance(entropy, int):\n        raise TypeError(\"entropy must be an integer\")\n\n    _lib.RAND_add(buffer, len(buffer), entropy)\n"}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return (self.options.run_mapper or\n                self.options.run_combiner or\n                self.options.run_reducer or\n                self.options.run_spark)\n"}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    return connect('cloudsearch', region_name,\n                   connection_cls=CloudSearchConnection, **kw_params)\n"}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    from barf.core.smt.smtsymbol import Bool\n    assert type(cond) is Bool\n\n    return BitVec(size, \"ite\", cond, true, false)\n"}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        return self.nest(\n            Message(\n                body=body,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    from ..encoding.hash import ripemd160\n    stack.append(ripemd160(stack.pop()).digest())\n"}
{"namespace": "falcon.request.Request.host", "completion": "        try:\n            # NOTE(kgriffs): Prefer the host header; the web server\n            # isn't supposed to mess with it, so it should be what\n            # the client actually sent.\n            host_header = self.env['HTTP_HOST']\n            host, port = parse_host(host_header)\n        except KeyError:\n            # PERF(kgriffs): According to PEP-3333, this header\n            # will always be present.\n            host = self.env['SERVER_NAME']\n\n        return host\n"}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, str):\n        path = ascii_(path)\n    D = traverse(resource, path)\n    view_name = D['view_name']\n    context = D['context']\n    if view_name:\n        raise KeyError('%r has no subelement %s' % (context, view_name))\n    return context\n"}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        from mrjob.fs.local import LocalFilesystem\n        from mrjob.fs.hadoop import HadoopFilesystem\n        if self._fs is None:\n            self._fs = CompositeFilesystem()\n\n            # don't pass [] to fs; this means not to use hadoop until\n            # fs.set_hadoop_bin() is called (used for running hadoop over SSH).\n            hadoop_bin = self._opts['hadoop_bin'] or None\n\n            self._fs.add_fs('hadoop',\n                            HadoopFilesystem(hadoop_bin))\n            self._fs.add_fs('local', LocalFilesystem())\n\n        return self._fs\n"}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n    if recipe.depends is None:\n        dependencies = []\n    else:\n        # Turn all dependencies into tuples so that product will work\n        dependencies = fix_deplist(recipe.depends)\n\n        # Filter out blacklisted items and turn lowercase:\n        dependencies = [\n            tuple(set(deptuple) - blacklist)\n            for deptuple in dependencies\n            if tuple(set(deptuple) - blacklist)\n        ]\n    return dependencies\n"}
{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n            return True\n        else:\n            msg = 'You need to provide the global indexes to ' \\\n                  'update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n"}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        sentences_words = [(s, self._to_words_set(s)) for s in document.sentences]\n        ratings = defaultdict(float)\n\n        for (sentence1, words1), (sentence2, words2) in combinations(sentences_words, 2):\n            rank = self._rate_sentences_edge(words1, words2)\n            ratings[sentence1] += rank\n            ratings[sentence2] += rank\n\n        return ratings\n"}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    if note[0] not in _note_dict:\n        return False\n    for post in note[1:]:\n        if post != \"b\" and post != \"#\":\n            return False\n    return True\n"}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        key, val = dict.popitem(self)\n        dict.__delitem__(self.inv, val)\n        return key, val\n"}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    ud_url = _build_instance_metadata_url(url, version, 'user-data')\n    user_data = retry_url(ud_url, retry_on_404=False, num_retries=num_retries, timeout=timeout)\n    if user_data:\n        if sep:\n            l = user_data.split(sep)\n            user_data = {}\n            for nvpair in l:\n                t = nvpair.split('=')\n                user_data[t[0].strip()] = t[1].strip()\n    return user_data\n"}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        quota_root_rep = self._raw_command_untagged(\n            b\"GETQUOTAROOT\", to_bytes(mailbox), uid=False, response_name=\"QUOTAROOT\"\n        )\n        quota_rep = self._imap.untagged_responses.pop(\"QUOTA\", [])\n        quota_root_rep = parse_response(quota_root_rep)\n        quota_root = MailboxQuotaRoots(\n            to_unicode(quota_root_rep[0]), [to_unicode(q) for q in quota_root_rep[1:]]\n        )\n        return quota_root, _parse_quota(quota_rep)\n"}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file) as fd:\n        triggers_dict = json.load(fd)\n    sources = [Source(s) for s in triggers_dict['sources']]\n    sinks = [\n        Sink.from_json(trigger, data)\n        for trigger, data in triggers_dict['sinks'].items()\n    ]\n    return Definitions(sources, sinks)\n"}
{"namespace": "rest_framework.reverse.reverse", "completion": "    scheme = getattr(request, 'versioning_scheme', None)\n    if scheme is not None:\n        try:\n            url = scheme.reverse(viewname, args, kwargs, request, format, **extra)\n        except NoReverseMatch:\n            # In case the versioning scheme reversal fails, fallback to the\n            # default implementation\n            url = _reverse(viewname, args, kwargs, request, format, **extra)\n    else:\n        url = _reverse(viewname, args, kwargs, request, format, **extra)\n\n    return preserve_builtin_query_params(url, request)\n"}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "        self.storage_dir = expanduser(storage_dir)\n        if ' ' in self.storage_dir:\n            raise ValueError('storage dir path cannot contain spaces, please '\n                             'specify a path with --storage-dir')\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dists')\n"}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        bound_cookies = self.cookie_profile.bind(request)\n        token = bound_cookies.get_value()\n        if not token:\n            token = self.new_csrf_token(request)\n        return token\n"}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        with self._recv_lock:\n            if maxsize is _UNSET:\n                maxsize = self.maxsize\n            if maxsize is None:\n                maxsize = _RECV_LARGE_MAXSIZE\n            try:\n                recvd = self.recv_size(maxsize + 1, timeout)\n            except ConnectionClosed:\n                ret, self.rbuf = self.rbuf, b''\n            else:\n                # put extra received bytes (now in rbuf) after recvd\n                self.rbuf = recvd + self.rbuf\n                size_read = min(maxsize, len(self.rbuf))\n                raise MessageTooLong(size_read)  # check receive buffer\n        return ret\n"}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        spec_obj = list(\n            filter(\n                lambda x: x.channel == channel and x.value == \"\" if hasattr(x, \"channel\") else False,\n                self._inferred_intent,\n            )\n        )\n        return spec_obj\n"}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "    from zulipterminal.config.regexes import REGEX_QUOTED_FENCE_LENGTH\n    max_length_fence = 3\n\n    matches = findall(REGEX_QUOTED_FENCE_LENGTH, content, flags=MULTILINE)\n    if len(matches) != 0:\n        max_length_fence = max(max_length_fence, len(max(matches, key=len)) + 1)\n\n    return \"`\" * max_length_fence\n"}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        lookup = {v: k for k, v in self.levels.items()}\n        if value in lookup:\n            return encode(lookup[value])\n        return \"\"\n"}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        if not self.iter:\n            return b\"\"\n        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"\n"}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        if state is None:\n            state = self.block.state_cls()\n\n        state.env['__file__'] = filepath\n        with open(filepath, 'rb') as f:\n            s = f.read()\n\n        s = s.decode(encoding)\n        return self.parse(s, state)\n"}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n"}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        expected_args = self._get_expected_args(bound)\n        return [a for a in expected_args[len(args) :] if a not in kwargs]\n"}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    from zulipterminal.config.regexes import REGEX_COLOR_6_DIGIT\n    from zulipterminal.config.regexes import REGEX_COLOR_3_DIGIT\n    if match(REGEX_COLOR_6_DIGIT, color, ASCII) is not None:\n        # '#xxxxxx' color, stored by current zulip server\n        return (color[:2] + color[3] + color[5]).lower()\n    elif match(REGEX_COLOR_3_DIGIT, color, ASCII) is not None:\n        # '#xxx' color, which may be stored by the zulip server <= 2.0.0\n        # Potentially later versions too\n        return color.lower()\n    else:\n        raise ValueError(f'Unknown format for color \"{color}\"')\n"}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    routes = []\n    for sr, _, _ in app._static_routes:\n        info = StaticRouteInfo(sr._prefix, sr._directory, sr._fallback_filename)\n        routes.append(info)\n    return routes\n"}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        if value.lower() in log.COLORS:\n            return encode(value.lower())\n        return \"\"\n"}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        current_buffer_size = len(self.buffer)\n        if ((current_buffer_size - self.buffer_end_offset) /\n                current_buffer_size) < self.BUFFER_RESIZE_TRIGGER:\n            resized_buffer = bytearray(current_buffer_size * 2)\n            # copy current buffer content to new buffer.\n            resized_buffer[0:current_buffer_size] = self.buffer\n            # point to new buffer.\n            self.buffer = resized_buffer\n\n        # Memory view is required in order to read into a subset of a byte\n        # array\n        try:\n            length_read = self.stream.readinto(\n                memoryview(self.buffer)[self.buffer_end_offset:])\n            self.buffer_end_offset += length_read\n\n            if not length_read:\n                logger.debug(u'JSON RPC Reader reached end of stream')\n                raise EOFError(u'End of stream reached, no output.')\n\n            return True\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Reader on read_next_chunk encountered exception: %s', ex)\n            # Stream was closed.\n            raise\n"}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    from .util import parse_cli_arg\n    operation_name = commands[0]\n\n    op = try_import_module_attribute(operation_name, prefix=\"pyinfra.operations\")\n\n    # Parse the arguments\n    operation_args = commands[1:]\n\n    if len(operation_args) == 1:\n        # Check if we're JSON (in which case we expect a list of two items:\n        # a list of args and a dict of kwargs).\n        try:\n            args, kwargs = json.loads(operation_args[0])\n            return op, (args or (), kwargs or {})\n        except ValueError:\n            pass\n\n    args = [parse_cli_arg(arg) for arg in operation_args if \"=\" not in arg]\n\n    kwargs = {\n        key: parse_cli_arg(value)\n        for key, value in [arg.split(\"=\", 1) for arg in operation_args if \"=\" in arg]\n    }\n\n    return op, (args, kwargs)\n"}
{"namespace": "mrjob.conf.combine_opts", "completion": "    final_opts = {}\n\n    keys = set()\n    for opts in opts_list:\n        if isinstance(opts, ClearedValue):\n            raise TypeError\n        elif opts:\n            keys.update(opts)\n\n    for key in keys:\n        values = _resolve_clear_tags_in_list(\n            opts[key] for opts in opts_list if opts and key in opts)\n\n        combine_func = combiners.get(key) or combine_values\n        final_opts[key] = combine_func(*values)\n\n    return final_opts\n"}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)\n"}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type_byte = data[0:NODE_TYPE_BYTES]\n        node_type_int = int.from_bytes(node_type_byte, ENDIAN)\n        if node_type_int == 1:\n            return LonelyRootNode(tree_conf, data, page)\n        elif node_type_int == 2:\n            return RootNode(tree_conf, data, page)\n        elif node_type_int == 3:\n            return InternalNode(tree_conf, data, page)\n        elif node_type_int == 4:\n            return LeafNode(tree_conf, data, page)\n        else:\n            assert False, 'No Node with type {} exists'.format(node_type_int)\n"}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        from googleapiclient.errors import HttpError\n        self._log_response(resp, content)\n        # Error handling is TBD, for example, do we retry\n        # for some operation/error combinations?\n        if resp.status < 300:\n            if resp.status == 204:\n                # A 204: No Content response should be treated differently\n                # to all the other success states\n                return self.no_content_response\n            return self.deserialize(content)\n        else:\n            LOGGER.debug(\"Content from bad request was: %r\" % content)\n            raise HttpError(resp, content)\n"}
{"namespace": "fs.path.parts", "completion": "    _path = normpath(path)\n    components = _path.strip(\"/\")\n\n    _parts = [\"/\" if _path.startswith(\"/\") else \"./\"]\n    if components:\n        _parts += components.split(\"/\")\n    return _parts\n"}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    if svc.bento is not None:\n        bento = svc.bento\n        event_properties = ServeInitEvent(\n            serve_id=serve_info.serve_id,\n            serve_from_bento=True,\n            serve_from_server_api=from_server_api,\n            production=production,\n            serve_kind=serve_kind,\n            bento_creation_timestamp=bento.info.creation_time,\n            num_of_models=len(bento.info.models),\n            num_of_runners=len(svc.runners),\n            num_of_apis=len(bento.info.apis),\n            model_types=[m.module for m in bento.info.models],\n            runnable_types=[r.runnable_type for r in bento.info.runners],\n            api_input_types=[api.input_type for api in bento.info.apis],\n            api_output_types=[api.output_type for api in bento.info.apis],\n        )\n    else:\n        event_properties = ServeInitEvent(\n            serve_id=serve_info.serve_id,\n            serve_from_bento=False,\n            serve_from_server_api=from_server_api,\n            production=production,\n            serve_kind=serve_kind,\n            bento_creation_timestamp=None,\n            num_of_models=len(\n                set(\n                    svc.models\n                    + [model for runner in svc.runners for model in runner.models]\n                )\n            ),\n            num_of_runners=len(svc.runners),\n            num_of_apis=len(svc.apis.keys()),\n            runnable_types=[r.runnable_class.__name__ for r in svc.runners],\n            api_input_types=[api.input.__class__.__name__ for api in svc.apis.values()],\n            api_output_types=[\n                api.output.__class__.__name__ for api in svc.apis.values()\n            ],\n        )\n\n    track(event_properties)\n"}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        for cont in self.bar:\n            cont[2].transpose(interval, up)\n"}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        category = self._categories.setdefault(intr.category_name, {})\n        category[intr.discriminator] = intr\n        category[intr.discriminator_hash] = intr\n        intr.order = self._counter\n        self._counter += 1\n"}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item not in self.item_index_map:\n            self.item_index_map[item] = len(self.item_list)\n            self.item_list.append(item)\n"}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        from ydata_profiling.model.typeset import ProfilingTypeSet\n        if self._typeset is None:\n            self._typeset = ProfilingTypeSet(self.config, self._type_schema)\n        return self._typeset\n"}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(\n            self.key, self.before, self.after\n        )\n"}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            self._cached_forwarded_prefix = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.app\n            )\n\n        return self._cached_forwarded_prefix\n"}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )\n"}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        word_freq = self._compute_tf(sentences)\n        ratings = {}\n\n        # make it a list so that it can be modified\n        sentences_list = list(sentences)\n\n        # get all content words once for efficiency\n        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n\n        # Removes one sentence per iteration by adding to summary\n        while len(sentences_list) > 0:\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            best_sentence = sentences_list.pop(best_sentence_index)\n\n            # value is the iteration in which it was removed multiplied by -1 so that the first sentences removed (the most important) have highest values\n            ratings[best_sentence] = -len(ratings)\n\n            # update probabilities\n            best_sentence_words = sentences_as_words.pop(best_sentence_index)\n            self._update_tf(word_freq, best_sentence_words)\n\n        return ratings\n"}
{"namespace": "fs.info.Info.type", "completion": "        from .enums import ResourceType\n        self._require_namespace(\"details\")\n        return ResourceType(self.get(\"details\", \"type\", 0))\n"}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    can_be_synced = True\n\n    # If the given path is relative, prepend home\n    fullpath = os.path.join(os.environ[\"HOME\"], path)\n\n    # Compute the ~/Library path on macOS\n    # End it with a slash because we are looking for this specific folder and\n    # not any file/folder named LibrarySomething\n    library_path = os.path.join(os.environ[\"HOME\"], \"Library/\")\n\n    if platform.system() == constants.PLATFORM_LINUX:\n        if fullpath.startswith(library_path):\n            can_be_synced = False\n\n    return can_be_synced\n"}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    elif urllib.parse.urlparse(arg).scheme == \"\":\n        raise exceptions.ValidationError(msg.format(arg=arg))\n"}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError(\n                'MultiFileReader.seek() only supports os.SEEK_SET')\n        if offset != 0:\n            raise NotImplementedError(\n                'MultiFileReader only supports seeking to start at this time')\n        for f in self._fileobjs:\n            f.seek(0)\n"}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        with self._database.atomic():\n            try:\n                result = self[key]\n            except KeyError:\n                if default is Sentinel:\n                    raise\n                return default\n            del self[key]\n\n        return result\n"}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        if name is None:\n            name = self.name\n        if package is None:\n            package = self.package\n        if registry is None:\n            registry = self.registry\n        return self.__class__(name=name, package=package, registry=registry)\n"}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gunicorn.workers.gthread.ThreadWorker\"\n\n        worker_class = util.load_class(uri)\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class\n"}
{"namespace": "boltons.iterutils.chunked", "completion": "    chunk_iter = chunked_iter(src, size, **kw)\n    if count is None:\n        return list(chunk_iter)\n    else:\n        return list(itertools.islice(chunk_iter, count))\n"}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        key = self.get_keys()\n        # Build a new dict of only the data we're changing.\n        final_data, fields = self.prepare_partial()\n\n        if not final_data:\n            return False\n\n        # Remove the key(s) from the ``final_data`` if present.\n        # They should only be present if this is a new item, in which\n        # case we shouldn't be sending as part of the data to update.\n        for fieldname, value in key.items():\n            if fieldname in final_data:\n                del final_data[fieldname]\n\n                try:\n                    # It's likely also in ``fields``, so remove it there too.\n                    fields.remove(fieldname)\n                except KeyError:\n                    pass\n\n        # Build expectations of only the fields we're planning to update.\n        expects = self.build_expects(fields=fields)\n        returned = self.table._update_item(key, final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned\n"}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        tb_str = self.tb_info.get_formatted()\n        return ''.join([tb_str, '%s: %s' % (self.exc_type, self.exc_msg)])\n"}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        if not isinstance(allowed_method, (list, tuple)):\n            allowed_method = [allowed_method]\n        if not isinstance(allowed_origin, (list, tuple)):\n            allowed_origin = [allowed_origin]\n        if not isinstance(allowed_origin, (list, tuple)):\n            if allowed_origin is None:\n                allowed_origin = []\n            else:\n                allowed_origin = [allowed_origin]\n        if not isinstance(expose_header, (list, tuple)):\n            if expose_header is None:\n                expose_header = []\n            else:\n                expose_header = [expose_header]\n        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header,\n                        max_age_seconds, expose_header)\n        self.append(rule)\n"}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token\n"}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    from jc.parsers.pyedid.edid import Edid\n    from jc.parsers.pyedid.helpers.edid_helper import EdidHelper\n    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    if not re.match(_edid_head_pattern, next_line):\n        next_lines.append(next_line)\n        return None\n\n    edid_hex_value = \"\"\n\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n\n        if not result:\n            next_lines.append(next_line)\n            break\n\n        matches = result.groupdict()\n        edid_hex_value += matches[\"edid_line\"]\n\n    edid = Edid(EdidHelper.hex2bytes(edid_hex_value))\n\n    model: Model = {\n        \"name\": edid.name or \"Generic\",\n        \"product_id\": str(edid.product),\n        \"serial_number\": str(edid.serial),\n    }\n    return model\n"}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    if match['graph'] in ['qwerty', 'dvorak']:\n        s = KEYBOARD_STARTING_POSITIONS\n        d = KEYBOARD_AVERAGE_DEGREE\n    else:\n        s = KEYPAD_STARTING_POSITIONS\n        d = KEYPAD_AVERAGE_DEGREE\n    guesses = 0\n    L = len(match['token'])\n    t = match['turns']\n    # estimate the number of possible patterns w/ length L or less with t turns\n    # or less.\n    for i in range(2, L + 1):\n        possible_turns = min(t, i - 1) + 1\n        for j in range(1, possible_turns):\n            guesses += nCk(i - 1, j - 1) * s * pow(d, j)\n    # add extra guesses for shifted keys. (% instead of 5, A instead of a.)\n    # math is similar to extra guesses of l33t substitutions in dictionary\n    # matches.\n    if match['shifted_count']:\n        S = match['shifted_count']\n        U = len(match['token']) - match['shifted_count']  # unshifted count\n        if S == 0 or U == 0:\n            guesses *= 2\n        else:\n            shifted_variations = 0\n            for i in range(1, min(S, U) + 1):\n                shifted_variations += nCk(S + U, i)\n            guesses *= shifted_variations\n\n    return guesses\n"}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    search = \"\"\n    if instrument is not None:\n        search = str.upper(instrument)\n    result = []\n    keys = list(_known.keys())\n    inkeys = search in keys\n    for x in keys:\n        if (\n            instrument is None\n            or not inkeys\n            and x.find(search) == 0\n            or inkeys\n            and search == x\n        ):\n            if nr_of_strings is None and nr_of_courses is None:\n                result += list(_known[x][1].values())\n            elif nr_of_strings is not None and nr_of_courses is None:\n                result += [\n                    y\n                    for y in six.itervalues(_known[x][1])\n                    if y.count_strings() == nr_of_strings\n                ]\n            elif nr_of_strings is None and nr_of_courses is not None:\n                result += [\n                    y\n                    for y in six.itervalues(_known[x][1])\n                    if y.count_courses() == nr_of_courses\n                ]\n            else:\n                result += [\n                    y\n                    for y in six.itervalues(_known[x][1])\n                    if y.count_strings() == nr_of_strings\n                    and y.count_courses() == nr_of_courses\n                ]\n    return result\n"}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key in self.data:\n            for val in self.data[key]:\n                yield key, val\n"}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        bare_path_glob = _from_file_uri(path_glob)\n        uri_scheme = path_glob[0:-len(bare_path_glob)]  # 'file:///' or ''\n\n        for path in glob.glob(bare_path_glob):\n            if os.path.isdir(path):\n                for dirname, _, filenames in os.walk(path, followlinks=True):\n                    for filename in filenames:\n                        yield uri_scheme + os.path.join(dirname, filename)\n            else:\n                yield uri_scheme + path\n"}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()\n"}
{"namespace": "fs.path.iswildcard", "completion": "    assert path is not None\n    return not _WILD_CHARS.isdisjoint(path)\n"}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.alias import AliasDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.alias]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return AliasDefinition(self.identifier, self._build_modifiers_repr())\n"}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.dynamodb.layer2 import Layer2\n    return connect('dynamodb', region_name, connection_cls=Layer2, **kw_params)\n"}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    from .errors import MissingTokenException\n    from .errors import MissingTokenTypeException\n    fragment = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(fragment, keep_blank_values=True))\n\n    if 'access_token' not in params:\n        raise MissingTokenException()\n\n    if 'token_type' not in params:\n        raise MissingTokenTypeException()\n\n    if state and params.get('state', None) != state:\n        raise MismatchingStateException()\n\n    return params\n"}
{"namespace": "dominate.util.include", "completion": "  fl = open(f, 'r')\n  data = fl.read()\n  fl.close()\n  return raw(data)\n"}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        from mrjob.options import _RUNNER_OPTS\n        kwargs = combine_dicts(\n            self._non_option_kwargs(),\n            # don't screen out irrelevant opts (see #1898)\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\n            self._job_kwargs(),\n        )\n\n        if self._runner_class().alias in ('inline', 'spark'):\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\n\n        # pass steps to runner (see #1845)\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\n\n        return kwargs\n"}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            self.send(b'')\n        return\n"}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size * len(args), \"concat\", *args)\n"}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        autocomplete_map = OrderedDict(\n            [\n                (\"@_\", self.autocomplete_users),\n                (\"@_**\", self.autocomplete_users),\n                (\"@\", self.autocomplete_mentions),\n                (\"@*\", self.autocomplete_groups),\n                (\"@**\", self.autocomplete_users),\n                (\"#\", self.autocomplete_streams),\n                (\"#**\", self.autocomplete_streams),\n                (\":\", self.autocomplete_emojis),\n            ]\n        )\n\n        # Look in a reverse order to find the last autocomplete prefix used in\n        # the text. For instance, if text='@#example', use '#' as the prefix.\n        # FIXME: Mentions can actually start with '#', and streams with\n        #        anything; this implementation simply chooses the right-most\n        #        match of the longest length\n        prefix_indices = {prefix: text.rfind(prefix) for prefix in autocomplete_map}\n\n        text = self.validate_and_patch_autocomplete_stream_and_topic(\n            text, autocomplete_map, prefix_indices\n        )\n\n        found_prefix_indices = {\n            prefix: index for prefix, index in prefix_indices.items() if index > -1\n        }\n        # Return text if it doesn't have any of the autocomplete prefixes.\n        if not found_prefix_indices:\n            return text\n\n        # Use latest longest matching prefix (so @_ wins vs @)\n        prefix_index = max(found_prefix_indices.values())\n        prefix = max(\n            (len(prefix), prefix)\n            for prefix, index in found_prefix_indices.items()\n            if index == prefix_index\n        )[1]\n        autocomplete_func = autocomplete_map[prefix]\n\n        # NOTE: The following block only executes if any of the autocomplete\n        # prefixes exist.\n        typeaheads, suggestions = autocomplete_func(text[prefix_index:], prefix)\n\n        typeahead = self._process_typeaheads(typeaheads, state, suggestions)\n        if typeahead:\n            typeahead = text[:prefix_index] + typeahead\n        return typeahead\n"}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        tmpl = '%0{0}X'.format(2 * (self.len // 8 + ((self.len % 8) != 0)))\n        ret = tmpl % self.val\n        return ret\n"}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        qs_pairs = self.get_querystring_pairs()\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select {col} as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by {col} order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet\": column})\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                if self.table:\n                    # Attempt to expand foreign keys into labels\n                    values = [row[\"value\"] for row in facet_rows]\n                    expanded = await self.ds.expand_foreign_keys(\n                        self.database, self.table, column, values\n                    )\n                else:\n                    expanded = {}\n                for row in facet_rows:\n                    column_qs = column\n                    if column.startswith(\"_\"):\n                        column_qs = \"{}__exact\".format(column)\n                    selected = (column_qs, str(row[\"value\"])) in qs_pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {column_qs: str(row[\"value\"])}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {column_qs: row[\"value\"]}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": row[\"value\"],\n                            \"label\": expanded.get((column, row[\"value\"]), row[\"value\"]),\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, self.ds.urls.path(toggle_path)\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out\n"}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(\n            adapter, (type_or_iface,), IJSONAdapter\n        )\n"}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, bytes):\n            # Assume we have the correct encoding\n            binary = message\n        else:\n            # Encode the unicode message into a bytestring\n            if isinstance(message, hl7.Message):\n                message = str(message)\n            binary = message.encode(self.encoding)\n\n        # wrap in MLLP message container\n        data = SB + binary + EB + CR\n        return self.send(data)\n"}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        parts = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n          '<WebsiteConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">']\n        if self.suffix is not None:\n            parts.append(tag('IndexDocument', tag('Suffix', self.suffix)))\n        if self.error_key is not None:\n            parts.append(tag('ErrorDocument', tag('Key', self.error_key)))\n        if self.redirect_all_requests_to is not None:\n            parts.append(self.redirect_all_requests_to.to_xml())\n        if self.routing_rules:\n            parts.append(self.routing_rules.to_xml())\n        parts.append('</WebsiteConfiguration>')\n        return ''.join(parts)\n"}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        contexts = []\n\n        def handler(loop, ctx):\n            contexts.append(ctx)\n\n        old_handler = self.loop.get_exception_handler()\n        self.loop.set_exception_handler(handler)\n        try:\n            yield\n\n            for ctx in contexts:\n                msg = ctx.get('message')\n                if msg and re.search(msg_re, msg):\n                    return\n\n            raise AssertionError(\n                'no message matching {!r} was logged with '\n                'loop.call_exception_handler()'.format(msg_re))\n\n        finally:\n            self.loop.set_exception_handler(old_handler)\n"}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    txt = (\n        \"\"\"\nfrom alembic import context\n\nconfig = context.config\n\"\"\"\n        + txt\n    )\n\n    path = os.path.join(dir_, \"env.py\")\n    pyc_path = util.pyc_file_from_path(path)\n    if pyc_path:\n        os.unlink(pyc_path)\n\n    with open(path, \"w\") as f:\n        f.write(txt)\n"}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        if not cls.modname_to_dist:\n            # some packagenames don't match the module names (e.g. PyYAML)\n            # so we set up a dict to map from module name to package name\n            for dist in pkg_resources.working_set:\n                try:\n                    toplevel_names = dist._get_metadata(\"top_level.txt\")\n                    for tln in toplevel_names:\n                        cls.modname_to_dist[tln] = dist.project_name, dist.version\n                except Exception:\n                    pass\n\n        name, version = cls.modname_to_dist.get(mod.__name__, (mod.__name__, None))\n\n        return PackageDependency(name, version)\n"}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        return self._introspect_all_indexes(\n            raw_indexes, self._PROJECTION_TYPE_TO_INDEX.get('local_indexes'))\n"}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            with open(self._filename) as fp:\n                authorizer.refresh_token = fp.read().strip()\n"}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        self._checkClosed()\n        return self._tell\n"}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )\n"}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    from .pycompat import string_types\n    if isinstance(x, collections_abc.Iterable) and \\\n                                               not isinstance(x, string_types):\n        return tuple(x)\n    else:\n        return (x,)\n"}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        if not self.secret_key:\n            raise ValueError(\"JWT does not have a signing key configured.\")\n\n        headers = self.headers.copy()\n\n        payload = self.payload.copy()\n        if ttl:\n            payload[\"exp\"] = int(time.time()) + ttl\n\n        return jwt_lib.encode(\n            payload, self.secret_key, algorithm=self.algorithm, headers=headers\n        )\n"}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)\n"}
{"namespace": "telethon.extensions.html.parse", "completion": "    from ..helpers import strip_text\n    if not html:\n        return html, []\n\n    parser = HTMLToTelegramParser()\n    parser.feed(add_surrogate(html))\n    text = strip_text(parser.text, parser.entities)\n    return del_surrogate(text), parser.entities\n"}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        summarization_method = self._build_cue_method_instance()\n        return summarization_method(document, sentences_count, bonus_word_value,\n            stigma_word_value)\n"}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 10)\n"}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if isinstance(id_, (list, tuple, set, frozenset)):\n            return sum([self.get_revisions(id_elem) for id_elem in id_], ())\n        else:\n            resolved_id, branch_label = self._resolve_revision_number(id_)\n            if len(resolved_id) == 1:\n                try:\n                    rint = int(resolved_id[0])\n                    if rint < 0:\n                        # branch@-n -> walk down from heads\n                        select_heads = self.get_revisions(\"heads\")\n                        if branch_label is not None:\n                            select_heads = tuple(\n                                head\n                                for head in select_heads\n                                if branch_label\n                                in is_revision(head).branch_labels\n                            )\n                        return tuple(\n                            self._walk(head, steps=rint)\n                            for head in select_heads\n                        )\n                except ValueError:\n                    # couldn't resolve as integer\n                    pass\n            return tuple(\n                self._revision_for_ident(rev_id, branch_label)\n                for rev_id in resolved_id\n            )\n"}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        with self._recv_lock:\n            with self._send_lock:\n                self.rbuf = b''\n                self.rbuf_unconsumed = self.rbuf\n                self.sbuf[:] = []\n                self.sock.close()\n        return\n"}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    return _uq_constraint(constraint, autogen_context, False)\n"}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for k, v in kw.items():\n            myval = self._received.get(k, _marker)\n            if myval is _marker:\n                myval = self._implementation._received.get(k, _marker)\n                if myval is _marker:\n                    raise AssertionError(\n                        'A value for key \"%s\" was not passed to the renderer'\n                        % k\n                    )\n\n            if myval != v:\n                raise AssertionError(\n                    '\\nasserted value for %s: %r\\nactual value: %r'\n                    % (k, v, myval)\n                )\n        return True\n"}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        from pycoin.coins.SolutionChecker import ScriptError\n        from . import errno\n        if len(s) == 0:\n            return 0\n        s = bytearray(s)\n        s.reverse()\n        i = s[0]\n        v = i & 0x7f\n        if require_minimal:\n            if v == 0:\n                if len(s) <= 1 or ((s[1] & 0x80) == 0):\n                    raise ScriptError(\"non-minimally encoded\", errno.UNKNOWN_ERROR)\n        is_negative = ((i & 0x80) > 0)\n        for b in s[1:]:\n            v <<= 8\n            v += b\n        if is_negative:\n            v = -v\n        return v\n"}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        from . import response_lexer\n        data = self._command_and_check(\"getacl\", self._normalise_folder(folder))\n        parts = list(response_lexer.TokenSource(data))\n        parts = parts[1:]  # First item is folder name\n        return [(parts[i], parts[i + 1]) for i in range(0, len(parts), 2)]\n"}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    request = urllib.request.Request(url)\n    request.add_header('User-Agent', agent)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if isinstance(modified, str):\n        modified = _parse_date(modified)\n    elif isinstance(modified, datetime.datetime):\n        modified = modified.utctimetuple()\n    if modified:\n        # format into an RFC 1123-compliant timestamp. We can't use\n        # time.strftime() since the %a and %b directives can be affected\n        # by the current locale, but RFC 2616 states that dates must be\n        # in English.\n        short_weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n        request.add_header('If-Modified-Since', '%s, %02d %s %04d %02d:%02d:%02d GMT' % (short_weekdays[modified[6]], modified[2], months[modified[1] - 1], modified[0], modified[3], modified[4], modified[5]))\n    if referrer:\n        request.add_header('Referer', referrer)\n    request.add_header('Accept-encoding', 'gzip, deflate')\n    if auth:\n        request.add_header('Authorization', 'Basic %s' % auth)\n    if accept_header:\n        request.add_header('Accept', accept_header)\n    # use this for whatever -- cookies, special headers, etc\n    # [('Cookie','Something'),('x-special-header','Another Value')]\n    for header_name, header_value in request_headers.items():\n        request.add_header(header_name, header_value)\n    request.add_header('A-IM', 'feed')  # RFC 3229 support\n    return request\n"}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    from zxcvbn.scoring import most_guessable_match_sequence\n    matches = []\n    greedy = re.compile(r'(.+)\\1+')\n    lazy = re.compile(r'(.+?)\\1+')\n    lazy_anchored = re.compile(r'^(.+?)\\1+$')\n    last_index = 0\n    while last_index < len(password):\n        greedy_match = greedy.search(password, pos=last_index)\n        lazy_match = lazy.search(password, pos=last_index)\n\n        if not greedy_match:\n            break\n\n        if len(greedy_match.group(0)) > len(lazy_match.group(0)):\n            # greedy beats lazy for 'aabaab'\n            #   greedy: [aabaab, aab]\n            #   lazy:   [aa,     a]\n            match = greedy_match\n            # greedy's repeated string might itself be repeated, eg.\n            # aabaab in aabaabaabaab.\n            # run an anchored lazy match on greedy's repeated string\n            # to find the shortest repeated string\n            base_token = lazy_anchored.search(match.group(0)).group(1)\n        else:\n            match = lazy_match\n            base_token = match.group(1)\n\n        i, j = match.span()[0], match.span()[1] - 1\n\n        # recursively match and score the base string\n        base_analysis = most_guessable_match_sequence(\n            base_token,\n            omnimatch(base_token)\n        )\n        base_matches = base_analysis['sequence']\n        base_guesses = base_analysis['guesses']\n        matches.append({\n            'pattern': 'repeat',\n            'i': i,\n            'j': j,\n            'token': match.group(0),\n            'base_token': base_token,\n            'base_guesses': base_guesses,\n            'base_matches': base_matches,\n            'repeat_count': len(match.group(0)) / len(base_token),\n        })\n        last_index = j + 1\n\n    return matches\n"}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        sock = self.socket()\n\n        # make the socket non-blocking so the timeout can be\n        # implemented for this call\n        sock.settimeout(None)\n        sock.setblocking(0)\n\n        if POLL_SUPPORT:\n            poll_func = self._poll_socket\n        else:\n            poll_func = self._select_poll_socket\n\n        try:\n            resps = []\n            events = poll_func(sock, timeout)\n            if events:\n                while True:\n                    try:\n                        line = self._imap._get_line()\n                    except (socket.timeout, socket.error):\n                        break\n                    except IMAPClient.AbortError:\n                        # An imaplib.IMAP4.abort with \"EOF\" is raised\n                        # under Python 3\n                        err = sys.exc_info()[1]\n                        if \"EOF\" in err.args[0]:\n                            break\n                        raise\n                    else:\n                        resps.append(_parse_untagged_response(line))\n            return resps\n        finally:\n            sock.setblocking(1)\n            self._set_read_timeout()\n"}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if isinstance(ip, ipaddress._BaseNetwork):  # pylint disable=protected-access\n    imprecise_ip = ip\n  else:\n    imprecise_ip = ipaddress.ip_network(ip, strict=strict)\n  if imprecise_ip.version == 4:\n    return IPv4(ip, comment, token, strict=strict)\n  elif imprecise_ip.version == 6:\n    return IPv6(ip, comment, token, strict=strict)\n  raise ValueError('Provided IP string \"%s\" is not a valid v4 or v6 address'\n                   % ip)\n"}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    stream_name = stream_name.replace(\" \", \"-\")\n    return str(stream_id) + \"-\" + hash_util_encode(stream_name)\n"}
{"namespace": "discord.ext.tasks.loop", "completion": "    def decorator(func: LF) -> Loop[LF]:\n        return Loop[LF](\n            func,\n            seconds=seconds,\n            minutes=minutes,\n            hours=hours,\n            count=count,\n            time=time,\n            reconnect=reconnect,\n        )\n\n    return decorator\n"}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)\n"}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    assert type(s) in (Constant, BitVec)\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset + size - 1, offset), s)\n"}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.cloudsearch.layer1 import Layer1\n    return connect('cloudsearch', region_name, connection_cls=Layer1,\n                   **kw_params)\n"}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        normalized_words = self._normalize_words(sentence.words)\n        normalized_content_words = self._filter_out_stop_words(normalized_words)\n        stemmed_normalized_content_words = self._stem_words(normalized_content_words)\n        return stemmed_normalized_content_words\n"}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        pos = self.tell()\n        if self._rolled:\n            self.seek(0)\n            val = os.fstat(self.fileno()).st_size\n        else:\n            self.seek(0, os.SEEK_END)\n            val = self.tell()\n        self.seek(pos)\n        return val\n"}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    lines = [line.strip() for line in content.splitlines() if len(line.strip())]\n    if not len(lines):\n        return lines\n\n    # The strace output will start with the exec call of its argument.\n    strace_mode = extract_exec_path(lines[0]) is not None\n    if not strace_mode:\n        return lines\n\n    # Extract files from `open()`, `openat()`, and `exec()` calls.\n    paths = set()\n    for line in lines:\n        path = extract_exec_path(line) or extract_open_path(line) or extract_stat_path(line)\n        if path:\n            blacklisted = any(path.startswith(directory) for directory in blacklisted_directories)\n            if not blacklisted:\n                if not existing_only:\n                    paths.add(path)\n                    continue\n                if os.path.exists(path) and os.access(path, os.R_OK) and not os.path.isdir(path):\n                    paths.add(path)\n\n    return list(paths)\n"}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        from boto.exception import BotoClientError\n        if validate is False:\n            if headers or version_id or response_headers:\n                raise BotoClientError(\n                    \"When providing 'validate=False', no other params \" + \\\n                    \"are allowed.\"\n                )\n\n            # This leans on the default behavior of ``new_key`` (not hitting\n            # the service). If that changes, that behavior should migrate here.\n            return self.new_key(key_name)\n\n        query_args_l = []\n        if version_id:\n            query_args_l.append('versionId=%s' % version_id)\n        if response_headers:\n            for rk, rv in six.iteritems(response_headers):\n                query_args_l.append('%s=%s' % (rk, urllib.parse.quote(rv)))\n\n        key, resp = self._get_key_internal(key_name, headers, query_args_l)\n        return key\n"}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        if not parsed_link:\n            return \"The narrow link seems to be either broken or unsupported\"\n\n        # Validate stream data.\n        if \"stream\" in parsed_link:\n            error = self._validate_and_patch_stream_data(parsed_link)\n            if error:\n                return error\n\n        # Validate topic name.\n        if \"topic_name\" in parsed_link:\n            topic_name = parsed_link[\"topic_name\"]\n            stream_id = parsed_link[\"stream\"][\"stream_id\"]\n\n            if topic_name not in self.model.topics_in_stream(stream_id):\n                return \"Invalid topic name\"\n\n        # Validate message ID for near.\n        if \"near\" in parsed_link[\"narrow\"]:\n            message_id = parsed_link.get(\"message_id\")\n\n            if message_id is None:\n                return \"Invalid message ID\"\n\n        return \"\"\n"}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        from mrjob.util import log_to_stream\n        from mrjob.util import log_to_null\n        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\n            log_to_stream(name='__main__', debug=verbose, stream=stream)\n"}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [\n        note,\n        intervals.major_third(note),\n        notes.augment(intervals.major_fifth(note)),\n    ]\n"}
{"namespace": "twtxt.config.Config.discover", "completion": "        file = os.path.join(Config.config_dir, Config.config_name)\n        return cls.from_file(file)\n"}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        if self._reverse:\n            cols_and_constraints = self._reverse.columns\n        else:\n            cols_and_constraints = []\n\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        t = schema_obj.table(\n            self.table_name,\n            *cols_and_constraints,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            schema=self.schema,\n            _constraints_included=self._reverse._constraints_included\n            if self._reverse\n            else False,\n            **self.table_kw,\n        )\n        return t\n"}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        from boto.ec2.instancestatus import InstanceStatusSet\n        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if max_results:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_object('DescribeInstanceStatus', params,\n                               InstanceStatusSet, verb='POST')\n"}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                instance_id=instance_id,\n                public_ip=self.public_ip,\n                allocation_id=self.allocation_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n        return self.connection.associate_address(\n            instance_id=instance_id,\n            public_ip=self.public_ip,\n            network_interface_id=network_interface_id,\n            private_ip_address=private_ip_address,\n            allow_reassociation=allow_reassociation,\n            dry_run=dry_run\n        )\n"}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n"}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        if not value:\n            return \"\"\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        serialized_values = []\n        for item in value:\n            serialized_value = subtype.serialize(item, display=display)\n            if serialized_value:\n                serialized_values.append(serialized_value)\n\n        return \"\\n  \" + \"\\n  \".join(serialized_values)\n"}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.increment(\"gunicorn.log.critical\", 1)\n"}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        num_executors = self._num_cores()\n\n        # for now assigning one core per executor, so we don't have to worry\n        # about a number of cores that's not evenly divisible\n        cores_per_executor = 1\n\n        executor_mem_bytes = _to_num_bytes(\n            self._opts['jobconf'].get('spark.executor.memory') or\n            _DEFAULT_EXECUTOR_MEMORY)\n        executor_mem_mb = math.ceil(executor_mem_bytes / 1024.0 / 1024.0)\n\n        return 'local-cluster[%d,%d,%d]' % (\n            num_executors, cores_per_executor, executor_mem_mb)\n"}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)\n"}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n"}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        if global_index:\n            gsi_data = []\n            gsi_data_attr_def = []\n\n            gsi_data.append({\n                \"Create\": global_index.schema()\n            })\n\n            for attr_def in global_index.parts:\n                gsi_data_attr_def.append(attr_def.definition())\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n                attribute_definitions=gsi_data_attr_def\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global_index to ' \\\n                  'create_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n"}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "    def full_path(self):\n        qs = self.query_string\n"}
{"namespace": "boto.utils.retry_url", "completion": "    for i in range(0, num_retries):\n        try:\n            proxy_handler = urllib.request.ProxyHandler({})\n            opener = urllib.request.build_opener(proxy_handler)\n            req = urllib.request.Request(url)\n            r = opener.open(req, timeout=timeout)\n            result = r.read()\n\n            if(not isinstance(result, six.string_types) and\n                    hasattr(result, 'decode')):\n                result = result.decode('utf-8')\n\n            return result\n        except urllib.error.HTTPError as e:\n            code = e.getcode()\n            if code == 404 and not retry_on_404:\n                return ''\n        except Exception as e:\n            boto.log.exception('Caught exception reading instance data')\n        # If not on the last iteration of the loop then sleep.\n        if i + 1 != num_retries:\n            boto.log.debug('Sleeping before retrying')\n            time.sleep(min(2 ** i,\n                           boto.config.get('Boto', 'max_retry_delay', 60)))\n    boto.log.error('Unable to read instance data, giving up')\n    return ''\n"}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    ret = []\n\n    for lit, fname, fspec, conv in Formatter().parse(fstr):\n        if fname is None:\n            ret.append((lit, None))\n            continue\n        field_str = construct_format_field_str(fname, fspec, conv)\n        ret.append((lit, field_str))\n    return ret\n"}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        result = decode_ssh_public_key(self.public_data)\n        result.set_comment(self._comment)\n        result.set_filename(self._filename)\n        return result\n"}
{"namespace": "faker.utils.text.slugify", "completion": "    pattern: Pattern = _re_pattern_allow_dots if allow_dots else _re_pattern\n\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize(\"NFKC\", value)\n        value = pattern.sub(\"\", value).strip().lower()\n        return _re_spaces.sub(\"-\", value)\n    value = unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    value = pattern.sub(\"\", value).strip().lower()\n    return _re_spaces.sub(\"-\", value)\n"}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        final_data = {}\n        fields = set()\n        alterations = self._determine_alterations()\n\n        for key, value in alterations['adds'].items():\n            final_data[key] = {\n                'Action': 'PUT',\n                'Value': self._dynamizer.encode(self._data[key])\n            }\n            fields.add(key)\n\n        for key, value in alterations['changes'].items():\n            final_data[key] = {\n                'Action': 'PUT',\n                'Value': self._dynamizer.encode(self._data[key])\n            }\n            fields.add(key)\n\n        for key in alterations['deletes']:\n            final_data[key] = {\n                'Action': 'DELETE',\n            }\n            fields.add(key)\n\n        return final_data, fields\n"}
{"namespace": "falcon.util.misc.secure_filename", "completion": "    if not filename:\n        raise ValueError('filename may not be an empty string')\n\n    filename = unicodedata.normalize('NFKD', filename)\n    if filename.startswith('.'):\n        filename = filename.replace('.', '_', 1)\n    return _UNSAFE_CHARS.sub('_', filename)\n"}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    if len(password) == 1:\n        return []\n\n    def update(i, j, delta):\n        if j - i > 1 or (delta and abs(delta) == 1):\n            if 0 < abs(delta) <= MAX_DELTA:\n                token = password[i:j + 1]\n                if re.compile(r'^[a-z]+$').match(token):\n                    sequence_name = 'lower'\n                    sequence_space = 26\n                elif re.compile(r'^[A-Z]+$').match(token):\n                    sequence_name = 'upper'\n                    sequence_space = 26\n                elif re.compile(r'^\\d+$').match(token):\n                    sequence_name = 'digits'\n                    sequence_space = 10\n                else:\n                    sequence_name = 'unicode'\n                    sequence_space = 26\n                result.append({\n                    'pattern': 'sequence',\n                    'i': i,\n                    'j': j,\n                    'token': password[i:j + 1],\n                    'sequence_name': sequence_name,\n                    'sequence_space': sequence_space,\n                    'ascending': delta > 0\n                })\n\n    result = []\n    i = 0\n    last_delta = None\n\n    for k in range(1, len(password)):\n        delta = ord(password[k]) - ord(password[k - 1])\n        if last_delta is None:\n            last_delta = delta\n        if delta == last_delta:\n            continue\n        j = k - 1\n        update(i, j, last_delta)\n        i = j\n        last_delta = delta\n    update(i, len(password) - 1, last_delta)\n\n    return result\n"}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "    from ydata_profiling.model.pandas.discretize_pandas import DiscretizationType\n    from ydata_profiling.model.pandas.discretize_pandas import Discretizer\n    threshold = config.categorical_maximum_correlation_distinct\n    numerical_columns = [\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Numeric\", \"TimeSeries\"} and value[\"n_distinct\"] > 1\n    ]\n    categorical_columns = [\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n        and 1 < value[\"n_distinct\"] <= threshold\n    ]\n\n    if len(numerical_columns + categorical_columns) <= 1:\n        return None\n\n    df_discretized = Discretizer(\n        DiscretizationType.UNIFORM, n_bins=config.correlations[\"auto\"].n_bins\n    ).discretize_dataframe(df)\n    columns_tested = numerical_columns + categorical_columns\n    correlation_matrix = pd.DataFrame(\n        np.ones((len(columns_tested), len(columns_tested))),\n        index=columns_tested,\n        columns=columns_tested,\n    )\n    for col_1_name, col_2_name in itertools.combinations(columns_tested, 2):\n\n        method = (\n            _pairwise_spearman\n            if col_1_name and col_2_name not in categorical_columns\n            else _pairwise_cramers\n        )\n\n        def f(col_name: str, method: Callable) -> pd.Series:\n            return (\n                df_discretized\n                if col_name in numerical_columns and method is _pairwise_cramers\n                else df\n            )\n\n        score = method(\n            f(col_1_name, method)[col_1_name], f(col_2_name, method)[col_2_name]\n        )\n        (\n            correlation_matrix.loc[col_1_name, col_2_name],\n            correlation_matrix.loc[col_2_name, col_1_name],\n        ) = (score, score)\n\n    return correlation_matrix\n"}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    from ..util.sqla_compat import create_mock_engine\n    buf = []\n\n    def dump(sql, *multiparams, **params):\n        buf.append(str(sql.compile(dialect=engine.dialect)))\n\n    engine = create_mock_engine(dialect, dump)\n    return engine, buf\n"}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        cn = self.__class__.__name__\n        if self.headers:\n            return '%s(headers=%r, data=%r)' % (cn, self.headers, self._data)\n        else:\n            return '%s(%r)' % (cn, self._data)\n"}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        name = name or receiver.__name__\n        key = (name, sender)\n        if key not in self._receivers:\n            self._receivers.add(key)\n            self._receiver_list.append((name, receiver, sender))\n        else:\n            raise ValueError('receiver named %s (for sender=%s) already '\n                             'connected' % (name, sender or 'any'))\n"}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.cloudhsm.layer1 import CloudHSMConnection\n    return connect('cloudhsm', region_name, connection_cls=CloudHSMConnection,\n                   **kw_params)\n"}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        self.update_keywords(text)\n        self.update_names(text)\n"}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    from googleapiclient import errors\n    headers = _upper_header_keys(headers)\n    channel_id = headers[X_GOOG_CHANNEL_ID]\n    if channel.id != channel_id:\n        raise errors.InvalidNotificationError(\n            \"Channel id mismatch: %s != %s\" % (channel.id, channel_id)\n        )\n    else:\n        message_number = int(headers[X_GOOG_MESSAGE_NUMBER])\n        state = headers[X_GOOG_RESOURCE_STATE]\n        resource_uri = headers[X_GOOG_RESOURCE_URI]\n        resource_id = headers[X_GOOG_RESOURCE_ID]\n        return Notification(message_number, state, resource_uri, resource_id)\n"}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir:\n            return (\n                os.path.relpath(self.filename, os.path.realpath(base_dir)),\n                self.digest,\n            )\n        else:\n            return self.filename, self.digest\n"}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.beanstalk.layer1 import Layer1\n    return connect('elasticbeanstalk', region_name, connection_cls=Layer1,\n                   **kw_params)\n"}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    # detect the constraint being part of\n    # a parent type which is probably in the Table already.\n    # ideally SQLAlchemy would give us more of a first class\n    # way to detect this.\n    if (\n        constraint._create_rule  # type:ignore[attr-defined]\n        and hasattr(\n            constraint._create_rule, \"target\"  # type:ignore[attr-defined]\n        )\n        and isinstance(\n            constraint._create_rule.target,  # type:ignore[attr-defined]\n            sqltypes.TypeEngine,\n        )\n    ):\n        return None\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n    return \"%(prefix)sCheckConstraint(%(sqltext)s%(opts)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"opts\": \", \" + (\", \".join(\"%s=%s\" % (k, v) for k, v in opts))\n        if opts\n        else \"\",\n        \"sqltext\": _render_potential_expr(\n            constraint.sqltext, autogen_context, wrap_in_text=False\n        ),\n    }\n"}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n\n        dt_object = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        return datetime.date(dt_object.year, dt_object.month, dt_object.day)\n"}
{"namespace": "sacred.utils.rel_path", "completion": "    if base == path:\n        return \"\"\n    assert is_prefix(base, path), \"{} not a prefix of {}\".format(base, path)\n    return path[len(base) :].strip(\".\")\n"}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            o = override(resource_name)\n            if o is not None:\n                yield o\n"}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self.redirection_file_path is None and self._file_mode is None:\n            print(text)\n        elif self._file_mode == 'quiet':\n            return\n        else:\n            if self.buffered_text is None:\n                self.buffered_text = str(text)\n            else:\n                self.buffered_text += '\\n' + str(text)\n"}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        socket_path = path.get_unix_socket_path(value)\n        if socket_path is not None:\n            path_str = Path(not self._required).deserialize(socket_path)\n            return f\"unix:{path_str}\"\n\n        try:\n            socket.getaddrinfo(value, None)\n        except OSError:\n            raise ValueError(\"must be a resolveable hostname or valid IP\")\n\n        return value\n"}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if batch.shape:\n            if not (batch.flags[\"C_CONTIGUOUS\"] or batch.flags[\"F_CONTIGUOUS\"]):\n                # TODO: use fortan contiguous if it's faster\n                batch = np.ascontiguousarray(batch)\n\n            bs: bytes\n            concat_buffer_bs: bytes\n            indices: list[int]\n            bs, concat_buffer_bs, indices = pep574_dumps(batch)\n            bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n            return cls.create_payload(\n                concat_buffer_bs,\n                batch.shape[batch_dim],\n                {\n                    \"format\": \"pickle5\",\n                    \"pickle_bytes_str\": bs_str,\n                    \"indices\": indices,\n                },\n            )\n\n        return cls.create_payload(\n            pickle.dumps(batch),\n            batch.shape[batch_dim],\n            {\"format\": \"default\"},\n        )\n"}
{"namespace": "boto.opsworks.regions", "completion": "    from boto.regioninfo import get_regions\n    from boto.opsworks.layer1 import OpsWorksConnection\n    return get_regions('opsworks', connection_cls=OpsWorksConnection)\n"}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return (\n            f\"{self.__class__.__name__}(error_type={self.error_type!r},\"\n            f\" message={self.message!r}, field={self.field!r})\"\n        )\n"}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            raise KeyError\n        return result[0]\n"}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        try:\n            data = self._read_page(0)\n        except ReachedEndOfFile:\n            raise ValueError('Metadata not set yet')\n        end_root_node_page = PAGE_REFERENCE_BYTES\n        root_node_page = int.from_bytes(\n            data[0:end_root_node_page], ENDIAN\n        )\n        end_page_size = end_root_node_page + OTHERS_BYTES\n        page_size = int.from_bytes(\n            data[end_root_node_page:end_page_size], ENDIAN\n        )\n        end_order = end_page_size + OTHERS_BYTES\n        order = int.from_bytes(\n            data[end_page_size:end_order], ENDIAN\n        )\n        end_key_size = end_order + OTHERS_BYTES\n        key_size = int.from_bytes(\n            data[end_order:end_key_size], ENDIAN\n        )\n        end_value_size = end_key_size + OTHERS_BYTES\n        value_size = int.from_bytes(\n            data[end_key_size:end_value_size], ENDIAN\n        )\n        self._tree_conf = TreeConf(\n            page_size, order, key_size, value_size, self._tree_conf.serializer\n        )\n        return root_node_page, self._tree_conf\n"}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    if len(likelihoods) == 0:\n        return [], np.nan\n    min_lik = min(likelihoods)\n    ind = likelihoods.index(min_lik)\n    return session[ind : ind + window_len], min_lik  # noqa: E203\n"}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        minutes = abs(self.minutes)\n        return \"{0}{1:02}{2:02}\".format(\n            \"-\" if self.minutes < 0 else \"+\", minutes // 60, minutes % 60\n        )\n"}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        self._command_and_check(\"select\", self._normalise_folder(folder), readonly)\n        return self._process_select_response(self._imap.untagged_responses)\n"}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            list(table.c) + list(table.constraints),  # type:ignore[arg-type]\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            # given a Table() object, this Table will contain full Index()\n            # and UniqueConstraint objects already constructed in response to\n            # each unique=True / index=True flag on a Column.  Carry this\n            # state along so that when we re-convert back into a Table, we\n            # skip unique=True/index=True so that these constraints are\n            # not doubled up. see #844 #848\n            _constraints_included=True,\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n            **table.kwargs,\n        )\n"}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        chords = []\n        for x in self.bar:\n            chords.append([x[0], x[2].determine(shorthand)])\n        return chords\n"}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  SetupFlags()\n  app.run(main)\n"}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    data = {\n        'error': 'Server Error (500)'\n    }\n    return JsonResponse(data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n"}
{"namespace": "boto.swf.connect_to_region", "completion": "    from boto.regioninfo import connect\n    return connect('swf', region_name,\n                   connection_cls=boto.swf.layer1.Layer1, **kw_params)\n"}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        current_paragraph = []\n        paragraphs = []\n        for line in self._text.splitlines():\n            line = line.strip()\n            if line.isupper():\n                heading = Sentence(line, self._tokenizer, is_heading=True)\n                current_paragraph.append(heading)\n            elif not line and current_paragraph:\n                sentences = self._to_sentences(current_paragraph)\n                paragraphs.append(Paragraph(sentences))\n                current_paragraph = []\n            elif line:\n                current_paragraph.append(line)\n\n        sentences = self._to_sentences(current_paragraph)\n        paragraphs.append(Paragraph(sentences))\n\n        return ObjectDocumentModel(paragraphs)\n"}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.scheme + '://' + self.netloc + self.relative_uri\n\n            self._cached_uri = value\n\n        return self._cached_uri\n"}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        parse_s3_uri(uri)\n        return True\n    except ValueError:\n        return False\n"}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    from pycoin.intbytes import iterbytes\n    return \"%d.%d.%d.%d\" % tuple(iterbytes(ip_bin[-4:]))\n"}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        (min, max) = (100000, -1)\n        for cont in self.bar:\n            for note in cont[2]:\n                if int(note) < int(min):\n                    min = note\n                elif int(note) > int(max):\n                    max = note\n        return (min, max)\n"}
{"namespace": "datasette.utils.parse_metadata", "completion": "    try:\n        return json.loads(content)\n    except json.JSONDecodeError:\n        try:\n            return yaml.safe_load(content)\n        except yaml.YAMLError:\n            raise BadMetadataError(\"Metadata is not valid JSON or YAML\")\n"}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        return iter_options(\n            self.grouped_choices,\n            cutoff=self.html_cutoff,\n            cutoff_text=self.html_cutoff_text\n        )\n"}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, six.string_types):\n        json_dict = json.loads(stream)\n    else:\n        json_dict = json.load(stream)\n\n    return PolicyDocument(_parse_statements(json_dict['Statement']), Version=json_dict['Version'])\n"}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as f:\n        return handler.load(f)\n"}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        elif value.lower() in self.false_values:\n            return False\n        raise ValueError(f\"invalid value for boolean: {value!r}\")\n"}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    expiration_ms = 0\n    if expiration:\n        delta = expiration - EPOCH\n        expiration_ms = (\n            delta.microseconds / 1000 + (delta.seconds + delta.days * 24 * 3600) * 1000\n        )\n        if expiration_ms < 0:\n            expiration_ms = 0\n\n    return Channel(\n        \"web_hook\",\n        str(uuid.uuid4()),\n        token,\n        url,\n        expiration=expiration_ms,\n        params=params,\n    )\n"}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    result = dict(**b)\n    for key, value in a.items():\n        if isinstance(value, collections_abc.Mapping):\n            value = dict_merge(value, result.setdefault(key, {}))\n        result[key] = value\n    return result\n"}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            return self.model.timestamp()\n        except storage_exceptions.ReadonlyError as e:\n            # If the instance is configured to be readonly, and if the\n            # resource is empty, the backend will try to bump the timestamp.\n            # It fails if the configured db user has not write privileges.\n            logger.exception(e)\n            error_msg = (\n                \"Resource timestamp cannot be written. \"\n                \"Plural endpoint must be hit at least once from a \"\n                \"writable instance.\"\n            )\n            raise http_error(HTTPServiceUnavailable(), errno=ERRORS.BACKEND, message=error_msg)\n"}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if type(o) == dict:\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif type(o) == list:\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif type(o) == tuple:\n        return tuple(map(make_read_only, o))\n    else:\n        return o\n"}
{"namespace": "gunicorn.config.Config.address", "completion": "        s = self.settings['bind'].get()\n        return [util.parse_address(util.bytes_to_str(bind)) for bind in s]\n"}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self\n"}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])\n"}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        from diffprivlib.utils import PrivacyLeakWarning\n        self._validate_params()\n        self.accountant.check(self.epsilon, 0)\n\n        if sample_weight is not None:\n            self._warn_unused_args(\"sample_weight\")\n\n        random_state = check_random_state(self.random_state)\n\n        X, y = self._validate_data(X, y, accept_sparse=False, y_numeric=True, multi_output=True)\n\n        if self.bounds_X is None or self.bounds_y is None:\n            warnings.warn(\n                \"Bounds parameters haven't been specified, so falling back to determining bounds from the \"\n                \"data.\\n\"\n                \"This will result in additional privacy leakage. To ensure differential privacy with no \"\n                \"additional privacy loss, specify `bounds_X` and `bounds_y`.\",\n                PrivacyLeakWarning)\n\n            if self.bounds_X is None:\n                self.bounds_X = (np.min(X, axis=0), np.max(X, axis=0))\n            if self.bounds_y is None:\n                self.bounds_y = (np.min(y, axis=0), np.max(y, axis=0))\n\n        # pylint: disable=no-member\n        self.bounds_X = self._check_bounds(self.bounds_X, X.shape[1])\n        self.bounds_y = self._check_bounds(self.bounds_y, y.shape[1] if y.ndim > 1 else 1)\n\n        n_features = X.shape[1]\n        n_targets = y.shape[1] if y.ndim > 1 else 1\n        epsilon_intercept_scale = 1 / (n_features + 1) if self.fit_intercept else 0\n\n        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n            X, y, fit_intercept=self.fit_intercept, bounds_X=self.bounds_X, bounds_y=self.bounds_y,\n            epsilon=self.epsilon * epsilon_intercept_scale, copy=self.copy_X, random_state=random_state)\n\n        bounds_X = (self.bounds_X[0] - X_offset, self.bounds_X[1] - X_offset)\n        bounds_y = (self.bounds_y[0] - y_offset, self.bounds_y[1] - y_offset)\n\n        objs, obj_coefs = _construct_regression_obj(\n            X, y, bounds_X, bounds_y, epsilon=self.epsilon * (1 - epsilon_intercept_scale), alpha=0,\n            random_state=random_state)\n        coef = np.zeros((n_features, n_targets))\n\n        for i, obj in enumerate(objs):\n            opt_result = minimize(obj, np.zeros(n_features), jac=True)\n            coef[:, i] = opt_result.x\n\n        self.coef_ = coef.T\n        self._obj_coefs = obj_coefs\n\n        if y.ndim == 1:\n            self.coef_ = np.ravel(self.coef_)\n        self._set_intercept(X_offset, y_offset, X_scale)\n\n        self.accountant.spend(self.epsilon, 0)\n\n        return self\n"}
{"namespace": "boto.elasticache.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.elasticache.layer1 import ElastiCacheConnection\n    return connect('elasticache', region_name,\n                   connection_cls=ElastiCacheConnection, **kw_params)\n"}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    return ServeInfo(\n        serve_id=secrets.token_urlsafe(32),\n        serve_started_timestamp=datetime.now(timezone.utc),\n    )\n"}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)\n"}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    filename = Path(os.path.abspath(os.path.realpath(filename)))\n    experiment_path = Path(os.path.abspath(os.path.realpath(experiment_path)))\n    if experiment_path not in filename.parents:\n        return False\n    rel_path = filename.relative_to(experiment_path)\n    path_parts = convert_path_to_module_parts(rel_path)\n\n    mod_parts = modname.split(\".\")\n    if path_parts == mod_parts:\n        return True\n    if len(path_parts) > len(mod_parts):\n        return False\n    abs_path_parts = convert_path_to_module_parts(filename)\n    return all([p == m for p, m in zip(reversed(abs_path_parts), reversed(mod_parts))])\n"}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        try:\n            packet = SSHPacket(sig)\n            sig_algorithm = packet.get_string()\n\n            if sig_algorithm not in self.all_sig_algorithms:\n                return False\n\n            return self.verify_ssh(data, sig_algorithm, packet)\n        except PacketDecodeError:\n            return False\n"}
{"namespace": "fs._url_tools.url_quote", "completion": "    if _WINDOWS_PLATFORM and _has_drive_letter(path_snippet):\n        drive_letter, path = path_snippet.split(\":\", 1)\n        if six.PY2:\n            path = path.encode(\"utf-8\")\n        path = six.moves.urllib.request.pathname2url(path)\n        path_snippet = \"{}:{}\".format(drive_letter, path)\n    else:\n        if six.PY2:\n            path_snippet = path_snippet.encode(\"utf-8\")\n        path_snippet = six.moves.urllib.request.pathname2url(path_snippet)\n    return path_snippet\n"}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    res: dict[str, t.Any] = {}\n    for resource_kind, resource in _RESOURCE_REGISTRY.items():\n        res[resource_kind] = resource.from_system()\n    return res\n"}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        streams_list = self.view.pinned_streams + self.view.unpinned_streams\n        streams = [stream[\"name\"] for stream in streams_list]\n\n        # match_streams takes stream names and typeaheads,\n        # but we don't have typeaheads here.\n        # FIXME: Refactor match_stream\n        stream_data = list(zip(streams, streams))\n        matched_streams = match_stream(stream_data, text, self.view.pinned_streams)\n\n        # matched_streams[0] and matched_streams[1] contains the same data.\n        return self._process_typeaheads(matched_streams[0], state, matched_streams[1])\n"}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        ChangeLog = self.model\n        if create_table:\n            ChangeLog.create_table()\n\n        actions = list(zip((insert, update, delete), self._actions))\n        if drop:\n            for _, action in actions:\n                self.db.execute_sql(self.drop_trigger_sql(model, action))\n\n        for enabled, action in actions:\n            if enabled:\n                sql = self.trigger_sql(model, action, skip_fields)\n                self.db.execute_sql(sql)\n"}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        return self.page(\n            title=title,\n            ns=ns,\n            unquote=unquote,\n        )\n"}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if not isinstance(spec, (int, float, str)):\n            raise TypeError(\"cpu must be int, float or str\")\n\n        if isinstance(spec, (int, float)):\n            return float(spec)\n\n        milli_match = re.match(\"([0-9]+)m\", spec)\n        if milli_match:\n            return float(milli_match[1]) / 1000.0\n\n        try:\n            return float(spec)\n        except ValueError:\n            raise BentoMLConfigException(f\"Invalid CPU resource limit '{spec}'. \")\n"}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str\n"}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or not value.strip():\n            return None\n\n        result = cls.EMAIL_REGEXP.findall(value)\n        if not result:\n            value_error(value, cls)\n        else:\n            return result[0]\n"}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        from mopidy.config import types\n        errors = {}\n        result = {}\n\n        for key, value in values.items():\n            try:\n                result[key] = self[key].deserialize(value)\n            except KeyError:  # not in our schema\n                errors[key] = \"unknown config key.\"\n                suggestion = _did_you_mean(key, self.keys())\n                if suggestion:\n                    errors[key] += f\" Did you mean {suggestion!r}?\"\n            except ValueError as e:  # deserialization failed\n                result[key] = None\n                errors[key] = str(e)\n\n        for key in self.keys():\n            if isinstance(self[key], types.Deprecated):\n                result.pop(key, None)\n            elif key not in result and key not in errors:\n                result[key] = None\n                errors[key] = \"config key not found.\"\n\n        return result, errors\n"}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        data = {}\n\n        for (key, value) in self.param_defaults.items():\n\n            # If the value is a list, we need to create a list to hold the\n            # dicts created by an object supporting the AsDict() method,\n            # i.e., if it inherits from TwitterModel. If the item in the list\n            # doesn't support the AsDict() method, then we assign the value\n            # directly. An example being a list of Media objects contained\n            # within a Status object.\n            if isinstance(getattr(self, key, None), (list, tuple, set)):\n                data[key] = list()\n                for subobj in getattr(self, key, None):\n                    if getattr(subobj, 'AsDict', None):\n                        data[key].append(subobj.AsDict())\n                    else:\n                        data[key].append(subobj)\n\n            # Not a list, *but still a subclass of TwitterModel* and\n            # and we can assign the data[key] directly with the AsDict()\n            # method of the object. An example being a Status object contained\n            # within a User object.\n            elif getattr(getattr(self, key, None), 'AsDict', None):\n                data[key] = getattr(self, key).AsDict()\n\n            # If the value doesn't have an AsDict() method, i.e., it's not\n            # something that subclasses TwitterModel, then we can use direct\n            # assigment.\n            elif getattr(self, key, None):\n                data[key] = getattr(self, key, None)\n        return data\n"}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        result = self._store.get(request, self.NO_VALUE)\n        if result is self.NO_VALUE:\n            if creator is None:\n                creator = self._creator\n                if creator is None:\n                    raise ValueError(\n                        'no creator function has been registered with the '\n                        'cache or supplied to \"get_or_create\"'\n                    )\n            result = creator(request)\n            self.set(request, result)\n        return result\n"}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        ret = dict(reversed(list(zip(reversed(self.args),\n                                     reversed(self.defaults or [])))))\n        kwonlydefaults = getattr(self, 'kwonlydefaults', None)\n        if kwonlydefaults:\n            ret.update(kwonlydefaults)\n        return ret\n"}
{"namespace": "mrjob.step._Step.description", "completion": "        result = dict(\n            (k, getattr(self, k))\n            for k in self._STEP_ATTRS\n            if k not in self._HIDDEN_ATTRS\n        )\n        result['type'] = self._STEP_TYPE\n\n        return result\n"}
{"namespace": "boltons.iterutils.remap", "completion": "    if not callable(visit):\n        raise TypeError('visit expected callable, not: %r' % visit)\n    if not callable(enter):\n        raise TypeError('enter expected callable, not: %r' % enter)\n    if not callable(exit):\n        raise TypeError('exit expected callable, not: %r' % exit)\n    reraise_visit = kwargs.pop('reraise_visit', True)\n    if kwargs:\n        raise TypeError('unexpected keyword arguments: %r' % kwargs.keys())\n\n    path, registry, stack = (), {}, [(None, root)]\n    new_items_stack = []\n    while stack:\n        key, value = stack.pop()\n        id_value = id(value)\n        if key is _REMAP_EXIT:\n            key, new_parent, old_parent = value\n            id_value = id(old_parent)\n            path, new_items = new_items_stack.pop()\n            value = exit(path, key, old_parent, new_parent, new_items)\n            registry[id_value] = value\n            if not new_items_stack:\n                continue\n        elif id_value in registry:\n            value = registry[id_value]\n        else:\n            res = enter(path, key, value)\n            try:\n                new_parent, new_items = res\n            except TypeError:\n                # TODO: handle False?\n                raise TypeError('enter should return a tuple of (new_parent,'\n                                ' items_iterator), not: %r' % res)\n            if new_items is not False:\n                # traverse unless False is explicitly passed\n                registry[id_value] = new_parent\n                new_items_stack.append((path, []))\n                if value is not root:\n                    path += (key,)\n                stack.append((_REMAP_EXIT, (key, new_parent, value)))\n                if new_items:\n                    stack.extend(reversed(list(new_items)))\n                continue\n        if visit is _orig_default_visit:\n            # avoid function call overhead by inlining identity operation\n            visited_item = (key, value)\n        else:\n            try:\n                visited_item = visit(path, key, value)\n            except Exception:\n                if reraise_visit:\n                    raise\n                visited_item = True\n            if visited_item is False:\n                continue  # drop\n            elif visited_item is True:\n                visited_item = (key, value)\n            # TODO: typecheck?\n            #    raise TypeError('expected (key, value) from visit(),'\n            #                    ' not: %r' % visited_item)\n        try:\n            new_items_stack[-1][1].append(visited_item)\n        except IndexError:\n            raise TypeError('expected remappable root, not: %r' % root)\n    return value\n"}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        batch_data = {\n            self.table.table_name: [\n                # We'll insert data here shortly.\n            ],\n        }\n\n        for put in self._to_put:\n            item = Item(self.table, data=put)\n            batch_data[self.table.table_name].append({\n                'PutRequest': {\n                    'Item': item.prepare_full(),\n                }\n            })\n\n        for delete in self._to_delete:\n            batch_data[self.table.table_name].append({\n                'DeleteRequest': {\n                    'Key': self.table._encode_keys(delete),\n                }\n            })\n\n        resp = self.table.connection.batch_write_item(batch_data)\n        self.handle_unprocessed(resp)\n\n        self._to_put = []\n        self._to_delete = []\n        return True\n"}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api >= ARMEABI_MAX_TARGET_API and arch == 'armeabi':\n        raise BuildInterruptingException(\n            UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(\n                req_ndk_api=api, max_ndk_api=ARMEABI_MAX_TARGET_API\n            ),\n            instructions='You probably want to build with --arch=armeabi-v7a instead')\n\n    if api < MIN_TARGET_API:\n        warning('Target API {} < {}'.format(api, MIN_TARGET_API))\n        warning(OLD_API_MESSAGE)\n"}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    hrpgot, data, spec = bech32_decode(addr)\n    if hrpgot != hrp:\n        return (None, None)\n    decoded = convertbits(data[1:], 5, 8, False)\n    if decoded is None or len(decoded) < 2 or len(decoded) > 40:\n        return (None, None)\n    if data[0] > 16:\n        return (None, None)\n    if data[0] == 0 and len(decoded) != 20 and len(decoded) != 32:\n        return (None, None)\n    if data[0] == 0 and spec != Encoding.BECH32 or data[0] != 0 and spec != Encoding.BECH32M:\n        return (None, None)\n    return (data[0], decoded)\n"}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"Installer for homebrew is not yet supported on macOS,\"\n            \"the nice news is that the installation process is easy!\"\n            \"See: https://brew.sh for further instructions.\"\n        )\n"}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.session.get(self.key, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token\n"}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        error_msg = \"Please install Kinto with monitoring dependencies (e.g. statsd package)\"\n        raise ConfigurationError(error_msg)\n\n    settings = config.get_settings()\n    uri = settings[\"statsd_url\"]\n    uri = urlparse(uri)\n\n    if settings[\"project_name\"] != \"\":\n        prefix = settings[\"project_name\"]\n    else:\n        prefix = settings[\"statsd_prefix\"]\n\n    return Client(uri.hostname, uri.port, prefix)\n"}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n        return False\n    elif 'NotResource' in statement:\n        result = True\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                result = False\n                break\n        return result\n    else:\n        return True\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_end_token is True\"\n            )\n\n    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_token is True\"\n            )\n\n    w_len = len(window)\n    if w_len == 0:\n        return np.nan\n    prob: float = 1\n\n    cur_cmd = window[0].name\n    params = window[0].params\n    param_cond_prob = compute_prob_setofparams_given_cmd(\n        cmd=cur_cmd,\n        params=params,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        use_geo_mean=True,\n    )\n\n    if use_start_token:\n        prob *= trans_probs[start_token][cur_cmd] * param_cond_prob\n    else:\n        prob *= prior_probs[cur_cmd] * param_cond_prob\n\n    for i in range(1, w_len):\n        prev, cur = window[i - 1], window[i]\n        prev_cmd, cur_cmd = prev.name, cur.name\n        cur_par = cur.params\n        prob *= trans_probs[prev_cmd][cur_cmd]\n        param_cond_prob = compute_prob_setofparams_given_cmd(\n            cmd=cur_cmd,\n            params=cur_par,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_geo_mean=True,\n        )\n        prob *= param_cond_prob\n\n    if use_end_token:\n        prob *= trans_probs[cur_cmd][end_token]\n\n    return prob\n"}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        # Seek to position from the start of the file\n        if mode == os.SEEK_SET:\n            self.buffer.seek(0)\n            self._traverse_codepoints(0, pos)\n            self._tell = pos\n        # Seek to new position relative to current position\n        elif mode == os.SEEK_CUR:\n            start_pos = self.tell()\n            self._traverse_codepoints(self.tell(), pos)\n            self._tell = start_pos + pos\n        elif mode == os.SEEK_END:\n            self.buffer.seek(0)\n            dest_position = self.len - pos\n            self._traverse_codepoints(0, dest_position)\n            self._tell = dest_position\n        else:\n            raise ValueError(\n                \"Invalid whence ({0}, should be 0, 1, or 2)\".format(mode)\n            )\n        return self.tell()\n"}
{"namespace": "mopidy.config.format_initial", "completion": "    from mopidy.internal import versioning\n    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    defaults.extend(d.extension.get_default_config() for d in extensions_data)\n    raw_config = _load([], defaults, [])\n\n    schemas = _schemas[:]\n    schemas.extend(d.extension.get_config_schema() for d in extensions_data)\n\n    config, errors = _validate(raw_config, schemas)\n\n    versions = [f\"Mopidy {versioning.get_version()}\"]\n    extensions_data = sorted(\n        extensions_data, key=lambda d: d.extension.dist_name\n    )\n    for data in extensions_data:\n        versions.append(f\"{data.extension.dist_name} {data.extension.version}\")\n\n    header = _INITIAL_HELP.strip().format(versions=\"\\n#   \".join(versions))\n    formatted_config = _format(\n        config=config, comments={}, schemas=schemas, display=False, disable=True\n    )\n    return header + \"\\n\\n\" + formatted_config\n"}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        from sslyze.plugins.certificate_info._certificate_utils import get_public_key_sha256\n        has_whitelisted_cert = False\n        has_blacklisted_cert = False\n\n        # Is there a Symantec root certificate in the chain?\n        for certificate in verified_certificate_chain:\n            key_hash = binascii.hexlify(get_public_key_sha256(certificate)).decode(\"ascii\")\n            if key_hash in cls._CA_KEYS_BLACKLIST:\n                has_blacklisted_cert = True\n            if key_hash in cls._CA_KEYS_WHITELIST:\n                has_whitelisted_cert = True\n\n        distrust_enum = None\n        if has_blacklisted_cert and not has_whitelisted_cert:\n            leaf_cert = verified_certificate_chain[0]\n            if leaf_cert.not_valid_before < datetime(year=2016, month=6, day=1):\n                distrust_enum = SymantecDistrustTimelineEnum.MARCH_2018\n            else:\n                distrust_enum = SymantecDistrustTimelineEnum.SEPTEMBER_2018\n        return distrust_enum\n"}
{"namespace": "falcon.response.Response.get_header", "completion": "        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('Getting Set-Cookie is not currently supported.')\n\n        return self._headers.get(name, default)\n"}
{"namespace": "playhouse.db_url.connect", "completion": "    parsed = urlparse(url)\n    connect_kwargs = parseresult_to_dict(parsed, unquote_password)\n    connect_kwargs.update(connect_params)\n    database_class = schemes.get(parsed.scheme)\n\n    if database_class is None:\n        if database_class in schemes:\n            raise RuntimeError('Attempted to use \"%s\" but a required library '\n                               'could not be imported.' % parsed.scheme)\n        else:\n            raise RuntimeError('Unrecognized or unsupported scheme: \"%s\".' %\n                               parsed.scheme)\n\n    return database_class(**connect_kwargs)\n"}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse is None:\n            raise ValueError(\n                \"operation is not reversible; \"\n                \"original column is not present\"\n            )\n\n        return AddColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self._reverse.column\n        )\n"}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    year_space = max(abs(match['year'] - REFERENCE_YEAR), MIN_YEAR_SPACE)\n    guesses = year_space * 365\n    if match.get('separator', False):\n        guesses *= 4\n\n    return guesses\n"}
{"namespace": "boltons.iterutils.research", "completion": "    ret = []\n\n    if not callable(query):\n        raise TypeError('query expected callable, not: %r' % query)\n\n    def enter(path, key, value):\n        try:\n            if query(path, key, value):\n                ret.append((path + (key,), value))\n        except Exception:\n            if reraise:\n                raise\n        return default_enter(path, key, value)\n\n    remap(root, enter=enter)\n    return ret\n"}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding)\n\n        exporter = self._export_formats[format](query)\n        exporter.export(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()\n"}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    from sacred.settings import SETTINGS\n    from sacred.serializer import restore\n    try:\n        return restore(ast.literal_eval(value))\n    except (ValueError, SyntaxError):\n        if SETTINGS.COMMAND_LINE.STRICT_PARSING:\n            raise\n        # use as string if nothing else worked\n        return value\n"}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "        from chatette.units.modifiable.choice import Choice\n        self._check_information()\n        return Choice(\n            self.leading_space, self._build_modifiers_repr(),\n            self.rules\n        )\n"}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        from pyramid.events import ApplicationCreated\n        from pyramid.router import Router\n        self.commit()\n        app = Router(self.registry)\n\n        # Allow tools like \"pshell development.ini\" to find the 'last'\n        # registry configured.\n        global_registries.add(self.registry)\n\n        # Push the registry onto the stack in case any code that depends on\n        # the registry threadlocal APIs used in listeners subscribed to the\n        # IApplicationCreated event.\n        self.begin()\n        try:\n            self.registry.notify(ApplicationCreated(app))\n        finally:\n            self.end()\n\n        return app\n"}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    cmd = combine_values(*cmds)\n\n    if cmd is None:\n        return None\n    elif isinstance(cmd, string_types):\n        return shlex_split(cmd)\n    else:\n        return list(cmd)\n"}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n    else:\n        return update_query_params(url, {name: value})\n"}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "        read = self.output_protocol().read\n\n        for line in to_lines(chunks):\n            yield read(line)\n"}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    from zulipterminal.config.color import term16\n    pygments = theme_meta[\"pygments\"]\n    pygments_styles = pygments[\"styles\"]\n    pygments_bg = pygments[\"background\"]\n    pygments_overrides = pygments[\"overrides\"]\n\n    term16_styles = term16.styles\n    term16_bg = term16.background_color\n\n    for token, css_class in STANDARD_TYPES.items():\n        if css_class in pygments_overrides:\n            pygments_styles[token] = pygments_overrides[css_class]\n\n        # Inherit parent pygments style if not defined.\n        # Eg: Use `String` if `String.Double` is not present.\n        if pygments_styles[token] == \"\":\n            try:\n                t = [k for k, v in STANDARD_TYPES.items() if v == css_class[0]]\n                pygments_styles[token] = pygments_styles[t[0]]\n            except IndexError:\n                pass\n\n        if term16_styles[token] == \"\":\n            try:\n                t = [k for k, v in STANDARD_TYPES.items() if v == css_class[0]]\n                term16_styles[token] = term16_styles[t[0]]\n            except IndexError:\n                pass\n\n        new_style = (\n            f\"pygments:{css_class}\",\n            term16_styles[token],\n            term16_bg,\n            \"bold\",  # Mono style\n            pygments_styles[token],\n            pygments_bg,\n        )\n        urwid_theme.append(new_style)\n"}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "        from ydata_profiling.report.presentation.flavours.html import templates\n        if isinstance(self.content[\"rows\"][0], list):\n            html = \"\"\n\n            kwargs = self.content.copy()\n            del kwargs[\"rows\"]\n            for idx, rows in enumerate(self.content[\"rows\"]):\n                html += templates.template(\"frequency_table.html\").render(\n                    rows=rows, idx=idx, **kwargs\n                )\n            return html\n        else:\n            return templates.template(\"frequency_table.html\").render(\n                **self.content, idx=0\n            )\n"}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    knowledge = pipe(all_known_iam_permissions(),\n                     mapz(_parse_action),\n                     groupbyz(lambda x: x.prefix))\n\n    return knowledge.get(prefix, [])\n"}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n\n    return value\n"}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    path_parts = path.split(\"/\")\n    filename, extension = path_parts[-1].split(\".\", 1)\n    file_path = \"/\".join(path_parts[:-1] + [filename])\n    v_str = re.sub(version_clean, \"_\", str(version))\n\n    return f\"{file_path}.v{v_str}m{hash_value}.{extension}\"\n"}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    combined = data + bech32_create_checksum(hrp, data, spec)\n    return hrp + '1' + ''.join([CHARSET[d] for d in combined])\n"}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        srclines = self.src.split('\\n')\n        src = '\\n'.join('    %s' % x for x in srclines)\n        return 'Line %s of file %s:\\n%s' % (self.line, self.file, src)\n"}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "    recipes_with_deps = list(recipes)\n    for entry in recipes:\n        if not isinstance(entry, (tuple, list)) or len(entry) == 1:\n            if isinstance(entry, (tuple, list)):\n                entry = entry[0]\n            try:\n                recipe = Recipe.get_recipe(entry, ctx)\n                recipes_with_deps += recipe.depends\n            except ValueError:\n                # it's a pure python package without a recipe, so we\n                # don't know the dependencies...skipping for now\n                pass\n\n    # Split up lists by available alternatives:\n    recipe_lists = [[]]\n    for recipe in recipes_with_deps:\n        if isinstance(recipe, (tuple, list)):\n            new_recipe_lists = []\n            for alternative in recipe:\n                for old_list in recipe_lists:\n                    new_list = [i for i in old_list]\n                    new_list.append(alternative)\n                    new_recipe_lists.append(new_list)\n            recipe_lists = new_recipe_lists\n        else:\n            for existing_list in recipe_lists:\n                existing_list.append(recipe)\n    return recipe_lists\n"}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0:\n            size = self.remaining\n        else:\n            size = min(size, self.remaining)\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n        return data\n"}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime) or isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, str):\n        return d\n"}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        ip = None\n        if \"{\" in server_str and \"}\" in server_str:\n            raw_target = server_str.split(\"{\")\n            raw_ip = raw_target[1]\n\n            ip = raw_ip.replace(\"}\", \"\")\n\n            # Clean the target\n            server_str = raw_target[0]\n\n        # Look for ipv6 hint in target\n        if \"[\" in server_str:\n            (host, port) = cls._parse_ipv6_server_string(server_str)\n        else:\n            # Look for ipv6 hint in the ip\n            if ip is not None and \"[\" in ip:\n                (ip, port) = cls._parse_ipv6_server_string(ip)\n\n            # Fallback to ipv4\n            (host, port) = cls._parse_ipv4_server_string(server_str)\n\n        return host, ip, port\n"}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n    cache = registry.cache\n    activation_key = cache.get(cache_key)\n    return activation_key\n"}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        expected_args = self._get_expected_args(bound)\n        self._assert_no_unexpected_args(expected_args, args)\n        self._assert_no_unexpected_kwargs(expected_args, kwargs)\n        self._assert_no_duplicate_args(expected_args, args, kwargs)\n\n        args, kwargs = self._fill_in_options(args, kwargs, options, bound)\n\n        self._assert_no_missing_args(args, kwargs, bound)\n        return args, kwargs\n"}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        schema_data = IncludeIndex.schema(self)\n        # Also the throughput.\n        schema_data.update(GlobalBaseIndexField.schema(self))\n        return schema_data\n"}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n"}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing = set()\n        for key in self.fixed:\n            if not dict.__contains__(self, key):\n                self[key] = self.fixed[key]\n                missing.add(key)\n\n            if isinstance(self[key], (DogmaticDict, DogmaticList)):\n                missing |= {key + \".\" + k for k in self[key].revelation()}\n        return missing\n"}
{"namespace": "datasette.utils.check_connection", "completion": "    tables = [\n        r[0]\n        for r in conn.execute(\n            \"select name from sqlite_master where type='table'\"\n        ).fetchall()\n    ]\n    for table in tables:\n        try:\n            conn.execute(\n                f\"PRAGMA table_info({escape_sqlite(table)});\",\n            )\n        except sqlite3.OperationalError as e:\n            if e.args[0] == \"no such module: VirtualSpatialIndex\":\n                raise SpatialiteConnectionProblem(e)\n            else:\n                raise ConnectionProblem(e)\n"}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n"}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]\n"}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    parsed = sqlparse.parse(sql)\n    if not parsed:\n        return []\n\n    # INSERT statements must stop looking for tables at the sign of first\n    # Punctuation. eg: INSERT INTO abc (col1, col2) VALUES (1, 2)\n    # abc is the table name, but if we don't stop at the first lparen, then\n    # we'll identify abc, col1 and col2 as table names.\n    insert_stmt = parsed[0].token_first().value.lower() == \"insert\"\n    stream = extract_from_part(parsed[0], stop_at_punctuation=insert_stmt)\n    return list(extract_table_identifiers(stream))\n"}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        async with httpx.AsyncClient(app=self.app) as client:\n            return await client.get(self._fix(path), **kwargs)\n"}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    from pymorphy2.utils import longest_common_substring\n    forms, tags = list(zip(*lexeme))\n\n    if len(forms) == 1:\n        stem = forms[0]\n        prefixes = ['']\n    else:\n        stem = longest_common_substring(forms)\n        prefixes = [form[:form.index(stem)] for form in forms]\n\n        # only allow prefixes from PARADIGM_PREFIXES\n        if any(pref not in paradigm_prefixes for pref in prefixes):\n            # With right PARADIGM_PREFIXES empty stem is fine;\n            # os.path.commonprefix doesn't return anything useful\n            # for prediction.\n            # stem = os.path.commonprefix(forms)\n            stem = \"\"\n            prefixes = [''] * len(tags)\n\n    suffixes = (\n        form[len(pref)+len(stem):]\n        for form, pref in zip(forms, prefixes)\n    )\n    return stem, tuple(zip(suffixes, tags, prefixes))\n"}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        if self.stack:\n            return self.stack.pop()\n"}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    all_column_types = {}\n    for record in records:\n        for key, value in record.items():\n            all_column_types.setdefault(key, set()).add(type(value))\n    return types_for_column_types(all_column_types)\n"}
{"namespace": "sacred.utils.get_package_version", "completion": "    version_string = importlib.import_module(name).__version__\n    return parse_version(version_string)\n"}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        if isinstance(translations, gettext.GNUTranslations):\n            self._catalog.update(translations._catalog)\n            if isinstance(translations, Translations):\n                self.files.extend(translations.files)\n\n        return self\n"}
{"namespace": "oletools.ooxml.get_type", "completion": "    parser = XmlParser(filename)\n    if parser.is_single_xml():\n        match = None\n        with uopen(filename, 'r') as handle:\n            match = re.search(OFFICE_XML_PROGID_REGEX, handle.read(1024))\n        if not match:\n            return DOCTYPE_NONE\n        prog_id = match.groups()[0]\n        if prog_id == WORD_XML_PROG_ID:\n            return DOCTYPE_WORD_XML\n        if prog_id == EXCEL_XML_PROG_ID:\n            return DOCTYPE_EXCEL_XML\n        return DOCTYPE_NONE\n\n    is_doc = False\n    is_xls = False\n    is_ppt = False\n    try:\n        for _, elem, _ in parser.iter_xml(FILE_CONTENT_TYPES):\n            logger.debug(u'  ' + debug_str(elem))\n            try:\n                content_type = elem.attrib['ContentType']\n            except KeyError:         # ContentType not an attr\n                continue\n            is_xls |= content_type.startswith(CONTENT_TYPES_EXCEL)\n            is_doc |= content_type.startswith(CONTENT_TYPES_WORD)\n            is_ppt |= content_type.startswith(CONTENT_TYPES_PPT)\n    except BadOOXML as oo_err:\n        if oo_err.more_info.startswith('invalid subfile') and \\\n                FILE_CONTENT_TYPES in oo_err.more_info:\n            # no FILE_CONTENT_TYPES in zip, so probably no ms office xml.\n            return DOCTYPE_NONE\n        raise\n\n    if is_doc and not is_xls and not is_ppt:\n        return DOCTYPE_WORD\n    if not is_doc and is_xls and not is_ppt:\n        return DOCTYPE_EXCEL\n    if not is_doc and not is_xls and is_ppt:\n        return DOCTYPE_POWERPOINT\n    if not is_doc and not is_xls and not is_ppt:\n        return DOCTYPE_NONE\n    logger.warning('Encountered contradictory content types')\n    return DOCTYPE_MIXED\n"}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        data = self.unreader.read()\n        while data:\n            self.buf.write(data)\n            if self.buf.tell() > size:\n                break\n            data = self.unreader.read()\n\n        if not data:\n            self.finished = True\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n"}
{"namespace": "flower.command.apply_options", "completion": "    from .options import DEFAULT_CONFIG_FILE\n    argv = list(filter(is_flower_option, argv))\n    # parse the command line to get --conf option\n    parse_command_line([prog_name] + argv)\n    try:\n        parse_config_file(os.path.abspath(options.conf), final=False)\n        parse_command_line([prog_name] + argv)\n    except IOError:\n        if os.path.basename(options.conf) != DEFAULT_CONFIG_FILE:\n            raise\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n\n        if self.session_type == SessionType.cmds_only:\n            rare_tuples = [\n                cmds_only.rarest_window_session(\n                    session=ses,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                    use_geo_mean=use_geo_mean,\n                )\n                for ses in self.sessions\n            ]\n        elif self.session_type == SessionType.cmds_params_only:\n            rare_tuples = [\n                cmds_params_only.rarest_window_session(\n                    session=ses,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                    use_geo_mean=use_geo_mean,\n                )\n                for ses in self.sessions\n            ]\n        else:\n            rare_tuples = [\n                cmds_params_values.rarest_window_session(\n                    session=ses,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    value_cond_param_probs=self.value_cond_param_probs,\n                    modellable_params=self.modellable_params,\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                    use_geo_mean=use_geo_mean,\n                )\n                for ses in self.sessions\n            ]\n\n        if use_geo_mean:\n            self.rare_windows_geo[window_len] = [rare[0] for rare in rare_tuples]\n            self.rare_window_likelihoods_geo[window_len] = [\n                rare[1] for rare in rare_tuples\n            ]\n        else:\n            self.rare_windows[window_len] = [rare[0] for rare in rare_tuples]\n            self.rare_window_likelihoods[window_len] = [rare[1] for rare in rare_tuples]\n"}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        user = ls[:3]\n        group = ls[3:6]\n        other = ls[6:9]\n        return cls(user=user, group=group, other=other)\n"}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "    a = util.rev_id()\n    b = util.rev_id()\n    c = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(a, \"revision a\", refresh=True, head=\"base\")\n    write_script(\n        script,\n        a,\n        \"\"\"\\\n\"Rev A\"\nrevision = '%s'\ndown_revision = None\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 1\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 1\")\n\n\"\"\"\n        % a,\n    )\n\n    script.generate_revision(b, \"revision b\", refresh=True, head=a)\n    write_script(\n        script,\n        b,\n        f\"\"\"# coding: utf-8\n\"Rev B, m\u00e9il, %3\"\nrevision = '{b}'\ndown_revision = '{a}'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 2\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 2\")\n\n\"\"\",\n        encoding=\"utf-8\",\n    )\n\n    script.generate_revision(c, \"revision c\", refresh=True, head=b)\n    write_script(\n        script,\n        c,\n        \"\"\"\\\n\"Rev C\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 3\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 3\")\n\n\"\"\"\n        % (c, b),\n    )\n    return a, b, c\n"}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        from boto.exception import JSONResponseError\n        try:\n            self.get_item(**kwargs)\n        except (JSONResponseError, exceptions.ItemNotFound):\n            return False\n\n        return True\n"}
{"namespace": "mopidy.internal.playlists.parse", "completion": "    handlers = {\n        detect_extm3u_header: parse_extm3u,\n        detect_pls_header: parse_pls,\n        detect_asx_header: parse_asx,\n        detect_xspf_header: parse_xspf,\n    }\n    for detector, parser in handlers.items():\n        if detector(data):\n            return list(parser(data))\n    return list(parse_urilist(data))  # Fallback\n"}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        if not callable(func):\n            raise TypeError('expected callable object, not %r' % (func,))\n\n        if isinstance(func, functools.partial):\n            if _IS_PY2:\n                raise ValueError('Cannot build FunctionBuilder instances from partials in python 2.')\n            kwargs = {'name': func.func.__name__,\n                      'doc': func.func.__doc__,\n                      'module': getattr(func.func, '__module__', None),  # e.g., method_descriptor\n                      'annotations': getattr(func.func, \"__annotations__\", {}),\n                      'dict': getattr(func.func, '__dict__', {})}\n        else:\n            kwargs = {'name': func.__name__,\n                      'doc': func.__doc__,\n                      'module': getattr(func, '__module__', None),  # e.g., method_descriptor\n                      'annotations': getattr(func, \"__annotations__\", {}),\n                      'dict': getattr(func, '__dict__', {})}\n\n        kwargs.update(cls._argspec_to_dict(func))\n\n        if _inspect_iscoroutinefunction(func):\n            kwargs['is_async'] = True\n\n        return cls(**kwargs)\n"}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n        content_word_tf = dict((w, f / content_words_count) for w, f in content_words_freq.items())\n        return content_word_tf\n"}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(\n                f\"Invalid negative CPU resource limit '{val}'.\"\n            )\n        if not math.isclose(val, cls.from_system()) and val > cls.from_system():\n            raise BentoMLConfigException(\n                f\"CPU resource limit {val} is greater than the system available: {cls.from_system()}\"\n            )\n"}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for cred in credentials:\n            credpath = self.make_credpath(cred[\"name\"], cred[\"login\"])\n            os.remove(credpath)\n            if not os.listdir(os.path.dirname(credpath)):\n                shutil.rmtree(os.path.dirname(credpath))\n"}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        return [self.stem_word(w) for w in words if w not in self._stop_words]\n"}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        from .utils import is_pem_format\n        from .utils import is_ssh_key\n        key_bytes = force_bytes(key)\n\n        if is_pem_format(key_bytes) or is_ssh_key(key_bytes):\n            raise InvalidKeyError(\n                \"The specified key is an asymmetric key or x509 certificate and\"\n                \" should not be used as an HMAC secret.\"\n            )\n\n        return key_bytes\n"}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        path_glob = _from_file_uri(path_glob)\n        return sum(os.path.getsize(path) for path in self.ls(path_glob))\n"}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)\n"}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        with self._wlock:\n            self._mapping.clear()\n            self._queue.clear()\n"}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            # PERF(kgriffs): Simple concatenation like this is slightly faster\n            #   than %s string formatting.\n            return 'W/\"' + self + '\"'\n\n        return '\"' + self + '\"'\n"}
{"namespace": "fs.path.isbase", "completion": "    _path1 = forcedir(abspath(path1))\n    _path2 = forcedir(abspath(path2))\n    return _path2.startswith(_path1)  # longer one is child\n"}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        base_spec = {\n            \"host\": self.request.host,\n            \"schemes\": [self.settings.get(\"http_scheme\") or \"http\"],\n            \"securityDefinitions\": self.security_definitions,\n        }\n\n        return super(OpenAPI, self).generate(swagger=base_spec)\n"}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        for key in self.params:\n            if key != 'q' and other.params.get(key, None) != self.params.get(key, None):\n                return False\n\n        if self.sub_type != '*' and other.sub_type != '*' and other.sub_type != self.sub_type:\n            return False\n\n        if self.main_type != '*' and other.main_type != '*' and other.main_type != self.main_type:\n            return False\n\n        return True\n"}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        category_name, discriminator = intr.category_name, intr.discriminator\n        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])\n"}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        if global_index_name:\n            gsi_data = [\n                {\n                    \"Delete\": {\n                        \"IndexName\": global_index_name\n                    }\n                }\n            ]\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index name to ' \\\n                  'delete_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n"}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    if range is None:\n        warnings.warn(\"Range parameter has not been specified. Falling back to taking range from the data.\\n\"\n                      \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                      \"specified independently of the data (i.e., using domain knowledge).\", PrivacyLeakWarning)\n\n    hist, bin_edges = np.histogram(sample, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n\n    for i in np.arange(dp_hist.shape[0]):\n        dp_hist[i] = dp_mech.randomise(int(hist[i]))\n\n    # dp_hist = dp_hist.astype(float, casting='safe')\n\n    accountant.spend(epsilon, 0)\n\n    if density:\n        bin_sizes = np.array(np.diff(bin_edges), float)\n        return dp_hist / bin_sizes / (dp_hist.sum() if dp_hist.sum() else 1), bin_edges\n\n    return dp_hist, bin_edges\n"}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        return [\".\" + suffix for suffix in name.split(\".\")[1:]]\n"}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param is not None:\n            params.update({\"alt\": self.alt_param})\n        astuples = []\n        for key, value in params.items():\n            if type(value) == type([]):\n                for x in value:\n                    x = x.encode(\"utf-8\")\n                    astuples.append((key, x))\n            else:\n                if isinstance(value, str) and callable(value.encode):\n                    value = value.encode(\"utf-8\")\n                astuples.append((key, value))\n        return \"?\" + urllib.parse.urlencode(astuples)\n"}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    from .packages.parseutils.utils import is_open_quote\n    if sql is not None and sql != \"\":\n        # remove comments\n        sql = sqlparse.format(sql, strip_comments=True)\n\n        # check for open comments\n        # remove all closed quotes to isolate instances of open comments\n        sql_no_quotes = re.sub(r'\".*?\"|\\'.*?\\'', '', sql)\n        is_open_comment = len(re.findall(r'\\/\\*', sql_no_quotes)) > 0\n\n        # check that 'go' is only token on newline\n        lines = sql.split('\\n')\n        lastline = lines[len(lines) - 1].lower().strip()\n        is_valid_go_on_lastline = lastline == 'go'\n\n        # check that 'go' is on last line, not in open quotes, and there's no open\n        # comment with closed comments and quotes removed.\n        # NOTE: this method fails when GO follows a closing '*/' block comment on the same line,\n        # we've taken a dependency with sqlparse\n        # (https://github.com/andialbrecht/sqlparse/issues/484)\n        return not is_open_quote(sql) and not is_open_comment and is_valid_go_on_lastline\n\n    return False\n"}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if not amt:\n            return self._joiner.join(f.read() for f in self._fileobjs)\n        parts = []\n        while amt > 0 and self._index < len(self._fileobjs):\n            parts.append(self._fileobjs[self._index].read(amt))\n            got = len(parts[-1])\n            if got < amt:\n                self._index += 1\n            amt -= got\n        return self._joiner.join(parts)\n"}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")\n\n    return path\n"}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        pos = self.buffer.tell()\n        self.buffer.seek(0)\n        total = 0\n        while True:\n            ret = self.read(READ_CHUNK_SIZE)\n            if not ret:\n                break\n            total += len(ret)\n        self.buffer.seek(pos)\n        return total\n"}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    try:\n        hook = _registry[name]\n    except KeyError as ke:\n        raise util.CommandError(\n            f\"No formatter with name '{name}' registered\"\n        ) from ke\n    else:\n        return hook(revision, options)\n"}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return self.merge(files, accumulate=True)\n"}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        lines = []\n        widths = []\n        headers = list(self.headers)\n        text_data = [[to_text(cell, maxlen=maxlen) for cell in row]\n                     for row in self._data]\n        for idx in range(self._width):\n            cur_widths = [len(cur) for cur in text_data]\n            if with_headers:\n                cur_widths.append(len(to_text(headers[idx], maxlen=maxlen)))\n            widths.append(max(cur_widths))\n        if with_headers:\n            lines.append(' | '.join([h.center(widths[i])\n                                     for i, h in enumerate(headers)]))\n            lines.append('-|-'.join(['-' * w for w in widths]))\n        for row in text_data:\n            lines.append(' | '.join([cell.center(widths[j])\n                                     for j, cell in enumerate(row)]))\n        return '\\n'.join(lines)\n"}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n\n        if path in self._path_to_name:\n            return posixpath.join(self.prefix, self._path_to_name[path])\n        else:\n            raise ValueError('%r is not a URI or a known local file' % (path,))\n"}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        kwargs = {}\n\n        if expects is not None:\n            kwargs['expected'] = expects\n\n        self.connection.put_item(self.table_name, item_data, **kwargs)\n        return True\n"}
{"namespace": "mingus.core.intervals.measure", "completion": "    res = notes.note_to_int(note2) - notes.note_to_int(note1)\n    if res < 0:\n        return 12 - res * -1\n    else:\n        return res\n"}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return connect('ec2containerservice', region_name,\n                   connection_cls=EC2ContainerServiceConnection, **kw_params)\n"}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_choice(value, self._choices)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n"}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "        self._has_batch = True\n        yield\n        self._has_batch = False\n"}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    words = FIELD_WORD_REGEX.findall(contents)\n    if not words:\n        return False\n\n    # check if first word is one of the commands on our blacklist\n    try:\n        index = FIELD_BLACKLIST_CMDS.index(words[0].lower())\n    except ValueError:    # first word is no blacklisted command\n        return False\n    logger.debug(u'trying to match \"{0}\" to blacklist command {1}'\n                 .format(contents, FIELD_BLACKLIST[index]))\n    _, nargs_required, nargs_optional, sw_with_arg, sw_solo, sw_format \\\n        = FIELD_BLACKLIST[index]\n\n    # check number of args\n    nargs = 0\n    for word in words[1:]:\n        if word[0] == '\\\\':  # note: words can never be empty, but can be '\"\"'\n            break\n        nargs += 1\n    if nargs < nargs_required:\n        logger.debug(u'too few args: found {0}, but need at least {1} in \"{2}\"'\n                     .format(nargs, nargs_required, contents))\n        return False\n    if nargs > nargs_required + nargs_optional:\n        logger.debug(u'too many args: found {0}, but need at most {1}+{2} in '\n                     u'\"{3}\"'\n                     .format(nargs, nargs_required, nargs_optional, contents))\n        return False\n\n    # check switches\n    expect_arg = False\n    arg_choices = []\n    for word in words[1+nargs:]:\n        if expect_arg:            # this is an argument for the last switch\n            if arg_choices and (word not in arg_choices):\n                logger.debug(u'Found invalid switch argument \"{0}\" in \"{1}\"'\n                             .format(word, contents))\n                return False\n            expect_arg = False\n            arg_choices = []   # in general, do not enforce choices\n            continue           # \"no further questions, your honor\"\n        elif not FIELD_SWITCH_REGEX.match(word):\n            logger.debug(u'expected switch, found \"{0}\" in \"{1}\"'\n                         .format(word, contents))\n            return False\n        # we want a switch and we got a valid one\n        switch = word[1]\n\n        if switch in sw_solo:\n            pass\n        elif switch in sw_with_arg:\n            expect_arg = True     # next word is interpreted as arg, not switch\n        elif switch == '#' and 'numeric' in sw_format:\n            expect_arg = True     # next word is numeric format\n        elif switch == '@' and 'datetime' in sw_format:\n            expect_arg = True     # next word is date/time format\n        elif switch == '*':\n            expect_arg = True     # next word is format argument\n            arg_choices += ['CHARFORMAT', 'MERGEFORMAT']  # always allowed\n            if 'string' in sw_format:\n                arg_choices += ['Caps', 'FirstCap', 'Lower', 'Upper']\n            if 'numeric' in sw_format:\n                arg_choices = []  # too many choices to list them here\n        else:\n            logger.debug(u'unexpected switch {0} in \"{1}\"'\n                         .format(switch, contents))\n            return False\n\n    # if nothing went wrong sofar, the contents seems to match the blacklist\n    return True\n"}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "    from ..encoding.hash import hash160\n    stack.append(hash160(stack.pop()))\n"}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        if get_bound_permissions:\n            bound_perms = get_bound_permissions(self._object_id_match, perm)\n        else:\n            bound_perms = [(self._object_id_match, perm)]\n        by_obj_id = self._get_accessible_objects(principals, bound_perms, with_children=False)\n        ids = by_obj_id.keys()\n        # Store for later use in ``Resource``.\n        self.shared_ids = [self._extract_object_id(id_) for id_ in ids]\n        return self.shared_ids\n"}
{"namespace": "check_dummies.find_backend", "completion": "    backends = _re_backend.findall(line)\n    if len(backends) == 0:\n        return None\n\n    return \"_and_\".join(backends)\n"}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        unique_y = np.unique(y)\n        n_total = y.shape[0]\n\n        # Use 1/3 of total epsilon budget for getting noisy class counts\n        mech = GeometricTruncated(epsilon=self.epsilon / 3, sensitivity=1, lower=1, upper=n_total,\n                                  random_state=random_state)\n        noisy_counts = np.array([mech.randomise((y == y_i).sum()) for y_i in unique_y])\n\n        argsort = np.argsort(noisy_counts)\n        i = 0 if noisy_counts.sum() > n_total else len(unique_y) - 1\n\n        while np.sum(noisy_counts) != n_total:\n            _i = argsort[i]\n            sgn = np.sign(n_total - noisy_counts.sum())\n            noisy_counts[_i] = np.clip(noisy_counts[_i] + sgn, 1, n_total)\n\n            i = (i - sgn) % len(unique_y)\n\n        return noisy_counts\n"}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    try:\n        stopwords_data = pkgutil.get_data(\"sumy\", \"data/stopwords/%s.txt\" % language)\n    except IOError as e:\n        raise LookupError(\"Stop-words are not available for language %s.\" % language)\n    return parse_stop_words(stopwords_data)\n"}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if size is not None and not isinstance(size, int):\n            raise TypeError(\"size parameter must be an int or long.\")\n\n        if size is not None:\n            if size == 0:\n                return b\"\"\n            if size < 0:\n                size = None\n\n        self.buf.seek(0, os.SEEK_END)\n\n        if size is None and self.buf.tell():\n            ret = self.buf.getvalue()\n            self.buf = io.BytesIO()\n            return ret\n        if size is None:\n            d = self.chunk()\n            return d\n\n        while self.buf.tell() < size:\n            chunk = self.chunk()\n            if not chunk:\n                ret = self.buf.getvalue()\n                self.buf = io.BytesIO()\n                return ret\n            self.buf.write(chunk)\n        data = self.buf.getvalue()\n        self.buf = io.BytesIO()\n        self.buf.write(data[size:])\n        return data[:size]\n"}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        orig_dest = None\n        if not isinstance(dest, URL):\n            dest, orig_dest = URL(dest), dest\n        if dest.scheme and dest.host:\n            # absolute URLs replace everything, but don't make an\n            # extra copy if we don't have to\n            return URL(dest) if orig_dest is None else dest\n        query_params = dest.query_params\n\n        if dest.path:\n            if dest.path.startswith(u'/'):   # absolute path\n                new_path_parts = list(dest.path_parts)\n            else:  # relative path\n                new_path_parts = list(self.path_parts[:-1]) \\\n                               + list(dest.path_parts)\n        else:\n            new_path_parts = list(self.path_parts)\n            if not query_params:\n                query_params = self.query_params\n\n        ret = self.from_parts(scheme=dest.scheme or self.scheme,\n                              host=dest.host or self.host,\n                              port=dest.port or self.port,\n                              path_parts=new_path_parts,\n                              query_params=query_params,\n                              fragment=dest.fragment,\n                              username=dest.username or self.username,\n                              password=dest.password or self.password)\n        ret.normalize()\n        return ret\n"}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "        if self.Effect != other.Effect:\n            raise ValueError(f\"Trying to combine two statements with differing effects: {self.Effect} {other.Effect}\")\n\n        effect = self.Effect\n\n        actions = list(sorted(set(self.Action + other.Action), key=lambda action: action.json_repr()))\n        resources = list(sorted(set(self.Resource + other.Resource)))\n\n        return Statement(\n            Effect=effect,\n            Action=actions,\n            Resource=resources,\n        )\n"}
{"namespace": "threatingestor.Ingestor.run", "completion": "        if self.config.daemon():\n            logger.debug(\"Running forever, in a loop\")\n            self.run_forever()\n        else:\n            logger.debug(\"Running once, to completion\")\n            with self.statsd.timer('run_once'):\n                self.run_once()\n"}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    sym.analyze_node(node)\n    return sym\n"}
{"namespace": "boto.redshift.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.redshift.layer1 import RedshiftConnection\n    return connect('redshift', region_name,\n                   connection_cls=RedshiftConnection, **kw_params)\n"}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        key = self.prefix + bucket.key\n        value = bucket.bytecode_to_string()\n\n        try:\n            if self.timeout is not None:\n                self.client.set(key, value, self.timeout)\n            else:\n                self.client.set(key, value)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n"}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz %s\" % path)\n"}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if self._regexp is None:\n            self._regexp = re.compile(self.regexp)\n        return self._regexp.match(object_id)\n"}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            if with_annotations:\n                annotations = self.annotations\n            else:\n                annotations = {}\n\n            return inspect_formatargspec(self.args,\n                                         self.varargs,\n                                         self.varkw,\n                                         [],\n                                         self.kwonlyargs,\n                                         {},\n                                         annotations)\n"}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json_dumps(text)\n    return urlsafe_b64encode(to_bytes(text))\n"}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        from chatette.utils import UnitType\n        from chatette.parsing.utils import INTENT_SYM\n        from chatette.parsing.utils import ALIAS_SYM\n        from chatette.parsing.utils import SLOT_SYM\n        unit_type_str = unit_type_str.lower()\n        if unit_type_str in (\"alias\", ALIAS_SYM):\n            return UnitType.alias\n        if unit_type_str in (\"slot\", SLOT_SYM):\n            return UnitType.slot\n        if unit_type_str in (\"intent\", INTENT_SYM):\n            return UnitType.intent\n        return None\n"}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        if not self.needs_save() and not overwrite:\n            return False\n\n        final_data = self.prepare_full()\n        expects = None\n\n        if overwrite is False:\n            # Build expectations about *all* of the data.\n            expects = self.build_expects()\n\n        returned = self.table._put_item(final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned\n"}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        file_hashes = sorted(file.hash for file in self.files)\n        combined_hashes = '\\n'.join(file_hashes).encode('utf-8')\n        return hashlib.sha256(combined_hashes).hexdigest()\n"}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        scope = ScopeURI(\"client\", \"outgoing\", {\"appSid\": application_sid})\n        if kwargs:\n            scope.add_param(\"appParams\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"outgoing\"] = scope\n"}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        responses = self.default_schemas.copy()\n        type_responses = getattr(self, f\"default_{endpoint_type}_schemas\")\n        responses.update(**type_responses)\n\n        verb_responses = f\"default_{method.lower()}_schemas\"\n        method_args = getattr(self, verb_responses, {})\n        responses.update(**method_args)\n\n        method_responses = f\"{endpoint_type}_{method.lower()}_schemas\"\n        endpoint_args = getattr(self, method_responses, {})\n        responses.update(**endpoint_args)\n\n        # Bind and clone schemas into a new dict\n        bound = {code: resp.bind(**kwargs) for code, resp in responses.items()}\n\n        return bound\n"}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        self._tree_conf = tree_conf\n        length = PAGE_REFERENCE_BYTES + 4 * OTHERS_BYTES\n        data = (\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            self._tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            bytes(self._tree_conf.page_size - length)\n        )\n        self._write_page_in_tree(0, data, fsync=True)\n"}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n    return _load_opts_from_mrjob_conf(runner_alias, conf_path, already_loaded)\n"}
{"namespace": "datasette.utils.escape_fts", "completion": "    if query.count('\"') % 2:\n        query += '\"'\n    bits = _escape_fts_re.split(query)\n    bits = [b for b in bits if b and b != '\"\"']\n    return \" \".join(\n        '\"{}\"'.format(bit) if not bit.startswith('\"') else bit for bit in bits\n    )\n"}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    try:\n        registry = request.registry\n    except AttributeError:\n        registry = None\n    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n\n    with hide_attrs(request, 'response'):\n        if response is not None:\n            request.response = response\n        result = helper.render_to_response(value, None, request=request)\n\n    return result\n"}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        serialized_first_value = self._subtypes[0].serialize(\n            value[0], display=display\n        )\n        serialized_second_value = self._subtypes[1].serialize(\n            value[1], display=display\n        )\n\n        if (\n            not display\n            and self._optional_pair\n            and serialized_first_value == serialized_second_value\n        ):\n            return serialized_first_value\n        else:\n            return \"{0}{1}{2}\".format(\n                serialized_first_value,\n                self._separator,\n                serialized_second_value,\n            )\n"}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        try:\n            with open(filename, 'wb') as fp:\n                self.get_contents_to_file(fp, headers, cb, num_cb,\n                                          torrent=torrent,\n                                          version_id=version_id,\n                                          res_download_handler=res_download_handler,\n                                          response_headers=response_headers)\n        except Exception:\n            os.remove(filename)\n            raise\n        # if last_modified date was sent from s3, try to set file's timestamp\n        if self.last_modified is not None:\n            try:\n                modified_tuple = email.utils.parsedate_tz(self.last_modified)\n                modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n                os.utime(fp.name, (modified_stamp, modified_stamp))\n            except Exception:\n                pass\n"}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    urwid_theme = []\n    for style_name, (fg, bg) in theme_styles.items():\n        fg_code16, fg_code256, fg_code24, *fg_props = fg.value.split()\n        bg_code16, bg_code256, bg_code24, *bg_props = bg.value.split()\n\n        new_style: StyleSpec\n        if color_depth == 1:\n            new_style = (style_name, \"\", \"\", REQUIRED_STYLES[style_name])\n\n        elif color_depth == 16:\n            fg = \" \".join([fg_code16] + fg_props).replace(\"_\", \" \")\n            bg = \" \".join([bg_code16] + bg_props).replace(\"_\", \" \")\n            new_style = (style_name, fg, bg)\n\n        elif color_depth == 256:\n            fg = \" \".join([fg_code256] + fg_props).lower()\n            bg = \" \".join([bg_code256] + bg_props).lower()\n            new_style = (style_name, \"\", \"\", \"\", fg, bg)\n\n        elif color_depth == 2**24:\n            fg = \" \".join([fg_code24] + fg_props).lower()\n            bg = \" \".join([bg_code24] + bg_props).lower()\n            new_style = (style_name, \"\", \"\", \"\", fg, bg)\n\n        urwid_theme.append(new_style)\n    return urwid_theme\n"}
{"namespace": "boltons.strutils.asciify", "completion": "    try:\n        try:\n            return text.encode('ascii')\n        except UnicodeDecodeError:\n            # this usually means you passed in a non-unicode string\n            text = text.decode('utf-8')\n            return text.encode('ascii')\n    except UnicodeEncodeError:\n        mode = 'replace'\n        if ignore:\n            mode = 'ignore'\n        transd = unicodedata.normalize('NFKD', text.translate(DEACCENT_MAP))\n        ret = transd.encode('ascii', mode)\n        return ret\n"}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.association_id:\n            return self.connection.disassociate_address(\n                association_id=self.association_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.disassociate_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )\n"}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "\n    async def post_body(self):\n        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            assert message[\"type\"] == \"http.request\", message\n            body += message.get(\"body\", b\"\")\n"}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        if \"$schema\" not in self.data:\n            return None\n\n        schema = self.data[\"$schema\"]\n\n        return int(schema.split(\"/\")[-1].split(\".\")[0].lstrip(\"v\"))\n"}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix\n        # Generate random char digits\n        number += \"#\" * (length - len(prefix) - 1)\n        number = self.numerify(number)\n        reverse = number[::-1]\n        # Calculate sum\n        tot = 0\n        pos = 0\n        while pos < length - 1:\n            tot += Provider.luhn_lookup[reverse[pos]]\n            if pos != (length - 2):\n                tot += int(reverse[pos + 1])\n            pos += 2\n        # Calculate check digit\n        check_digit = (10 - (tot % 10)) % 10\n        number += str(check_digit)\n        return number\n"}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        from mrjob.cat import decompress\n        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            for chunk in decompress(f, path):\n                yield chunk\n"}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size < 0 or size > self._chunk_size:\n            size = self._chunk_size\n\n        if self._buffer_pos > 0:\n            self._trim_buffer()\n\n        if self._buffer_len < size:\n            async for chunk in self._source:\n                self._buffer += chunk\n                self._buffer_len = len(self._buffer)\n                if self._buffer_len >= size:  # pragma: no py39,py310 cover\n                    break\n\n        return self._buffer[:size]\n"}
{"namespace": "threatingestor.state.State.get_state", "completion": "        logger.debug(f\"Getting state for '{name}'\")\n        self.cursor.execute('SELECT state FROM states WHERE name=?', (name,))\n        res = self.cursor.fetchone()\n        return res[0] if res else res\n"}
{"namespace": "mingus.core.value.determine", "completion": "    i = -2\n    for v in base_values:\n        if value == v:\n            return (value, 0, 1, 1)\n        if value < v:\n            break\n        i += 1\n    scaled = float(value) / 2 ** i\n    if scaled >= 0.9375:  # base value\n        return (base_values[i], 0, 1, 1)\n    elif scaled >= 0.8125:\n        # septuplet: scaled = 0.875\n        return (base_values[i + 1], 0, 7, 4)\n    elif scaled >= 17 / 24.0:\n        # triplet: scaled = 0.75\n        return (base_values[i + 1], 0, 3, 2)\n    elif scaled >= 31 / 48.0:\n        # dotted note (one dot): scaled = 2/3.0\n        return (v, 1, 1, 1)\n    elif scaled >= 67 / 112.0:\n        # quintuplet: scaled = 0.625\n        return (base_values[i + 1], 0, 5, 4)\n    d = 3\n    for x in range(2, 5):\n        d += 2 ** x\n        if scaled == 2.0 ** x / d:\n            return (v, x, 1, 1)\n    return (base_values[i + 1], 0, 1, 1)\n"}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            sorted_params = sorted([(k, v) for k, v in self.params.items()])\n            encoded_params = urlencode(sorted_params)\n            param_string = \"?{}\".format(encoded_params)\n        else:\n            param_string = \"\"\n        return \"scope:{}:{}{}\".format(self.service, self.privilege, param_string)\n"}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n"}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    data = string.encode(encoding='utf-8')\n    data_length = len(data)\n    return int_to_bytes(data_length, 2) + data\n"}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}\n"}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        assert all(isinstance(member, ChatMember) for member in self.members), \\\n            f\"Some members of this chat is not a valid one: {self.members!r}\"\n"}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        if name is None:\n            return None\n        if not hasattr(cls, 'bootstraps'):\n            cls.bootstraps = {}\n        if name in cls.bootstraps:\n            return cls.bootstraps[name]\n        mod = importlib.import_module('pythonforandroid.bootstraps.{}'\n                                      .format(name))\n        if len(logger.handlers) > 1:\n            logger.removeHandler(logger.handlers[1])\n        bootstrap = mod.bootstrap\n        bootstrap.bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n        bootstrap.ctx = ctx\n        return bootstrap\n"}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        scope = ScopeURI(\"stream\", \"subscribe\", {\"path\": \"/2010-04-01/Events\"})\n        if kwargs:\n            scope.add_param(\"params\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"events\"] = scope\n"}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n"}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        object_uri = utils.strip_uri_prefix(request.path)\n\n        if self.on_plural_endpoint and object_id is not None:\n            # With the current request on a plural endpoint, the object URI must\n            # be found out by inspecting the \"plural\" service and its sibling\n            # \"object\" service. (see `register_resource()`)\n            matchdict = {**request.matchdict, \"id\": object_id}\n            try:\n                object_uri = utils.instance_uri(request, self.resource_name, **matchdict)\n                object_uri = object_uri.replace(\"%2A\", \"*\")\n            except KeyError:\n                # Maybe the resource has no single object endpoint.\n                # We consider that object URIs in permissions backend will\n                # be stored naively:\n                object_uri = f\"{object_uri}/{object_id}\"\n\n        return object_uri\n"}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        status_dir = path.join(xlog_dir, 'archive_status')\n        statuses = os.listdir(status_dir)\n\n        # Try to send earliest segments first.\n        statuses.sort()\n\n        for status in statuses:\n            # Only bother with segments, not history files and such;\n            # it seems like special treatment of such quantities is\n            # more likely to change than that of the WAL segments,\n            # which are bulky and situated in a particular place for\n            # crash recovery.\n            match = re.match(storage.SEGMENT_READY_REGEXP, status)\n\n            if match:\n                seg_name = match.groupdict()['filename']\n                seg_path = path.join(xlog_dir, seg_name)\n\n                yield WalSegment(seg_path, explicit=False)\n"}
{"namespace": "mopidy.config._load", "completion": "    from mopidy.internal import path\n    parser = configparser.RawConfigParser(inline_comment_prefixes=(\";\",))\n\n    # TODO: simply return path to config file for defaults so we can load it\n    # all in the same way?\n    logger.info(\"Loading config from builtin defaults\")\n    for default in defaults:\n        if isinstance(default, bytes):\n            default = default.decode()\n        parser.read_string(default)\n\n    # Load config from a series of config files\n    for f in files:\n        f = path.expand_path(f)\n        if f.is_dir():\n            for g in f.iterdir():\n                if g.is_file() and g.suffix == \".conf\":\n                    _load_file(parser, g.resolve())\n        else:\n            _load_file(parser, f.resolve())\n\n    raw_config = {}\n    for section in parser.sections():\n        raw_config[section] = dict(parser.items(section))\n\n    logger.info(\"Loading config from command line options\")\n    for section, key, value in overrides:\n        raw_config.setdefault(section, {})[key] = value\n\n    return raw_config\n"}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    if gamma < 0:\n        raise ValueError(f\"Gamma must be non-negative, got {gamma}.\")\n\n    rng = check_random_state(random_state, True)\n\n    while gamma > 1:\n        gamma -= 1\n        if not bernoulli_neg_exp(1, rng):\n            return 0\n\n    counter = 1\n\n    while rng.random() <= gamma / counter:\n        counter += 1\n\n    return counter % 2\n"}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        bin_digits = int(kw.pop('bin_digits', 1))\n        if kw:\n            raise TypeError('unexpected keyword arguments: %r' % kw.keys())\n\n        if not bins:\n            bins = self._get_bin_bounds()\n        else:\n            try:\n                bin_count = int(bins)\n            except TypeError:\n                try:\n                    bins = [float(x) for x in bins]\n                except Exception:\n                    raise ValueError('bins expected integer bin count or list'\n                                     ' of float bin boundaries, not %r' % bins)\n                if self.min < bins[0]:\n                    bins = [self.min] + bins\n            else:\n                bins = self._get_bin_bounds(bin_count)\n\n        # floor and ceil really should have taken ndigits, like round()\n        round_factor = 10.0 ** bin_digits\n        bins = [floor(b * round_factor) / round_factor for b in bins]\n        bins = sorted(set(bins))\n\n        idxs = [bisect.bisect(bins, d) - 1 for d in self.data]\n        count_map = {}  # would have used Counter, but py26 support\n        for idx in idxs:\n            try:\n                count_map[idx] += 1\n            except KeyError:\n                count_map[idx] = 1\n\n        bin_counts = [(b, count_map.get(i, 0)) for i, b in enumerate(bins)]\n\n        return bin_counts\n"}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        try:\n            encoder = getattr(self, '_encode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            raise ValueError(\"Unable to encode dynamodb type: %s\" %\n                             dynamodb_type)\n        return {dynamodb_type: encoder(attr)}\n"}
{"namespace": "dominate.util.unescape", "completion": "  cc = re.compile(r'&(?:(?:#(\\d+))|([^;]+));')\n\n  result = []\n  m = cc.search(data)\n  while m:\n    result.append(data[0:m.start()])\n    d = m.group(1)\n    if d:\n      d = int(d)\n      result.append(unichr(d))\n    else:\n      d = _unescape.get(m.group(2), ord('?'))\n      result.append(unichr(d))\n\n    data = data[m.end():]\n    m = cc.search(data)\n\n  result.append(data)\n  return ''.join(result)\n"}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        return Instrument.can_play_notes(self, notes)\n"}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        InstancePropertyHelper.set_property(\n            self, callable, name=name, reify=reify\n        )\n"}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n"}
{"namespace": "databases.importer.import_from_string", "completion": "    module_str, _, attrs_str = import_str.partition(\":\")\n    if not module_str or not attrs_str:\n        message = (\n            'Import string \"{import_str}\" must be in format \"<module>:<attribute>\".'\n        )\n        raise ImportFromStringError(message.format(import_str=import_str))\n\n    try:\n        module = importlib.import_module(module_str)\n    except ImportError as exc:\n        if exc.name != module_str:\n            raise exc from None\n        message = 'Could not import module \"{module_str}\".'\n        raise ImportFromStringError(message.format(module_str=module_str))\n\n    instance = module\n    try:\n        for attr_str in attrs_str.split(\".\"):\n            instance = getattr(instance, attr_str)\n    except AttributeError as exc:\n        message = 'Attribute \"{attrs_str}\" not found in module \"{module_str}\".'\n        raise ImportFromStringError(\n            message.format(attrs_str=attrs_str, module_str=module_str)\n        )\n\n    return instance\n"}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    try:\n        renderer = _constraint_renderers.dispatch(constraint)\n    except ValueError:\n        util.warn(\"No renderer is established for object %r\" % constraint)\n        return \"[Unknown Python object %r]\" % constraint\n    else:\n        return renderer(constraint, autogen_context, namespace_metadata)\n"}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        if len(self.lists) == 1:\n            self.lists[0].insert(index, item)\n            self._balance_list(0)\n        else:\n            list_idx, rel_idx = self._translate_index(index)\n            if list_idx is None:\n                raise IndexError()\n            self.lists[list_idx].insert(rel_idx, item)\n            self._balance_list(list_idx)\n        return\n"}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        map_ = self._revision_map\n        if not _replace and revision.revision in map_:\n            util.warn(\n                \"Revision %s is present more than once\" % revision.revision\n            )\n        elif _replace and revision.revision not in map_:\n            raise Exception(\"revision %s not in map\" % revision.revision)\n\n        map_[revision.revision] = revision\n\n        revisions = [revision]\n        self._add_branches(revisions, map_)\n        self._map_branch_labels(revisions, map_)\n        self._add_depends_on(revisions, map_)\n\n        if revision.is_base:\n            self.bases += (revision.revision,)\n        if revision._is_real_base:\n            self._real_bases += (revision.revision,)\n\n        for downrev in revision._all_down_revisions:\n            if downrev not in map_:\n                util.warn(\n                    \"Revision %s referenced from %s is not present\"\n                    % (downrev, revision)\n                )\n            not_none(map_[downrev]).add_nextrev(revision)\n\n        self._normalize_depends_on(revisions, map_)\n\n        if revision._is_real_head:\n            self._real_heads = tuple(\n                head\n                for head in self._real_heads\n                if head\n                not in set(revision._all_down_revisions).union(\n                    [revision.revision]\n                )\n            ) + (revision.revision,)\n        if revision.is_head:\n            self.heads = tuple(\n                head\n                for head in self.heads\n                if head\n                not in set(revision._versioned_down_revisions).union(\n                    [revision.revision]\n                )\n            ) + (revision.revision,)\n"}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    time_tuple = parsedate_tz(_munge(timestamp))\n    if time_tuple is None:\n        raise ValueError(\"couldn't parse datetime %r\" % timestamp)\n\n    tz_offset_seconds = time_tuple[-1]\n    tz = None\n    if tz_offset_seconds is not None:\n        tz = FixedOffset(tz_offset_seconds / 60)\n\n    dt = datetime(*time_tuple[:6], tzinfo=tz)\n    if normalise and tz:\n        dt = datetime_to_native(dt)\n\n    return dt\n"}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    query = urlparse.urlparse(uri).query\n    params = dict(urlparse.parse_qsl(query))\n\n    if 'code' not in params:\n        raise MissingCodeException()\n\n    params_state = params.get('state')\n    if state and params_state != state:\n        raise MismatchingStateException()\n\n    return params\n"}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        from rest_framework.utils import json\n        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n\n        try:\n            decoded_stream = codecs.getreader(encoding)(stream)\n            parse_constant = json.strict_constant if self.strict else None\n            return json.load(decoded_stream, parse_constant=parse_constant)\n        except ValueError as exc:\n            raise ParseError('JSON parse error - %s' % str(exc))\n"}
{"namespace": "jwt.utils.base64url_decode", "completion": "    input_bytes = force_bytes(input)\n\n    rem = len(input_bytes) % 4\n\n    if rem > 0:\n        input_bytes += b\"=\" * (4 - rem)\n\n    return base64.urlsafe_b64decode(input_bytes)\n"}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        fn: Callable\n        if select_for_downgrade:\n            fn = self._collect_downgrade_revisions\n        else:\n            fn = self._collect_upgrade_revisions\n\n        revisions, heads = fn(\n            upper,\n            lower,\n            inclusive=inclusive,\n            implicit_base=implicit_base,\n            assert_relative_length=assert_relative_length,\n        )\n\n        for node in self._topological_sort(revisions, heads):\n            yield not_none(self.get_revision(node))\n"}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        from zulipterminal.config.regexes import REGEX_RECIPIENT_EMAIL\n        self.recipient_emails = re.findall(REGEX_RECIPIENT_EMAIL, write_box.edit_text)\n        self._set_regular_and_typing_recipient_user_ids(\n            [self.model.user_dict[email][\"user_id\"] for email in self.recipient_emails]\n        )\n"}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.isfile(path) and os.path.getsize(path) != 0:\n            raise OSError('Non-empty file %r already exists!' % (path,))\n\n        # zero out the file\n        with open(path, 'w'):\n            pass\n"}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    ret, max_anon = '', 0\n    # look for {: or {! or {. or {[ or {}\n    start, end, prev_end = 0, 0, 0\n    for match in _pos_farg_re.finditer(fstr):\n        start, end, group = match.start(), match.end(), match.group()\n        if prev_end < start:\n            ret += fstr[prev_end:start]\n        prev_end = end\n        if group == '{{' or group == '}}':\n            ret += group\n            continue\n        ret += '{%s%s' % (max_anon, group[1:])\n        max_anon += 1\n    ret += fstr[prev_end:]\n    return ret\n"}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not filename or not os.path.exists(filename):\n            raise ValueError('invalid filename or file not found \"{}\"'.format(filename))\n\n        main_file = get_py_file_if_possible(os.path.abspath(filename))\n        repo, commit, is_dirty = get_commit_if_possible(main_file, save_git_info)\n        return Source(main_file, get_digest(main_file), repo, commit, is_dirty)\n"}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        logger.debug('Json Rpc client started.')\n        self.request_thread = threading.Thread(\n            target=self._listen_for_request,\n            name=self.REQUEST_THREAD_NAME)\n        self.request_thread.daemon = True\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(\n            target=self._listen_for_response,\n            name=self.RESPONSE_THREAD_NAME)\n        self.response_thread.daemon = True\n        self.response_thread.start()\n"}
{"namespace": "alembic.command.downgrade", "completion": "    script = ScriptDirectory.from_config(config)\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n    elif sql:\n        raise util.CommandError(\n            \"downgrade with --sql requires <fromrev>:<torev>\"\n        )\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()\n"}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)\n        return\n"}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    from authlib.common.urls import add_params_to_uri\n    params = [\n        ('response_type', response_type),\n        ('client_id', client_id)\n    ]\n\n    if redirect_uri:\n        params.append(('redirect_uri', redirect_uri))\n    if scope:\n        params.append(('scope', list_to_scope(scope)))\n    if state:\n        params.append(('state', state))\n\n    for k in kwargs:\n        if kwargs[k] is not None:\n            params.append((to_unicode(k), kwargs[k]))\n\n    return add_params_to_uri(uri, params)\n"}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    try:\n        num_bins = len(bins)\n    except TypeError:\n        num_bins = 1\n\n    if num_bins not in (1, 2):\n        xedges = yedges = np.asarray(bins)\n        bins = [xedges, yedges]\n\n    hist, edges = histogramdd([array_x, array_y], epsilon=epsilon, bins=bins, range=range, weights=weights,\n                              density=density, random_state=random_state, accountant=accountant)\n    return hist, edges[0], edges[1]\n"}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        self.configurations.append(ConfigScope(function))\n        return self.configurations[-1]\n"}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        from .._compat import unicode\n        if hasattr(self._sentence_tokenizer, '_params'):\n            extra_abbreviations = self.LANGUAGE_EXTRA_ABREVS.get(self._language, [])\n            self._sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n        sentences = self._sentence_tokenizer.tokenize(to_unicode(paragraph))\n        return tuple(map(unicode.strip, sentences))\n"}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route is not None:\n            matches = [db for db in self.databases.values() if db.route == route]\n            if not matches:\n                raise KeyError\n            return matches[0]\n        if name is None:\n            # Return first database that isn't \"_internal\"\n            name = [key for key in self.databases.keys() if key != \"_internal\"][0]\n        return self.databases[name]\n"}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, principals, (),\n                                               hash_alg, comment)\n"}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    obj_parts = object_uri.split(\"/\")\n    for length in range(len(obj_parts) + 1):\n        parent_uri = \"/\".join(obj_parts[:length])\n        parent_resource_name, _ = _resource_endpoint(parent_uri)\n        if resource_name == parent_resource_name:\n            return parent_uri\n\n    error_msg = f\"Cannot get URL of resource '{resource_name}' from parent '{object_uri}'.\"\n    raise ValueError(error_msg)\n"}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    return sorted(\n        set([variable] + list(_JOBCONF_MAP.get(variable, {}).values())))\n"}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n    else:\n        return _to_lines(chunks)\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n    if len(likelihoods) == 0:\n        return [], np.nan\n\n    min_lik = min(likelihoods)\n    ind = likelihoods.index(min_lik)\n    return session[ind : ind + window_len], min_lik  # noqa E203\n"}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n"}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        validators.validate_choice(value, self._choices)\n        return value\n"}
{"namespace": "backtrader.trade.Trade.update", "completion": "        if not size:\n            return  # empty update, skip all other calculations\n\n        # Commission can only increase\n        self.commission += commission\n\n        # Update size and keep a reference for logic an calculations\n        oldsize = self.size\n        self.size += size  # size will carry the opposite sign if reducing\n\n        # Check if it has been currently opened\n        self.justopened = bool(not oldsize and size)\n\n        if self.justopened:\n            self.baropen = len(self.data)\n            self.dtopen = 0.0 if order.p.simulated else self.data.datetime[0]\n            self.long = self.size > 0\n\n        # Any size means the trade was opened\n        self.isopen = bool(self.size)\n\n        # Update current trade length\n        self.barlen = len(self.data) - self.baropen\n\n        # record if the position was closed (set to null)\n        self.isclosed = bool(oldsize and not self.size)\n\n        # record last bar for the trade\n        if self.isclosed:\n            self.isopen = False\n            self.barclose = len(self.data)\n            self.dtclose = self.data.datetime[0]\n\n            self.status = self.Closed\n        elif self.isopen:\n            self.status = self.Open\n\n        if abs(self.size) > abs(oldsize):\n            # position increased (be it positive or negative)\n            # update the average price\n            self.price = (oldsize * self.price + size * price) / self.size\n            pnl = 0.0\n\n        else:  # abs(self.size) < abs(oldsize)\n            # position reduced/closed\n            pnl = comminfo.profitandloss(-size, self.price, price)\n\n        self.pnl += pnl\n        self.pnlcomm = self.pnl - self.commission\n\n        self.value = comminfo.getvaluesize(self.size, self.price)\n\n        # Update the history if needed\n        if self.historyon:\n            dt0 = self.data.datetime[0] if not order.p.simulated else 0.0\n            histentry = TradeHistory(\n                self.status, dt0, self.barlen,\n                self.size, self.price, self.value,\n                self.pnl, self.pnlcomm, self.data._tz)\n            histentry.doupdate(order, size, price, commission)\n            self.history.append(histentry)\n"}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        member = self.make_system_member(name=name, alias=alias, id=id, uid=uid,\n                                         vendor_specific=vendor_specific, description=description,\n                                         middleware=middleware)\n        self.members.append(member)\n        return member\n"}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        captured_f = self.capture(function, prefix=prefix)\n        captured_f.unobserved = unobserved\n        self.commands[function.__name__] = captured_f\n        return captured_f\n"}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        d = {\"max_depth\": self.max_depth,\n             \"node_count\": self.node_count,\n             \"nodes\": np.array([tuple(node) for node in self.nodes], dtype=NODE_DTYPE),\n             \"values\": self.values_}\n        return d\n"}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        if len(self.containers) > 1:\n            # Return a new instance of this class using the tails of\n            # the separators and containers lists. Use self.__class__()\n            # in case :class:`hl7.ParsePlan` is subclassed\n            return self.__class__(\n                self.separators[self.separators.find(self.separator) + 1],\n                self.separators,\n                self.containers[1:],\n                self.esc,\n                self.factory,\n            )\n        # When we have no separators and containers left, return None,\n        # which indicates that we have nothing further.\n        return None\n"}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        self._idle_tag = self._imap._command(\"IDLE\")\n        resp = self._imap._get_response()\n        if resp is not None:\n            raise exceptions.IMAPClientError(\"Unexpected IDLE response: %s\" % resp)\n"}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    from boto.regioninfo import connect\n    return connect('cloudformation', region_name,\n                   connection_cls=CloudFormationConnection, **kw_params)\n"}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        return \"Section: {} ({}):\\n{}\\nSubsections ({}):\\n{}\".format(\n            self._title,\n            self._level,\n            self._text,\n            len(self._section),\n            \"\\n\".join(map(repr, self._section)),\n        )\n"}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "    if font_bold is None:\n        font_bold = lambda s: colorama.Style.BRIGHT + s + colorama.Style.RESET_ALL\n    if font_dim is None:\n        font_dim = lambda s: colorama.Style.DIM + s + colorama.Style.RESET_ALL\n    if font_red is None:\n        font_red = lambda s: colorama.Fore.RED + s + colorama.Style.RESET_ALL\n    if font_blue is None:\n        font_blue = lambda s: colorama.Fore.CYAN + s + colorama.Style.RESET_ALL\n    if font_normal is None:\n        font_normal = lambda s: s\n\n    result = []\n    for key, value in tokens:\n        if key == _PrettyTokenType.BODY:\n            value = font_bold(value)\n        elif key == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            value = font_red(value)\n        elif key == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            value = font_red(value)\n        elif key == _PrettyTokenType.WHITESPACE:\n            value = font_dim(_replace_whitespace(value))\n        elif key == _PrettyTokenType.NEWLINE:\n            value = font_dim(_replace_whitespace(value))\n        elif key == _PrettyTokenType.HINT:\n            value = font_dim(value)\n        elif key == _PrettyTokenType.LINENO:\n            value = font_blue(value)\n        elif key == _PrettyTokenType.OTHERS:\n            value = font_normal(value)\n        else:\n            assert False\n        result.append(value)\n    return ''.join(result)\n"}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        return inspect(connectable).has_table(tablename, schemaname)\n    else:\n        return connectable.dialect.has_table(\n            connectable, tablename, schemaname\n        )\n"}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        self._to_put.append(data)\n\n        if self.should_flush():\n            self.flush()\n"}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin[-4:])\n        return ip_bin_to_ip6_addr(self.ip_bin)\n"}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n"}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        attachment_id = getattr(self.attachment, 'id', None)\n\n        return self.connection.detach_network_interface(\n            attachment_id,\n            force,\n            dry_run=dry_run\n        )\n"}
{"namespace": "mingus.core.progressions.skip", "completion": "    i = numerals.index(roman_numeral) + skip_count\n    return numerals[i % 7]\n"}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)\n"}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        return identity['repoze.who.userid']\n"}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError()\n        return self._real_loader\n"}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "        try:\n            return getattr(self, 'visit_{}'.format(instance.__visit_name__))(instance)\n        except AttributeError as e:\n            raise RuntimeError(\n                'This visitor does not support {}'.format(type(instance))\n            ) from e\n"}
{"namespace": "mackup.utils.copy", "completion": "    assert isinstance(src, str)\n    assert os.path.exists(src)\n    assert isinstance(dst, str)\n\n    # Create the path to the dst file if it does not exist\n    abs_path = os.path.dirname(os.path.abspath(dst))\n    if not os.path.isdir(abs_path):\n        os.makedirs(abs_path)\n\n    # We need to copy a single file\n    if os.path.isfile(src):\n        # Copy the src file to dst\n        shutil.copy(src, dst)\n\n    # We need to copy a whole folder\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n\n    # What the heck is this?\n    else:\n        raise ValueError(\"Unsupported file: {}\".format(src))\n\n    # Set the good mode to the file or folder recursively\n    chmod(dst)\n"}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n"}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.codedeploy.layer1 import CodeDeployConnection\n    return connect('codedeploy', region_name,\n                   connection_cls=CodeDeployConnection, **kw_params)\n"}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n"}
{"namespace": "authlib.jose.util.extract_header", "completion": "    header_data = extract_segment(header_segment, error_cls, 'header')\n\n    try:\n        header = json_loads(header_data.decode('utf-8'))\n    except ValueError as e:\n        raise error_cls('Invalid header string: {}'.format(e))\n\n    if not isinstance(header, dict):\n        raise error_cls('Header must be a json object')\n    return header\n"}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if not dt.tzinfo:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n    fmt = \"%d-\" + _SHORT_MONTHS[dt.month] + \"-%Y %H:%M:%S %z\"\n    return dt.strftime(fmt)\n"}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(\n            Sms(\n                message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )\n"}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    if not socket.has_ipv6:\n        return False\n    try:\n        socket.socket(socket.AF_INET6).close()\n        return True\n    except OSError as exc:\n        logger.debug(\n            f\"Platform supports IPv6, but socket creation failed, \"\n            f\"disabling: {exc}\"\n        )\n    return False\n"}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            headers = self._cached_headers = {}\n\n            env = self.env\n            for name, value in env.items():\n                if name.startswith('HTTP_'):\n                    # NOTE(kgriffs): Don't take the time to fix the case\n                    # since headers are supposed to be case-insensitive\n                    # anyway.\n                    headers[name[5:].replace('_', '-')] = value\n\n                elif name in WSGI_CONTENT_HEADERS:\n                    headers[name.replace('_', '-')] = value\n\n        return self._cached_headers\n"}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n"}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    suffixes = []\n    candidates_without_suffix = []  # remember these as fallback\n    for candidate in (filename, src_path, tmp_path):\n        # remove path component. Could be from linux, mac or windows\n        idx = max(candidate.rfind('/'), candidate.rfind('\\\\'))\n        candidate = candidate[idx+1:].strip()\n\n        # sanitize\n        candidate = sanitize_filename(candidate, max_len=max_len)\n\n        if not candidate:\n            continue    # skip whitespace-only\n\n        # identify suffix. Dangerous suffixes are all short\n        idx = candidate.rfind('.')\n        if idx is -1:\n            candidates_without_suffix.append(candidate)\n            continue\n        elif idx < len(candidate)-5:\n            candidates_without_suffix.append(candidate)\n            continue\n\n        # remember suffix\n        suffixes.append(candidate[idx:])\n\n        yield candidate\n\n    # parts with suffix not good enough? try those without one\n    for candidate in candidates_without_suffix:\n        yield candidate\n\n    # then try random\n    suffixes.append('')  # ensure there is something in there\n    for _ in range(MAX_FILENAME_ATTEMPTS):\n        for suffix in suffixes:\n            leftover_len = max_len - len(suffix)\n            if leftover_len < 1:\n                continue\n            name = ''.join(random.sample('abcdefghijklmnopqrstuvwxyz',\n                                         min(26, leftover_len)))\n            yield name + suffix\n\n    # still not returned? Then we have to make up a name ourselves\n    # do not care any more about max_len (maybe it was 0 or negative)\n    yield 'oleobj_%03d' % noname_index\n"}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.unit_reference import UnitReference\n        self._check_information()\n        return UnitReference(\n            self.identifier, self.type,\n            self.leading_space, self._build_modifiers_repr()\n        )\n"}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        callbacks = self.response_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self, response)\n"}
{"namespace": "fs.path.join", "completion": "    absolute = False\n    relpaths = []  # type: List[Text]\n    for p in paths:\n        if p:\n            if p[0] == \"/\":\n                del relpaths[:]\n                absolute = True\n            relpaths.append(p)\n\n    path = normpath(\"/\".join(relpaths))\n    if absolute:\n        path = abspath(path)\n    return path\n"}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.field_name not in dictionary:\n            if getattr(self.root, 'partial', False):\n                return empty\n        # We override the default field access in order to support\n        # lists in HTML forms.\n        if html.is_html_input(dictionary):\n            return dictionary.getlist(self.field_name)\n        return dictionary.get(self.field_name, empty)\n"}
{"namespace": "mopidy.config._validate", "completion": "    config = {}\n    errors = {}\n    sections = set(raw_config)\n    for schema in schemas:\n        sections.discard(schema.name)\n        values = raw_config.get(schema.name, {})\n        result, error = schema.deserialize(values)\n        if error:\n            errors[schema.name] = error\n        if result:\n            config[schema.name] = result\n\n    for section in sections:\n        logger.warning(\n            f\"Ignoring config section {section!r} \"\n            f\"because no matching extension was found\"\n        )\n\n    return config, errors\n"}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        result = Components.registerSubscriptionAdapter(self, *arg, **kw)\n        self.has_listeners = True\n        return result\n"}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        return Statement(\n            Effect=\"Allow\",\n            Action=[Action(self._source_to_iam_prefix(), self._event_name_to_iam_action())],\n            Resource=sorted(self.resource_arns)\n        )\n"}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            # We don't know the structure of the table. Get a description to\n            # populate the schema.\n            self.describe()\n\n        return [field.name for field in self.schema]\n"}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if not method or not params:\n            raise ValueError(u'Method or Parameter was not found in request')\n\n        request = {u'method': method, u'params': params, u'id': request_id}\n        self.request_queue.put(request)\n"}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    path_parts = path.split(\"/\")\n    name_parts = path_parts[-1].split(\".\")\n\n    # Check if the resource has a fingerprint\n    if len(name_parts) > 2 and cache_regex.match(name_parts[1]):\n        original_name = \".\".join([name_parts[0]] + name_parts[2:])\n        return \"/\".join(path_parts[:-1] + [original_name]), True\n\n    return path, False\n"}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "    import sacred.optional as opt\n    experiment_path, main = get_main_file(globs, save_git_info)\n\n    base_dir = base_dir or experiment_path\n\n    gather_sources = source_discovery_strategies[SETTINGS[\"DISCOVER_SOURCES\"]]\n    sources = gather_sources(globs, base_dir, save_git_info)\n    if main is not None:\n        sources.add(main)\n\n    gather_dependencies = dependency_discovery_strategies[\n        SETTINGS[\"DISCOVER_DEPENDENCIES\"]\n    ]\n    dependencies = gather_dependencies(globs, base_dir)\n\n    if opt.has_numpy:\n        # Add numpy as a dependency because it might be used for randomness\n        dependencies.add(PackageDependency.create(opt.np))\n\n    return main, sources, dependencies\n"}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        expects = {}\n\n        if fields is None:\n            fields = list(self._data.keys()) + list(self._orig_data.keys())\n\n        # Only uniques.\n        fields = set(fields)\n\n        for key in fields:\n            expects[key] = {\n                'Exists': True,\n            }\n            value = None\n\n            # Check for invalid keys.\n            if not key in self._orig_data and not key in self._data:\n                raise ValueError(\"Unknown key %s provided.\" % key)\n\n            # States:\n            # * New field (only in _data)\n            # * Unchanged field (in both _data & _orig_data, same data)\n            # * Modified field (in both _data & _orig_data, different data)\n            # * Deleted field (only in _orig_data)\n            orig_value = self._orig_data.get(key, NEWVALUE)\n            current_value = self._data.get(key, NEWVALUE)\n\n            if orig_value == current_value:\n                # Existing field unchanged.\n                value = current_value\n            else:\n                if key in self._data:\n                    if not key in self._orig_data:\n                        # New field.\n                        expects[key]['Exists'] = False\n                    else:\n                        # Existing field modified.\n                        value = orig_value\n                else:\n                   # Existing field deleted.\n                    value = orig_value\n\n            if value is not None:\n                expects[key]['Value'] = self._dynamizer.encode(value)\n\n        return expects\n"}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    from boto.regioninfo import connect\n    return connect('elasticloadbalancing', region_name,\n                   connection_cls=ELBConnection, **kw_params)\n"}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    return datetime.fromtimestamp(t, tz=timezone.utc)\n"}
{"namespace": "bplustree.entry.Record.dump", "completion": "        assert self.value is None or self.overflow_page is None\n        key_as_bytes = self._tree_conf.serializer.serialize(\n            self.key, self._tree_conf.key_size\n        )\n        used_key_length = len(key_as_bytes)\n        overflow_page = self.overflow_page or 0\n        if overflow_page:\n            value = b''\n        else:\n            value = self.value\n        used_value_length = len(value)\n\n        data = (\n            used_key_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            key_as_bytes +\n            bytes(self._tree_conf.key_size - used_key_length) +\n            used_value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            value +\n            bytes(self._tree_conf.value_size - used_value_length) +\n            overflow_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n        return data\n"}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        from pyramid.events import BeforeRender\n        renderer = self.renderer\n        if system_values is None:\n            system_values = {\n                'view': None,\n                'renderer_name': self.name,  # b/c\n                'renderer_info': self,\n                'context': getattr(request, 'context', None),\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n\n        system_values = BeforeRender(system_values, value)\n\n        registry = self.registry\n        registry.notify(system_values)\n        result = renderer(value, system_values)\n        return result\n"}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        instance_id = None\n        if self.attach_data:\n            instance_id = self.attach_data.instance_id\n        device = None\n        if self.attach_data:\n            device = self.attach_data.device\n        return self.connection.detach_volume(\n            self.id,\n            instance_id,\n            device,\n            force,\n            dry_run=dry_run\n        )\n"}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            # PERF(kgriffs): While this code isn't exactly DRY (the same code\n            # is duplicated by the cookies property) it does make things a bit\n            # more performant by removing the extra function call that would\n            # be required to factor this out. If we ever have to do this in a\n            # *third* place, we would probably want to factor it out at that\n            # point.\n            header_value = self.get_header('Cookie')\n            if header_value:\n                self._cookies = helpers.parse_cookie_header(header_value)\n            else:\n                self._cookies = {}\n\n        return self._cookies.get(name)\n"}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr) > 1 or not attr or is_str(attr):\n            return attr\n        dynamodb_type = list(attr.keys())[0]\n        if dynamodb_type.lower() == dynamodb_type:\n            # It's not an actual type, just a single character attr that\n            # overlaps with the DDB types. Return it.\n            return attr\n        try:\n            decoder = getattr(self, '_decode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            return attr\n        return decoder(attr[dynamodb_type])\n"}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "    def yield_records():\n        for record in _parse_hadoop_log4j_records(lines):\n            if record_callback:\n                record_callback(record)\n            yield record\n\n    return _parse_spark_log_from_log4j_records(yield_records())\n"}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if not ns or self.__ns is False:\n            ##log.debug('adding %s without namespace', name)\n            element = self.__document.createElement(name)\n        else:\n            ##log.debug('adding %s ns \"%s\" %s', name, self.__ns, ns)\n            if isinstance(ns, basestring):\n                element = self.__document.createElement(name)\n                if ns:\n                    element.setAttribute(\"xmlns\", ns)\n            elif self.__prefix:\n                element = self.__document.createElementNS(self.__ns, \"%s:%s\" % (self.__prefix, name))\n            else:\n                element = self.__document.createElementNS(self.__ns, name)\n        # don't append null tags!\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                element.appendChild(self.__document.createCDATASection(text.data))\n            else:\n                element.appendChild(self.__document.createTextNode(text))\n        self._element.appendChild(element)\n        return SimpleXMLElement(\n            elements=[element],\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n"}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        try:\n            return self.mapping[a][0]  # if object is mapped, return ID\n        except KeyError:\n            pass\n\n        if self.free:  # if there are any free IDs, use the smallest\n            nxt = heapq.heappop(self.free)\n        else:  # if there are no free numbers, use the next highest ID\n            nxt = len(self.mapping)\n        ref = weakref.ref(a, self._clean)\n        self.mapping[a] = (nxt, ref)\n        self.ref_map[ref] = nxt\n        return nxt\n"}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    from pycoin.intbytes import indexbytes\n    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n\n    length = len(data)\n    h1 = seed\n    roundedEnd = (length & 0xfffffffc)  # round down to 4 byte block\n    for i in range(0, roundedEnd, 4):\n        # little endian load order\n        k1 = (indexbytes(data, i) & 0xff) | ((indexbytes(data, i + 1) & 0xff) << 8) | \\\n            ((indexbytes(data, i + 2) & 0xff) << 16) | (indexbytes(data, i + 3) << 24)\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1,15)\n        k1 *= c2\n\n        h1 ^= k1\n        h1 = (h1 << 13) | ((h1 & 0xffffffff) >> 19)  # ROTL32(h1,13)\n        h1 = h1 * 5 + 0xe6546b64\n\n    # tail\n    k1 = 0\n\n    val = length & 0x03\n    if val == 3:\n        k1 = (indexbytes(data, roundedEnd + 2) & 0xff) << 16\n    # fallthrough\n    if val in [2, 3]:\n        k1 |= (indexbytes(data, roundedEnd + 1) & 0xff) << 8\n    # fallthrough\n    if val in [1, 2, 3]:\n        k1 |= indexbytes(data, roundedEnd) & 0xff\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1,15)\n        k1 *= c2\n        h1 ^= k1\n\n    # finalization\n    h1 ^= length\n\n    # fmix(h1)\n    h1 ^= ((h1 & 0xffffffff) >> 16)\n    h1 *= 0x85ebca6b\n    h1 ^= ((h1 & 0xffffffff) >> 13)\n    h1 *= 0xc2b2ae35\n    h1 ^= ((h1 & 0xffffffff) >> 16)\n\n    return h1 & 0xffffffff\n"}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        unfiltered_rs = self.connection.get_all_volumes(\n            [self.id],\n            dry_run=dry_run\n        )\n        rs = [x for x in unfiltered_rs if x.id == self.id]\n        if len(rs) > 0:\n            self._update(rs[0])\n        elif validate:\n            raise ValueError('%s is not a valid Volume ID' % self.id)\n        return self.status\n"}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "    from pyinfra.api.state import State\n    from pyinfra import context\n    state = state or context.state\n    host = host or context.host\n\n    config = state.config\n    if context.ctx_config.isset():\n        config = context.config\n\n    meta_kwargs = host.current_deploy_kwargs or {}\n\n    arguments: AllArguments = {}\n    found_keys: list[str] = []\n\n    for key, type_ in AllArguments.__annotations__.items():\n        if keys_to_check and key not in keys_to_check:\n            continue\n\n        argument_meta = all_argument_meta[key]\n        handler = argument_meta.handler\n        default: Any = argument_meta.default(config)\n\n        host_default = getattr(host.data, key, default_sentinel)\n        if host_default is not default_sentinel:\n            default = host_default\n\n        if key in kwargs:\n            found_keys.append(key)\n            value = kwargs.pop(key)\n        else:\n            value = meta_kwargs.get(key, default)\n\n        if handler is not default_sentinel:\n            value = handler(config, value)\n\n        # TODO: why is type failing here?\n        arguments[key] = value  # type: ignore\n    return arguments, found_keys\n"}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    symbol = symbol.upper()\n    netcode = symbol.lower()\n    for prefix in search_prefixes():\n        try:\n            module = importlib.import_module(\"%s.%s\" % (prefix, netcode))\n            if module.network.symbol.upper() == symbol:\n                module.symbol = symbol\n                return module.network\n        except (AttributeError, ImportError):\n            pass\n    raise ValueError(\"no network with symbol %s found\" % netcode)\n"}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = None\n        cls._instance = cls(*args, **kwargs)\n        return cls._instance\n"}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    assert type(s) in (Constant, BitVec) and size - s.size >= 0\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, \"(_ zero_extend {})\".format(size - s.size), s)\n"}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        return dict((path, self.uri(path))\n                    for path in self._path_to_name)\n"}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, (ast.Tuple, ast.List)):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []\n"}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete = {\n        name\n        for name, theme in THEMES.items()\n        if set(theme.STYLES) == set(REQUIRED_STYLES)\n        if set(theme.META) == set(REQUIRED_META)\n        for meta, conf in theme.META.items()\n        if set(conf) == set(REQUIRED_META.get(meta, {}))\n    }\n    incomplete = list(set(THEMES) - complete)\n    return sorted(list(complete)), sorted(incomplete)\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    param_stats = [\n        (param, len(vals), param_counts[param], 100 * len(vals) / param_counts[param])\n        for param, vals in param_value_counts.items()\n    ]\n\n    modellable_params = [\n        param[0]\n        for param in param_stats\n        if param[1] <= 20 <= param[2] and param[3] <= 10\n    ]\n\n    return set(modellable_params)\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_token is True\"\n            )\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_end_token is True\"\n            )\n\n    w_len = len(window)\n    if w_len == 0:\n        return np.nan\n    prob: float = 1\n\n    cur_cmd = window[0].name\n    params = window[0].params\n    param_vals_prob = compute_prob_setofparams_given_cmd(\n        cmd=cur_cmd,\n        params_with_vals=params,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        use_geo_mean=True,\n    )\n\n    if use_start_token:\n        prob *= trans_probs[start_token][cur_cmd] * param_vals_prob\n    else:\n        prob *= prior_probs[cur_cmd] * param_vals_prob\n\n    for i in range(1, w_len):\n        prev, cur = window[i - 1], window[i]\n        prev_cmd, cur_cmd = prev.name, cur.name\n        cur_par = cur.params\n        prob *= trans_probs[prev_cmd][cur_cmd]\n        param_vals_prob = compute_prob_setofparams_given_cmd(\n            cmd=cur_cmd,\n            params_with_vals=cur_par,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_geo_mean=True,\n        )\n        prob *= param_vals_prob\n\n    if use_end_token:\n        prob *= trans_probs[cur_cmd][end_token]\n\n    return prob\n"}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)\n"}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        with self._recv_lock:\n            return self.rbuf\n"}
{"namespace": "falcon.request.Request.client_accepts", "completion": "        accept = self.accept\n\n        # PERF(kgriffs): Usually the following will be true, so\n        # try it first.\n        if (accept == media_type) or (accept == '*/*'):\n            return True\n\n        # Fall back to full-blown parsing\n        try:\n            return mimeparse.quality(media_type, accept) != 0.0\n        except ValueError:\n            return False\n"}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "    auth_methods = []\n\n    for method in _auth_methods:\n        if _server_auth_handlers[method].supported(conn):\n            auth_methods.append(method)\n\n    return auth_methods\n"}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        if n_noisy is None:\n            warnings.warn(\"Noisy class count has not been specified and will be read from the data. To use this \"\n                          \"method correctly, make sure it is run by the parent GaussianNB class.\", PrivacyLeakWarning)\n            n_noisy = X.shape[0]\n\n        if not n_noisy:\n            return mu, var\n\n        if sample_weight is not None:\n            warn_unused_args(\"sample_weight\")\n\n        # Split epsilon between each feature, using 1/3 of total budget for each of mean and variance\n        n_features = X.shape[1]\n        local_epsilon = self.epsilon / 3 / n_features\n\n        new_mu = np.zeros((n_features,))\n        new_var = np.zeros((n_features,))\n\n        for feature in range(n_features):\n            temp_x = X[:, feature]\n            lower, upper = self.bounds[0][feature], self.bounds[1][feature]\n            local_diameter = upper - lower\n\n            mech_mu = LaplaceTruncated(epsilon=local_epsilon, delta=0, sensitivity=local_diameter,\n                                       lower=lower * n_noisy, upper=upper * n_noisy, random_state=random_state)\n            _mu = mech_mu.randomise(temp_x.sum()) / n_noisy\n\n            local_sq_sens = max(_mu - lower, upper - _mu) ** 2\n            mech_var = LaplaceBoundedDomain(epsilon=local_epsilon, delta=0, sensitivity=local_sq_sens, lower=0,\n                                            upper=local_sq_sens * n_noisy, random_state=random_state)\n            _var = mech_var.randomise(((temp_x - _mu) ** 2).sum()) / n_noisy\n\n            new_mu[feature] = _mu\n            new_var[feature] = _var\n\n        if n_past == 0:\n            return new_mu, new_var\n\n        n_total = float(n_past + n_noisy)\n\n        # Combine mean of old and new data, taking into consideration\n        # (weighted) number of observations\n        total_mu = (n_noisy * new_mu + n_past * mu) / n_total\n\n        # Combine variance of old and new data, taking into consideration\n        # (weighted) number of observations. This is achieved by combining\n        # the sum-of-squared-differences (ssd)\n        old_ssd = n_past * var\n        new_ssd = n_noisy * new_var\n        total_ssd = old_ssd + new_ssd + (n_past / float(n_noisy * n_total)) * (n_noisy * mu - n_noisy * new_mu) ** 2\n        total_var = total_ssd / n_total\n\n        return total_mu, total_var\n"}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )\n"}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise BentoMLConfigException(\n            \"Configuration file %s not found.\" % path\n        ) from None\n    with open(path, \"rb\") as f:\n        config = yaml.safe_load(f)\n    return config\n"}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        if self.pk_field is not None:\n            data = self.pk_field.to_internal_value(data)\n        queryset = self.get_queryset()\n        try:\n            if isinstance(data, bool):\n                raise TypeError\n            return queryset.get(pk=data)\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', pk_value=data)\n        except (TypeError, ValueError):\n            self.fail('incorrect_type', data_type=type(data).__name__)\n"}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name.isupper():\n        return DUMMY_CONSTANT.format(name)\n    elif name.islower():\n        return DUMMY_FUNCTION.format(name, backend_name)\n    else:\n        return DUMMY_CLASS.format(name, backend_name)\n"}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        run = self._create_run(\n            command_name, config_updates, named_configs, info, meta_info, options\n        )\n        run()\n        return run\n"}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n        content_word_tf = dict((k, v / content_words_count) for (k, v) in content_words_freq.items())\n        return content_word_tf\n"}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    split_path = path.split(\".\")\n    for i in range(1, len(split_path) + 1):\n        yield join_paths(*split_path[:i])\n"}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names,\n                                   'LoadBalancerNames.member.%d')\n\n        if marker:\n            params['Marker'] = marker\n\n        return self.get_list('DescribeLoadBalancers', params,\n                             [('member', LoadBalancer)])\n"}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        rv: t.Dict[str, str] = {}\n        node: t.Optional[\"Symbols\"] = self\n\n        while node is not None:\n            for name in sorted(node.stores):\n                if name not in rv:\n                    rv[name] = self.find_ref(name)  # type: ignore\n\n            node = node.parent\n\n        return rv\n"}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)\n"}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    for query in sqlparse.split(queries):\n        if query and query_starts_with(query, prefixes) is True:\n            return True\n    return False\n"}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        path = os.path.join(self.working_directory, 'bundles', self.hash)\n        return os.path.normpath(os.path.abspath(path))\n"}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        data = None\n\n        if throughput:\n            self.throughput = throughput\n            data = {\n                'ReadCapacityUnits': int(self.throughput['read']),\n                'WriteCapacityUnits': int(self.throughput['write']),\n            }\n\n        gsi_data = None\n\n        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n        if throughput or global_indexes:\n            self.connection.update_table(\n                self.table_name,\n                provisioned_throughput=data,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide either the throughput or the ' \\\n                  'global_indexes to update method'\n            boto.log.error(msg)\n\n            return False\n"}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if key in self:\n            dict.__delitem__(self.inv, self[key])\n            return dict.pop(self, key)\n        if default is not _MISSING:\n            return default\n        raise KeyError()\n"}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    if args_verbosity_level:\n        result = base_verbosity_level + args_verbosity_level\n    else:\n        result = base_verbosity_level + (logging_config[\"verbosity\"] or 0)\n\n    if result < min(LOG_LEVELS.keys()):\n        result = min(LOG_LEVELS.keys())\n    if result > max(LOG_LEVELS.keys()):\n        result = max(LOG_LEVELS.keys())\n\n    return result\n"}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._normalized_resolved_dependencies\n        )\n"}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    from mrjob.py2 import to_unicode\n    if isinstance(stderr, bytes):\n        stderr = BytesIO(stderr)\n\n    if counters is None:\n        counters = {}\n    statuses = []\n    other = []\n\n    for line in stderr:\n        m = _COUNTER_RE.match(line.rstrip(b'\\r\\n'))\n        if m:\n            group, counter, amount_str = m.groups()\n\n            # don't leave these as bytes on Python 3\n            group = to_unicode(group)\n            counter = to_unicode(counter)\n\n            counters.setdefault(group, {})\n            counters[group].setdefault(counter, 0)\n            counters[group][counter] += int(amount_str)\n            continue\n\n        m = _STATUS_RE.match(line.rstrip(b'\\r\\n'))\n        if m:\n            # don't leave as bytes on Python 3\n            statuses.append(to_unicode(m.group(1)))\n            continue\n\n        other.append(to_unicode(line))\n\n    return {'counters': counters, 'statuses': statuses, 'other': other}\n"}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )\n"}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    input_size = _validate_positive_int(input_size, 'input_size', strictly_positive=False)\n    chunk_size = _validate_positive_int(chunk_size, 'chunk_size')\n    input_offset = _validate_positive_int(input_offset, 'input_offset', strictly_positive=False)\n    overlap_size = _validate_positive_int(overlap_size, 'overlap_size', strictly_positive=False)\n\n    input_stop = input_offset + input_size\n\n    if align:\n        initial_chunk_len = chunk_size - input_offset % (chunk_size - overlap_size)\n        if initial_chunk_len != overlap_size:\n            yield (input_offset, min(input_offset + initial_chunk_len, input_stop))\n            if input_offset + initial_chunk_len >= input_stop:\n                return\n            input_offset = input_offset + initial_chunk_len - overlap_size\n\n    for i in range(input_offset, input_stop, chunk_size - overlap_size):\n        yield (i, min(i + chunk_size, input_stop))\n\n        if i + chunk_size >= input_stop:\n            return\n"}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    dirs = {\n        \"XDG_CACHE_DIR\": pathlib.Path(\n            os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\")\n        ).expanduser(),\n        \"XDG_CONFIG_DIR\": pathlib.Path(\n            os.getenv(\"XDG_CONFIG_HOME\", \"~/.config\")\n        ).expanduser(),\n        \"XDG_DATA_DIR\": pathlib.Path(\n            os.getenv(\"XDG_DATA_HOME\", \"~/.local/share\")\n        ).expanduser(),\n    }\n\n    dirs.update(_get_user_dirs(dirs[\"XDG_CONFIG_DIR\"]))\n\n    return dirs\n"}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        now = datetime.now(timezone.utc)\n        created_at = self.created_at.astimezone(timezone.utc)\n\n        delta = humanize.naturaldelta(abs(created_at - now))\n        tense = \"from now\" if now < created_at else \"ago\"\n\n        return f\"{delta} {tense}\"\n"}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if self.has_listeners:\n            # iterating over subscribers assures they get executed\n            [_ for _ in self.subscribers(events, None)]\n"}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        from mrjob.parse import urlparse\n        from mrjob.parse import is_uri\n        all_paths = (path,) + paths\n\n        # if there's a URI, we only care about it and what follows\n        for i in range(len(all_paths), 0, -1):\n            if is_uri(all_paths[i - 1]):\n                scheme, netloc, uri_path = urlparse(all_paths[i - 1])[:3]\n                return '%s://%s%s' % (\n                    scheme, netloc, posixpath.join(\n                        uri_path or '/', *all_paths[i:]))\n        else:\n            return os.path.join(*all_paths)\n"}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels.keys())\n        return self.levels.get(value.lower())\n"}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    if match.get('guesses', False):\n        return Decimal(match['guesses'])\n\n    min_guesses = 1\n    if len(match['token']) < len(password):\n        if len(match['token']) == 1:\n            min_guesses = MIN_SUBMATCH_GUESSES_SINGLE_CHAR\n        else:\n            min_guesses = MIN_SUBMATCH_GUESSES_MULTI_CHAR\n\n    estimation_functions = {\n        'bruteforce': bruteforce_guesses,\n        'dictionary': dictionary_guesses,\n        'spatial': spatial_guesses,\n        'repeat': repeat_guesses,\n        'sequence': sequence_guesses,\n        'regex': regex_guesses,\n        'date': date_guesses,\n    }\n\n    guesses = estimation_functions[match['pattern']](match)\n    match['guesses'] = max(guesses, min_guesses)\n    match['guesses_log10'] = log(match['guesses'], 10)\n\n    return Decimal(match['guesses'])\n"}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data == '' or (self.trim_whitespace and str(data).strip() == ''):\n            if not self.allow_blank:\n                self.fail('blank')\n            return ''\n        return super().run_validation(data)\n"}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        if not isinstance(relative_spec, str):\n            return relative_spec\n        return self._make_spec(relative_spec)\n"}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key_data = self.get_keys()\n        return self.table.delete_item(**key_data)\n"}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        view = self.match(context, request)\n        if hasattr(view, '__permitted__'):\n            return view.__permitted__(context, request)\n        return True\n"}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        def chunks():\n            for m in msgs:\n                if isinstance(m, dict):\n                    if \"flags\" in m:\n                        yield to_bytes(seq_to_parenstr(m[\"flags\"]))\n                    if \"date\" in m:\n                        yield to_bytes('\"%s\"' % datetime_to_INTERNALDATE(m[\"date\"]))\n                    yield _literal(to_bytes(m[\"msg\"]))\n                else:\n                    yield _literal(to_bytes(m))\n\n        msgs = list(chunks())\n\n        return self._raw_command(\n            b\"APPEND\",\n            [self._normalise_folder(folder)] + msgs,\n            uid=False,\n        )\n"}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if sqla_compat._server_default_is_computed(default):\n        return _render_computed(cast(\"Computed\", default), autogen_context)\n    elif sqla_compat._server_default_is_identity(default):\n        return _render_identity(cast(\"Identity\", default), autogen_context)\n    elif isinstance(default, sa_schema.DefaultClause):\n        if isinstance(default.arg, str):\n            default = default.arg\n        else:\n            return _render_potential_expr(\n                default.arg, autogen_context, is_server_default=True\n            )\n\n    if isinstance(default, str) and repr_:\n        default = repr(re.sub(r\"^'|'$\", \"\", default))\n\n    return cast(str, default)\n"}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()\n"}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        if note >= self.range[0] and note <= self.range[1]:\n            return True\n        return False\n"}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    table = _lcs(x, y)\n    n, m = _get_index_of_lcs(x, y)\n    return table[n, m]\n"}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        self.check(epsilon, delta)\n        self.__spent_budget.append((epsilon, delta))\n        return self\n"}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    result = list(pipe(records, filterz(_by_timeframe(from_date, to_date)), filterz(_by_role_arns(arns_to_filter_for))))\n    if not result and records:\n        logging.warning(ALL_RECORDS_FILTERED)\n\n    return result\n"}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = []\n    plugin_to_distinfo = dict(pm.list_plugin_distinfo())\n    for plugin in pm.get_plugins():\n        plugin_info = {\n            \"name\": plugin.__name__,\n            \"hooks\": [h.name for h in pm.get_hookcallers(plugin)],\n        }\n        distinfo = plugin_to_distinfo.get(plugin)\n        if distinfo:\n            plugin_info[\"version\"] = distinfo.version\n            plugin_info[\"name\"] = distinfo.project_name\n        plugins.append(plugin_info)\n    return plugins\n"}
{"namespace": "datasette.utils.to_css_class", "completion": "    if css_class_re.match(s):\n        return s\n    md5_suffix = hashlib.md5(s.encode(\"utf8\")).hexdigest()[:6]\n    # Strip leading _, -\n    s = s.lstrip(\"_\").lstrip(\"-\")\n    # Replace any whitespace with hyphens\n    s = \"-\".join(s.split())\n    # Remove any remaining invalid characters\n    s = css_invalid_chars_re.sub(\"\", s)\n    # Attach the md5 suffix\n    bits = [b for b in (s, md5_suffix) if b]\n    return \"-\".join(bits)\n"}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    items = get_items(d, keys)\n    return items[-1] if items else (None, None, None)\n"}
{"namespace": "pythonforandroid.util.move", "completion": "    LOGGER.debug(\"Moving {} to {}\".format(source, destination))\n    shutil.move(source, destination)\n"}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = self._database.get_tables()\n        if self._include_views:\n            tables += self.views\n        return tables\n"}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    converted = CAPITALS.sub(lambda m: '_' + m.groups()[0].lower(), name)\n    return converted[1:] if converted[0] == '_' else converted\n"}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    result = {}\n\n    for key in keys:\n        if \".\" in key:\n            field, subfield = key.split(\".\", 1)\n            if isinstance(d.get(field), collections_abc.Mapping):\n                subvalue = dict_subset(d[field], [subfield])\n                result[field] = dict_merge(subvalue, result.get(field, {}))\n            elif field in d:\n                result[field] = d[field]\n        else:\n            if key in d:\n                result[key] = d[key]\n\n    return result\n"}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        ret_list = []\n        for k, v in self.iteritems(multi=True):\n            key = quote_query_part(to_unicode(k), full_quote=full_quote)\n            if v is None:\n                ret_list.append(key)\n            else:\n                val = quote_query_part(to_unicode(v), full_quote=full_quote)\n                ret_list.append(u'='.join((key, val)))\n        return u'&'.join(ret_list)\n"}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            dom = skip(last, 5)\n            a = interval_diff(last, dom, 8) + acc\n            res.append(tuple_to_string((dom, a, \"dom7\")))\n            last = next\n    return res\n"}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n"}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        if self.octave < 3:\n            res = self.name\n        else:\n            res = str.lower(self.name)\n        o = self.octave - 3\n        while o < -1:\n            res += \",\"\n            o += 1\n        while o > 0:\n            res += \"'\"\n            o -= 1\n        return res\n"}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        elif not isinstance(source, str):\n            return source\n\n        new_lines = []\n\n        for line in source.splitlines():\n            for pattern, transform in self.re_patterns:\n                m = pattern.match(line)\n                if m:\n                    new_lines.append(transform(self, m))\n                    break\n            else:\n                new_lines.append(line)\n\n        return \"\\n\".join(new_lines)\n"}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys})'\n        return repr_str\n"}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        paths = set()\n\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                paths.add(path)\n\n        for path_type, path in self._name_to_typed_path.values():\n            if type is None or path_type == type:\n                paths.add(path)\n\n        return paths\n"}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        content_body = {\n            u'jsonrpc': u'2.0',\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        json_content = json.dumps(content_body, sort_keys=True)\n        header = self.HEADER.format(str(len(json_content)))\n        try:\n            self.stream.write(header.encode(u'ascii'))\n            self.stream.write(json_content.encode(self.encoding))\n            self.stream.flush()\n\n        except ValueError as ex:\n            logger.debug(u'Send Request encountered exception %s', ex)\n            raise\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_token is True\"\n            )\n\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_end_token is True\"\n            )\n\n    w_len = len(window)\n    if w_len == 0:\n        return np.nan\n    prob = 1\n\n    cur = window[0]\n    if use_start_token:\n        prob *= trans_probs[start_token][cur]\n    else:\n        prob *= prior_probs[cur]\n\n    for i in range(1, w_len):\n        prev, cur = window[i - 1], window[i]\n        prob *= trans_probs[prev][cur]\n\n    if use_end_token:\n        prob *= trans_probs[cur][end_token]\n\n    return prob\n"}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        return self.client_accepts('application/x-msgpack') or self.client_accepts(\n            'application/msgpack'\n        )\n"}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        scheme = self.scheme\n        path = u'/'.join([quote_path_part(p, full_quote=full_quote)\n                          for p in self.path_parts])\n        authority = self.get_authority(full_quote=full_quote,\n                                       with_userinfo=True)\n        query_string = self.query_params.to_text(full_quote=full_quote)\n        fragment = quote_fragment_part(self.fragment, full_quote=full_quote)\n\n        parts = []\n        _add = parts.append\n        if scheme:\n            _add(scheme)\n            _add(':')\n        if authority:\n            _add('//')\n            _add(authority)\n        elif (scheme and path[:2] != '//' and self.uses_netloc):\n            _add('//')\n        if path:\n            if scheme and authority and path[:1] != '/':\n                _add('/')\n                # TODO: i think this is here because relative paths\n                # with absolute authorities = undefined\n            _add(path)\n        if query_string:\n            _add('?')\n            _add(query_string)\n        if fragment:\n            _add('#')\n            _add(fragment)\n        return u''.join(parts)\n"}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        if not isinstance(tb_str, text):\n            tb_str = tb_str.decode('utf-8')\n        tb_lines = tb_str.lstrip().splitlines()\n\n        # First off, handle some ignored exceptions. These can be the\n        # result of exceptions raised by __del__ during garbage\n        # collection\n        while tb_lines:\n            cl = tb_lines[-1]\n            if cl.startswith('Exception ') and cl.endswith('ignored'):\n                tb_lines.pop()\n            else:\n                break\n        if tb_lines and tb_lines[0].strip() == 'Traceback (most recent call last):':\n            start_line = 1\n            frame_re = _frame_re\n        elif len(tb_lines) > 1 and tb_lines[-2].lstrip().startswith('^'):\n            # This is to handle the slight formatting difference\n            # associated with SyntaxErrors, which also don't really\n            # have tracebacks\n            start_line = 0\n            frame_re = _se_frame_re\n        else:\n            raise ValueError('unrecognized traceback string format')\n\n        frames = []\n        line_no = start_line\n        while True:\n            frame_line = tb_lines[line_no].strip()\n            frame_match = frame_re.match(frame_line)\n            if frame_match:\n                frame_dict = frame_match.groupdict()\n                try:\n                    next_line = tb_lines[line_no + 1]\n                except IndexError:\n                    # We read what we could\n                    next_line = ''\n                next_line_stripped = next_line.strip()\n                if (\n                        frame_re.match(next_line_stripped) or\n                        # The exception message will not be indented\n                        # This check is to avoid overrunning on eval-like\n                        # tracebacks where the last frame doesn't have source\n                        # code in the traceback\n                        not next_line.startswith(' ')\n                ):\n                    frame_dict['source_line'] = ''\n                else:\n                    frame_dict['source_line'] = next_line_stripped\n                    line_no += 1\n            else:\n                break\n            line_no += 1\n            frames.append(frame_dict)\n\n        try:\n            exc_line = '\\n'.join(tb_lines[line_no:])\n            exc_type, _, exc_msg = exc_line.partition(': ')\n        except Exception:\n            exc_type, exc_msg = '', ''\n\n        return cls(exc_type, exc_msg, frames)\n"}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if isinstance(dotted, str):\n            package = self.package\n            if package is CALLER_PACKAGE:\n                package = caller_package()\n            return self._resolve(dotted, package)\n        return dotted\n"}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    reversed_password = ''.join(reversed(password))\n    matches = dictionary_match(reversed_password, _ranked_dictionaries)\n    for match in matches:\n        match['token'] = ''.join(reversed(match['token']))\n        match['reversed'] = True\n        match['i'], match['j'] = len(password) - 1 - match['j'], \\\n                                 len(password) - 1 - match['i']\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n"}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        from diffprivlib.utils import PrivacyLeakWarning\n        from diffprivlib.utils import check_random_state\n        self._validate_params()\n        self.accountant.check(self.epsilon, 0)\n\n        if sample_weight is not None:\n            self._warn_unused_args(\"sample_weight\")\n\n        del y\n\n        random_state = check_random_state(self.random_state)\n\n        X = self._validate_data(X, accept_sparse=False, dtype=[np.float64, np.float32])\n        n_samples, n_dims = X.shape\n\n        if n_samples < self.n_clusters:\n            raise ValueError(f\"n_samples={n_samples} should be >= n_clusters={self.n_clusters}\")\n\n        iters = self._calc_iters(n_dims, n_samples)\n\n        if self.bounds is None:\n            warnings.warn(\"Bounds have not been specified and will be calculated on the data provided.  This will \"\n                          \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                          \"privacy leakage, specify `bounds` for each dimension.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        self.bounds = self._check_bounds(self.bounds, n_dims, min_separation=1e-5)\n        X = self._clip_to_bounds(X, self.bounds)\n\n        centers = self._init_centers(n_dims, random_state=random_state)\n        labels = None\n        distances = None\n\n        # Run _update_centers first to ensure consistency of `labels` and `centers`, since convergence unlikely\n        for _ in range(-1, iters):\n            if labels is not None:\n                centers = self._update_centers(X, centers=centers, labels=labels, dims=n_dims, total_iters=iters,\n                                               random_state=random_state)\n\n            distances, labels = self._distances_labels(X, centers)\n\n        self.cluster_centers_ = centers\n        self.labels_ = labels\n        self.inertia_ = distances[np.arange(len(labels)), labels].sum()\n        self.n_iter_ = iters\n\n        self.accountant.spend(self.epsilon, 0)\n\n        return self\n"}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        try:\n            return_code = self.invoke_hadoop(\n                ['fs', '-ls', path_glob],\n                ok_returncodes=[0, -1, 255],\n                ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n\n            return (return_code == 0)\n        except CalledProcessError:\n            raise IOError(\"Could not check path %s\" % path_glob)\n"}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "        from .tools.align import align as aligner\n        from .tools.normalize import normalize as normalizer\n        from .tools.reduce import reduce as reducer\n        if data is None:\n            return self.xform_data\n        else:\n            formatted = format_data(\n                data,\n                semantic=self.semantic,\n                vectorizer=self.vectorizer,\n                corpus=self.corpus,\n                ppca=True)\n            norm = normalizer(formatted, normalize=self.normalize)\n            reduction = reducer(\n                norm,\n                reduce=self.reduce,\n                ndims=self.reduce['params']['n_components'])\n            return aligner(reduction, align=self.align)\n"}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if has_ipv6 and re.match(r\"\\d+.\\d+.\\d+.\\d+\", hostname) is not None:\n        hostname = f\"::ffff:{hostname}\"\n    return hostname\n"}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        from wal_e.exception import UserCritical\n        if self.closed:\n            raise UserCritical(msg='attempt to upload tar after closing',\n                               hint='report a bug')\n\n        while True:\n            too_many = (\n                self.concurrency_burden + 1 > self.max_concurrency\n                or self.member_burden + len(tpart) > self.max_members\n            )\n\n            if too_many:\n                # If there are not enough resources to start an upload\n                # even with zero uploads in progress, then something\n                # has gone wrong: the user should not be given enough\n                # rope to hang themselves in this way.\n                if self.concurrency_burden == 0:\n                    raise UserCritical(\n                        msg=('not enough resources in pool to '\n                             'support an upload'),\n                        hint='report a bug')\n\n                # _wait blocks until an upload finishes and clears its\n                # used resources, after which another attempt to\n                # evaluate scheduling resources for another upload\n                # might be worth evaluating.\n                #\n                # Alternatively, an error was encountered in a\n                # previous upload in which case it'll be raised here\n                # and cause the process to regard the upload as a\n                # failure.\n                self._wait()\n                gc.collect()\n            else:\n                # Enough resources available: commence upload\n                self._start(tpart)\n                return\n"}
{"namespace": "mingus.core.keys.get_notes", "completion": "    if key in _key_cache:\n        return _key_cache[key]\n    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n    result = []\n\n    # Calculate notes\n    altered_notes = [x[0] for x in get_key_signature_accidentals(key)]\n\n    if get_key_signature(key) < 0:\n        symbol = \"b\"\n    elif get_key_signature(key) > 0:\n        symbol = \"#\"\n\n    raw_tonic_index = base_scale.index(key.upper()[0])\n\n    for note in islice(cycle(base_scale), raw_tonic_index, raw_tonic_index + 7):\n        if note in altered_notes:\n            result.append(\"%s%s\" % (note, symbol))\n        else:\n            result.append(note)\n\n    # Save result to cache\n    _key_cache[key] = result\n    return result\n"}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        chosen_account = self.app.get_accounts(username=self.username)\n        if chosen_account:\n            self.result = self.app.acquire_token_silent_with_error(\n                scopes=self.scopes, account=chosen_account[0]\n            )\n            if not self.result:\n                self.result = self._app_auth(self.auth_type)\n        else:\n            self.result = self._app_auth(self.auth_type)\n        self.refresh_token()\n"}
{"namespace": "mackup.utils.delete", "completion": "    remove_acl(filepath)\n\n    # Some files have immutable attributes, let's remove them recursively\n    remove_immutable_attribute(filepath)\n\n    # Finally remove the files and folders\n    if os.path.isfile(filepath) or os.path.islink(filepath):\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n"}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        json_data = data.copy()\n        if kwargs:\n            for key, val in kwargs.items():\n                json_data[key] = val\n\n        c = cls(**json_data)\n        c._json = data\n        return c\n"}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.setdefault(x, [])\n            if x is not y and y not in L:\n                L.append(y)\n"}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        tagged_commands = self._imap.tagged_commands\n        resps = []\n        while True:\n            line = self._imap._get_response()\n            if tagged_commands[tag]:\n                break\n            resps.append(_parse_untagged_response(line))\n        typ, data = tagged_commands.pop(tag)\n        self._checkok(command, typ, data)\n        return data[0], resps\n"}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    if requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    if not path.startswith(\"/\"):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with a leading / are not supported.\n            You supplied: {path}\n            \"\"\"\n        )\n    return \"/\".join([requests_pathname.rstrip(\"/\"), path.lstrip(\"/\")])\n"}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        batch: ext.NpNDArray = np.concatenate(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n"}
{"namespace": "boto.rds.connect_to_region", "completion": "    from boto.regioninfo import connect\n    return connect('rds', region_name, region_cls=RDSRegionInfo,\n                   connection_cls=RDSConnection, **kw_params)\n"}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        if self.prefix_character:\n            prefix = [\" \", self.prefix_character, \" \"]\n        else:\n            prefix = [\" \"]\n        if count_text[1]:\n            suffix = [\" \", count_text, \" \"]\n        else:\n            suffix = [\"  \"]\n        self.button_prefix.set_text(prefix)\n        self.set_label(self._caption)\n        self.button_suffix.set_text(suffix)\n        self._w.set_attr_map({None: text_color})\n"}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)\n"}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table:\n            dependencies = [table]\n            if table in self._models:\n                model_class = self._models[table]\n                dependencies.extend([\n                    related._meta.table_name for _, related, _ in\n                    model_class._meta.model_graph()])\n            else:\n                dependencies.extend(self.get_table_dependencies(table))\n        else:\n            dependencies = None  # Update all tables.\n            self._models = {}\n        updated = self._introspector.generate_models(\n            skip_invalid=True,\n            table_names=dependencies,\n            literal_column_names=True,\n            include_views=self._include_views)\n        self._models.update(updated)\n"}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    if now is None:\n        now = datetime.now(timezone.utc)\n\n    tweets = []\n    for line in raw_tweets:\n        try:\n            tweet = parse_tweet(line, source, now)\n        except (ValueError, OverflowError) as e:\n            logger.debug(\"{0} - {1}\".format(source.url, e))\n        else:\n            tweets.append(tweet)\n\n    return tweets\n"}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        for path in unique(self._hadoop_streaming_jar_dirs()):\n            log.info('Looking for Hadoop streaming jar in %s...' % path)\n\n            streaming_jars = []\n            for path in self.fs.ls(path):\n                if _HADOOP_STREAMING_JAR_RE.match(posixpath.basename(path)):\n                    streaming_jars.append(path)\n\n            if streaming_jars:\n                # prefer shorter names and shallower paths\n                def sort_key(p):\n                    return (len(p.split('/')),\n                            len(posixpath.basename(p)),\n                            p)\n\n                streaming_jars.sort(key=sort_key)\n\n                return streaming_jars[0]\n\n        return None\n"}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if self._bounded_stream is None:\n            self._bounded_stream = self._get_wrapped_wsgi_input()\n\n        return self._bounded_stream\n"}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        ob = self.subs[name]\n        return ob\n"}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n\n        version = self.get_hadoop_version()\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-R', '-f', '-skipTrash', path_glob]\n        else:\n            args = ['fs', '-rmr', '-skipTrash', path_glob]\n\n        try:\n            self.invoke_hadoop(\n                args,\n                return_stdout=True, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not rm %s\" % path_glob)\n"}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        if not self.nodes:\n            raise ValueError(\"Fitting Tree must be built before calling fit().\")\n\n        leaves = self.apply(X)\n        unique_leaves = np.unique(leaves)\n        values = np.zeros(shape=(self.node_count, 1, len(self.classes)))\n\n        # Populate value of real leaves\n        for leaf in unique_leaves:\n            idxs = (leaves == leaf)\n            leaf_y = y[idxs]\n\n            counts = [np.sum(leaf_y == cls) for cls in self.classes]\n            mech = PermuteAndFlip(epsilon=self.epsilon, sensitivity=1, monotonic=True, utility=counts,\n                                  random_state=self.random_state)\n            values[leaf, 0, mech.randomise()] = 1\n\n        # Populate value of empty leaves\n        for node in self.nodes:\n            if values[node.node_id].sum() or node.left_child != self._TREE_LEAF:\n                continue\n\n            values[node.node_id, 0, self.random_state.randint(len(self.classes))] = 1\n\n        self.values_ = values\n\n        return self\n"}
{"namespace": "gunicorn.sock.create_sockets", "completion": "    listeners = []\n\n    # get it only once\n    addr = conf.address\n    fdaddr = [bind for bind in addr if isinstance(bind, int)]\n    if fds:\n        fdaddr += list(fds)\n    laddr = [bind for bind in addr if not isinstance(bind, int)]\n\n    # check ssl config early to raise the error on startup\n    # only the certfile is needed since it can contains the keyfile\n    if conf.certfile and not os.path.exists(conf.certfile):\n        raise ValueError('certfile \"%s\" does not exist' % conf.certfile)\n\n    if conf.keyfile and not os.path.exists(conf.keyfile):\n        raise ValueError('keyfile \"%s\" does not exist' % conf.keyfile)\n\n    # sockets are already bound\n    if fdaddr:\n        for fd in fdaddr:\n            sock = socket.fromfd(fd, socket.AF_UNIX, socket.SOCK_STREAM)\n            sock_name = sock.getsockname()\n            sock_type = _sock_type(sock_name)\n            listener = sock_type(sock_name, conf, log, fd=fd)\n            listeners.append(listener)\n\n        return listeners\n\n    # no sockets is bound, first initialization of gunicorn in this env.\n    for addr in laddr:\n        sock_type = _sock_type(addr)\n        sock = None\n        for i in range(5):\n            try:\n                sock = sock_type(addr, conf, log)\n            except socket.error as e:\n                if e.args[0] == errno.EADDRINUSE:\n                    log.error(\"Connection in use: %s\", str(addr))\n                if e.args[0] == errno.EADDRNOTAVAIL:\n                    log.error(\"Invalid address: %s\", str(addr))\n                if i < 5:\n                    msg = \"connection to {addr} failed: {error}\"\n                    log.debug(msg.format(addr=str(addr), error=str(e)))\n                    log.error(\"Retrying in 1 second.\")\n                    time.sleep(1)\n            else:\n                break\n\n        if sock is None:\n            log.error(\"Can't connect to %s\", str(addr))\n            sys.exit(1)\n\n        listeners.append(sock)\n\n    return listeners\n"}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 1)\n"}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {} () (Array (_ BitVec {}) (_ BitVec {})))\".format(self.name, self.key_size,\n                                                                                self.value_size)\n"}
{"namespace": "mingus.core.intervals.determine", "completion": "    if note1[0] == note2[0]:\n\n        def get_val(note):\n            \"\"\"Private function: count the value of accidentals.\"\"\"\n            r = 0\n            for x in note[1:]:\n                if x == \"b\":\n                    r -= 1\n                elif x == \"#\":\n                    r += 1\n            return r\n\n        x = get_val(note1)\n        y = get_val(note2)\n        if x == y:\n            if not shorthand:\n                return \"major unison\"\n            return \"1\"\n        elif x < y:\n            if not shorthand:\n                return \"augmented unison\"\n            return \"#1\"\n        elif x - y == 1:\n            if not shorthand:\n                return \"minor unison\"\n            return \"b1\"\n        else:\n            if not shorthand:\n                return \"diminished unison\"\n            return \"bb1\"\n\n    # Other intervals\n    n1 = notes.fifths.index(note1[0])\n    n2 = notes.fifths.index(note2[0])\n    number_of_fifth_steps = n2 - n1\n    if n2 < n1:\n        number_of_fifth_steps = len(notes.fifths) - n1 + n2\n\n    # [name, shorthand_name, half notes for major version of this interval]\n    fifth_steps = [\n        [\"unison\", \"1\", 0],\n        [\"fifth\", \"5\", 7],\n        [\"second\", \"2\", 2],\n        [\"sixth\", \"6\", 9],\n        [\"third\", \"3\", 4],\n        [\"seventh\", \"7\", 11],\n        [\"fourth\", \"4\", 5],\n    ]\n\n    # Count half steps between note1 and note2\n    half_notes = measure(note1, note2)\n\n    # Get the proper list from the number of fifth steps\n    current = fifth_steps[number_of_fifth_steps]\n\n    # maj = number of major steps for this interval\n    maj = current[2]\n\n    # if maj is equal to the half steps between note1 and note2 the interval is\n    # major or perfect\n    if maj == half_notes:\n        # Corner cases for perfect fifths and fourths\n        if current[0] == \"fifth\":\n            if not shorthand:\n                return \"perfect fifth\"\n        elif current[0] == \"fourth\":\n            if not shorthand:\n                return \"perfect fourth\"\n        if not shorthand:\n            return \"major \" + current[0]\n        return current[1]\n    elif maj + 1 <= half_notes:\n        # if maj + 1 is equal to half_notes, the interval is augmented.\n        if not shorthand:\n            return \"augmented \" + current[0]\n        return \"#\" * (half_notes - maj) + current[1]\n    elif maj - 1 == half_notes:\n        # etc.\n        if not shorthand:\n            return \"minor \" + current[0]\n        return \"b\" + current[1]\n    elif maj - 2 >= half_notes:\n        if not shorthand:\n            return \"diminished \" + current[0]\n        return \"b\" * (maj - half_notes) + current[1]\n"}
{"namespace": "dash.development.component_loader.load_components", "completion": "    from .base_component import ComponentRegistry\n    ComponentRegistry.registry.add(namespace)\n    components = []\n\n    data = _get_metadata(metadata_path)\n\n    # Iterate over each property name (which is a path to the component)\n    for componentPath in data:\n        componentData = data[componentPath]\n\n        # Extract component name from path\n        # e.g. src/components/MyControl.react.js\n        # TODO Make more robust - some folks will write .jsx and others\n        # will be on windows. Unfortunately react-docgen doesn't include\n        # the name of the component atm.\n        name = componentPath.split(\"/\").pop().split(\".\")[0]\n        component = generate_class(\n            name, componentData[\"props\"], componentData[\"description\"], namespace, None\n        )\n\n        components.append(component)\n\n    return components\n"}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    method_map = {}\n\n    for method in constants.COMBINED_METHODS:\n        try:\n            responder_name = 'on_' + method.lower()\n            if suffix:\n                responder_name += '_' + suffix\n\n            responder = getattr(resource, responder_name)\n        except AttributeError:\n            # resource does not implement this method\n            pass\n        else:\n            # Usually expect a method, but any callable will do\n            if callable(responder):\n                method_map[method] = responder\n\n    # If suffix is specified and doesn't map to any methods, raise an error\n    if suffix and not method_map:\n        raise SuffixedMethodNotFoundError(\n            'No responders found for the specified suffix'\n        )\n\n    return method_map\n"}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [\n        key_binding\n        for key_binding in KEY_BINDINGS.values()\n        if not key_binding.get(\"excluded_from_random_tips\", False)\n    ]\n"}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        if not key_name:\n            raise ValueError('Empty key names are not allowed')\n        return self._delete_key_internal(key_name, headers=headers,\n                                         version_id=version_id,\n                                         mfa_token=mfa_token,\n                                         query_args_l=None)\n"}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    _fix = _fix_clear_tags\n\n    if isinstance(x, list):\n        return [_fix(_strip_clear_tag(item)) for item in x]\n\n    elif isinstance(x, dict):\n        d = dict((_fix(k), _fix(v)) for k, v in x.items())\n\n        # handle cleared keys\n        for k, v in list(d.items()):\n            if isinstance(k, ClearedValue):\n                del d[k]\n                d[_strip_clear_tag(k)] = ClearedValue(_strip_clear_tag(v))\n\n        return d\n\n    elif isinstance(x, ClearedValue):\n        return ClearedValue(_fix(x.value))\n\n    else:\n        return x\n"}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    return connect('cognito-identity', region_name,\n                   connection_cls=CognitoIdentityConnection, **kw_params)\n"}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        data = smart_str(data).strip()\n\n        if self.localize:\n            data = sanitize_separators(data)\n\n        if len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            value = decimal.Decimal(data)\n        except decimal.DecimalException:\n            self.fail('invalid')\n\n        if value.is_nan():\n            self.fail('invalid')\n\n        # Check for infinity and negative infinity.\n        if value in (decimal.Decimal('Inf'), decimal.Decimal('-Inf')):\n            self.fail('invalid')\n\n        return self.quantize(self.validate_precision(value))\n"}
{"namespace": "boto.kinesis.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.kinesis.layer1 import KinesisConnection\n    return connect('kinesis', region_name,\n                   connection_cls=KinesisConnection, **kw_params)\n"}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n\n        # While the following line is not strictly necessary, it ensures that the\n        # refresh token is not used elsewhere. And also forces the pre_refresh_callback\n        # to always load the latest refresh_token from the database.\n        authorizer.refresh_token = None\n"}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        k = f'\"{k}\"' if any(i in punctuation for i in k) else k\n        nkey = parent + sep + k if parent else k\n        if isinstance(v, t.MutableMapping):\n            yield from flatten_dict(\n                t.cast(t.MutableMapping[str, t.Any], v), parent=nkey, sep=sep\n            )\n        else:\n            yield nkey, v\n"}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n"}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(\n            tzinfo=datetime.timezone.utc\n        )\n    except (TypeError, ValueError):\n        return s\n"}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.set_caption(self.search_text)\n        self.set_edit_text(\"\")\n"}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        item_index_map = self.item_index_map\n        len_self = len(item_index_map)\n        if index is None or index == -1 or index == len_self - 1:\n            ret = self.item_list.pop()\n            del item_index_map[ret]\n        else:\n            real_index = self._get_real_index(index)\n            ret = self.item_list[real_index]\n            self.item_list[real_index] = _MISSING\n            del item_index_map[ret]\n            self._add_dead(real_index)\n        self._cull()\n        return ret\n"}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "        if not self._spark_submit_bin:\n            self._spark_submit_bin = self._find_spark_submit_bin()\n        return self._spark_submit_bin\n"}
{"namespace": "jc.parsers.os_release.parse", "completion": "    jc.utils.compatibility(__name__, info.compatible, quiet)\n    raw_output = jc.parsers.kv.parse(data, raw, quiet)\n\n    return raw_output if raw else _process(raw_output)\n"}
{"namespace": "boto.sts.connect_to_region", "completion": "    from boto.regioninfo import connect\n    return connect('sts', region_name, connection_cls=STSConnection,\n                   **kw_params)\n"}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.column\n        )\n"}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    j = combine_dicts(*jobconfs)\n\n    return {k: _to_java_str(v) for k, v in j.items() if v is not None}\n"}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    for line in html_bytes.splitlines():\n        m = _RESOURCE_MANAGER_JS_RE.match(line)\n        if m:\n            return float(m.group('percent'))\n\n    return None\n"}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        from pyinfra.api.exceptions import InventoryError\n        mech_ssh_info = get_mech_config(limit)\n\n        logger.debug(\"Got Mech SSH info: \\n%s\", mech_ssh_info)\n\n        hosts = []\n        current_host = None\n\n        for line in mech_ssh_info:\n            if not line:\n                if current_host:\n                    hosts.append(_make_name_data(current_host))\n\n                current_host = None\n                continue\n\n            key, value = line.strip().split(\" \", 1)\n\n            if key == \"Host\":\n                if current_host:\n                    hosts.append(_make_name_data(current_host))\n\n                # Set the new host\n                current_host = {\n                    key: value,\n                }\n\n            elif current_host:\n                current_host[key] = value\n\n            else:\n                logger.debug(\"Extra Mech SSH key/value (%s=%s)\", key, value)\n\n        if current_host:\n            hosts.append(_make_name_data(current_host))\n\n        if not hosts:\n            raise InventoryError(\"No running Mech instances found!\")\n\n        return hosts\n"}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        assert authorizer.refresh_token is None\n        authorizer.refresh_token = self._get()\n"}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    if not detect_elf_binary(resolve_binary(binary)):\n        raise InvalidElfBinaryError('The \"%s\" file is not a binary ELF file.' % binary)\n\n    process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()\n    return stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')\n"}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        Logger.access(self, resp, req, environ, request_time)\n        duration_in_ms = request_time.seconds * 1000 + float(request_time.microseconds) / 10 ** 3\n        status = resp.status\n        if isinstance(status, str):\n            status = int(status.split(None, 1)[0])\n        self.histogram(\"gunicorn.request.duration\", duration_in_ms)\n        self.increment(\"gunicorn.requests\", 1)\n        self.increment(\"gunicorn.request.status.%d\" % status, 1)\n"}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        settings = self.registry.settings\n        if settings is None:\n            settings = {}\n        return settings\n"}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        already_set = request in self._store\n        self._store[request] = value\n\n        # avoid registering the callback more than once\n        if not already_set:\n            request.add_finished_callback(self._store.pop)\n"}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    def handle_mention(match):\n        name, url = match.groups()\n        return format_callback(name, url)\n\n    return mention_re.sub(handle_mention, text)\n"}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    lookup_tables = {}\n    for column, (table_name, value_column) in foreign_keys.items():\n        # Now apply this to the dataframes\n        for dataframe in dataframes:\n            if column in dataframe.columns:\n                lookup_table = lookup_tables.get(table_name)\n                if lookup_table is None:\n                    lookup_table = LookupTable(\n                        conn=conn,\n                        table_name=table_name,\n                        value_column=value_column,\n                        index_fts=index_fts,\n                    )\n                    lookup_tables[table_name] = lookup_table\n                dataframe[column] = dataframe[column].apply(lookup_table.id_for_value)\n    return dataframes\n"}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n\n    if sql:\n        destination_revs = []\n        starting_rev = None\n        for _revision in util.to_list(revision):\n            if \":\" in _revision:\n                srev, _revision = _revision.split(\":\", 2)\n\n                if starting_rev != srev:\n                    if starting_rev is None:\n                        starting_rev = srev\n                    else:\n                        raise util.CommandError(\n                            \"Stamp operation with --sql only supports a \"\n                            \"single starting revision at a time\"\n                        )\n            destination_revs.append(_revision)\n    else:\n        destination_revs = util.to_list(revision)\n\n    def do_stamp(rev, context):\n        return script._stamp_revs(util.to_tuple(destination_revs), rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_stamp,\n        as_sql=sql,\n        starting_rev=starting_rev if sql else None,\n        destination_rev=util.to_tuple(destination_revs),\n        tag=tag,\n        purge=purge,\n    ):\n        script.run_env()\n"}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        final_data = {}\n\n        for key, value in self._data.items():\n            if not self._is_storable(value):\n                continue\n\n            final_data[key] = self._dynamizer.encode(value)\n\n        return final_data\n"}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for json_name, param_name in CHANNEL_PARAMS.items():\n            value = resp.get(json_name)\n            if value is not None:\n                setattr(self, param_name, value)\n"}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        state = None\n        if self.attach_data:\n            state = self.attach_data.status\n        return state\n"}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    from .constants import ANSI_RESET\n    from .constants import ANSI_CURSOR_UP\n    previous_line_count = 0\n    remaining_loops = num_loops or None\n\n    try:\n        while remaining_loops is None or remaining_loops > 0:\n            for txt_frame in txt_frames:\n                stdout.write(ANSI_CURSOR_UP * previous_line_count)\n                stdout.write(txt_frame)\n                stdout.write('\\n')\n                stdout.flush()\n                previous_line_count = len(txt_frames[0].split('\\n'))\n                time.sleep(seconds_per_frame)\n\n            if remaining_loops is not None:\n                remaining_loops -= 1\n        stdout.write(ANSI_RESET)\n    except KeyboardInterrupt:\n        # ensure styling is reset\n        stdout.write(ANSI_RESET)\n        # we'll want an extra new line if CTRL+C was pressed\n        stdout.write('\\n')\n\n    stdout.flush()\n"}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        target_data = self.command_prefix.split('-')\n        return '{triplet}{ndk_api}'.format(\n            triplet='-'.join(['armv7a', target_data[1], target_data[2]]),\n            ndk_api=self.ctx.ndk_api,\n        )\n"}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for k in keys:\n            key_path = os.path.join(\"/\", k.strip(\"/\"))\n            os.remove(key_path)\n        # deleting files can leave empty dirs => trim them\n        common_path = os.path.join(\"/\", common_dir_path(keys).strip(\"/\"))\n        remove_empty_dirs(common_path)\n"}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for prefix, sink, _ in app._sinks:\n        source_info, name = _get_source_info_and_name(sink)\n        info = SinkInfo(prefix.pattern, name, source_info)\n        sinks.append(info)\n    return sinks\n"}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if isinstance(hex, bytes):\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(hex)\n"}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            self._cached_prefix = self.scheme + '://' + self.netloc + self.app\n\n        return self._cached_prefix\n"}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                log.debug('Recursively deleting %s' % path)\n                shutil.rmtree(path)\n            else:\n                log.debug('Deleting %s' % path)\n                os.remove(path)\n"}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )\n"}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list\n"}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if type(iterable) is type(self):\n            other = iterable\n            for k in other.data:\n                if k not in self.data:\n                    self.data[k] = other.data[k]\n                else:\n                    self.data[k].update(other.data[k])\n            for k in other.inv.data:\n                if k not in self.inv.data:\n                    self.inv.data[k] = other.inv.data[k]\n                else:\n                    self.inv.data[k].update(other.inv.data[k])\n        elif callable(getattr(iterable, 'keys', None)):\n            for k in iterable.keys():\n                self.add(k, iterable[k])\n        else:\n            for key, val in iterable:\n                self.add(key, val)\n        return\n"}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)\n"}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        from mrjob.logs.errors import _pick_error_attempt_ids\n        from mrjob.logs.errors import _pick_error\n        logs_needed = self._logs_needed_to_pick_error(step_type)\n\n        if self._read_logs() and not all(\n                log_type in log_interpretation for log_type in logs_needed):\n            log.info('Scanning logs for probable cause of failure...')\n\n            if 'step' in logs_needed:\n                self._interpret_step_logs(log_interpretation, step_type)\n\n            if 'history' in logs_needed:\n                self._interpret_history_log(log_interpretation)\n\n            if 'task' in logs_needed:\n                error_attempt_ids = _pick_error_attempt_ids(log_interpretation)\n\n                self._interpret_task_logs(\n                    log_interpretation, step_type, error_attempt_ids)\n\n        return _pick_error(log_interpretation)\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    from ..utils.laplace_smooth import laplace_smooth_cmd_counts\n    from ..utils.laplace_smooth import laplace_smooth_param_counts\n    cmds: List[str] = list(seq1_counts.keys()) + [unk_token]\n\n    # apply laplace smoothing for cmds\n    seq1_counts_ls, seq2_counts_ls = laplace_smooth_cmd_counts(\n        seq1_counts=copy.deepcopy(seq1_counts),\n        seq2_counts=copy.deepcopy(seq2_counts),\n        start_token=start_token,\n        end_token=end_token,\n        unk_token=unk_token,\n    )\n\n    # apply laplace smoothing for params\n    param_counts_ls, cmd_param_counts_ls = laplace_smooth_param_counts(\n        cmds=cmds,\n        param_counts=copy.deepcopy(param_counts),\n        cmd_param_counts=copy.deepcopy(cmd_param_counts),\n        unk_token=unk_token,\n    )\n\n    seq1_counts_sm = StateMatrix(states=seq1_counts_ls, unk_token=unk_token)\n    seq2_counts_sm = StateMatrix(states=seq2_counts_ls, unk_token=unk_token)\n    param_counts_sm = StateMatrix(states=param_counts_ls, unk_token=unk_token)\n    cmd_param_counts_sm = StateMatrix(states=cmd_param_counts_ls, unk_token=unk_token)\n\n    return seq1_counts_sm, seq2_counts_sm, param_counts_sm, cmd_param_counts_sm\n"}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n\n    for sub in enumerate_l33t_subs(\n            relevant_l33t_subtable(password, _l33t_table)):\n        if not len(sub):\n            break\n\n        subbed_password = translate(password, sub)\n        for match in dictionary_match(subbed_password, _ranked_dictionaries):\n            token = password[match['i']:match['j'] + 1]\n            if token.lower() == match['matched_word']:\n                # only return the matches that contain an actual substitution\n                continue\n\n            # subset of mappings in sub that are in use for this match\n            match_sub = {}\n            for subbed_chr, chr in sub.items():\n                if subbed_chr in token:\n                    match_sub[subbed_chr] = chr\n            match['l33t'] = True\n            match['token'] = token\n            match['sub'] = match_sub\n            match['sub_display'] = ', '.join(\n                [\"%s -> %s\" % (k, v) for k, v in match_sub.items()]\n            )\n            matches.append(match)\n\n    matches = [match for match in matches if len(match['token']) > 1]\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n"}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                binary_type.__name__,\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s)\n"}
{"namespace": "peewee.Index.where", "completion": "        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)\n"}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "    @classmethod\n    def json(cls, body, status=200, headers=None, default=None):\n        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n"}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    deps_were_added_by = dict()\n    deps = set()\n    if blacklist is None:\n        blacklist = set()\n\n    # Add dependencies for all recipes:\n    to_be_added = [(name_tuple, None) for name_tuple in name_tuples]\n    while len(to_be_added) > 0:\n        current_to_be_added = list(to_be_added)\n        to_be_added = []\n        for (added_tuple, adding_recipe) in current_to_be_added:\n            assert type(added_tuple) is tuple\n            if len(added_tuple) > 1:\n                # No obvious commitment in what to add, don't check it itself\n                # but throw it into deps for later comparing against\n                # (Remember this function only catches obvious issues)\n                deps.add(added_tuple)\n                continue\n\n            name = added_tuple[0]\n            recipe_conflicts = set()\n            recipe_dependencies = []\n            try:\n                # Get recipe to add and who's ultimately adding it:\n                recipe = Recipe.get_recipe(name, ctx)\n                recipe_conflicts = {c.lower() for c in recipe.conflicts}\n                recipe_dependencies = get_dependency_tuple_list_for_recipe(\n                    recipe, blacklist=blacklist\n                )\n            except ValueError:\n                pass\n            adder_first_recipe_name = adding_recipe or name\n\n            # Collect the conflicts:\n            triggered_conflicts = []\n            for dep_tuple_list in deps:\n                # See if the new deps conflict with things added before:\n                if set(dep_tuple_list).intersection(\n                       recipe_conflicts) == set(dep_tuple_list):\n                    triggered_conflicts.append(dep_tuple_list)\n                    continue\n\n                # See if what was added before conflicts with the new deps:\n                if len(dep_tuple_list) > 1:\n                    # Not an obvious commitment to a specific recipe/dep\n                    # to be added, so we won't check.\n                    # (remember this function only catches obvious issues)\n                    continue\n                try:\n                    dep_recipe = Recipe.get_recipe(dep_tuple_list[0], ctx)\n                except ValueError:\n                    continue\n                conflicts = [c.lower() for c in dep_recipe.conflicts]\n                if name in conflicts:\n                    triggered_conflicts.append(dep_tuple_list)\n\n            # Throw error on conflict:\n            if triggered_conflicts:\n                # Get first conflict and see who added that one:\n                adder_second_recipe_name = \"'||'\".join(triggered_conflicts[0])\n                second_recipe_original_adder = deps_were_added_by.get(\n                    (adder_second_recipe_name,), None\n                )\n                if second_recipe_original_adder:\n                    adder_second_recipe_name = second_recipe_original_adder\n\n                # Prompt error:\n                raise BuildInterruptingException(\n                    \"Conflict detected: '{}'\"\n                    \" inducing dependencies {}, and '{}'\"\n                    \" inducing conflicting dependencies {}\".format(\n                        adder_first_recipe_name,\n                        (recipe.name,),\n                        adder_second_recipe_name,\n                        triggered_conflicts[0]\n                    ))\n\n            # Actually add it to our list:\n            deps.add(added_tuple)\n            deps_were_added_by[added_tuple] = adding_recipe\n\n            # Schedule dependencies to be added\n            to_be_added += [\n                (dep, adder_first_recipe_name or name)\n                for dep in recipe_dependencies\n                if dep not in deps\n            ]\n    # If we came here, then there were no obvious conflicts.\n    return None\n"}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.edit_text:\n            # Use regular validation if already have text\n            return super().valid_char(ch)\n        elif len(ch) != 1:\n            # urwid expands some unicode to strings to be useful\n            # (so we need to work around eg 'backspace')\n            return False\n        else:\n            # Skip unicode 'Control characters' and 'space Zeperators'\n            # This includes various invalid characters and complex spaces\n            return unicodedata.category(ch) not in (\"Cc\", \"Zs\")\n"}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [\n            \"{}/{}\".format(\n                self.ctx.include_dir,\n                d.format(arch=self))\n            for d in self.ctx.include_dirs]\n"}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "    from boto.regioninfo import connect\n    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return connect('cloudtrail', region_name,\n                   connection_cls=CloudTrailConnection, **kw_params)\n"}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        self._imap.shutdown()\n        logger.info(\"Connection closed\")\n"}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        if kw is None:\n            kw = {}\n        action = extra\n        action.update(\n            dict(\n                discriminator=discriminator,\n                callable=callable,\n                args=args,\n                kw=kw,\n                includepath=includepath,\n                info=info,\n                order=order,\n                introspectables=introspectables,\n            )\n        )\n        self.actions.append(action)\n"}
{"namespace": "oletools.oleid.OleID.check", "completion": "        self.ftg = ftguess.FileTypeGuesser(filepath=self.filename, data=self.data)\n        ftype = self.ftg.ftype\n        # if it's an unrecognized OLE file, display the root CLSID in description:\n        if self.ftg.filetype == ftguess.FTYPE.GENERIC_OLE:\n            description = 'Unrecognized OLE file. Root CLSID: {} - {}'.format(\n                self.ftg.root_clsid, self.ftg.root_clsid_name)\n        else:\n            description = ''\n        ft = Indicator('ftype', value=ftype.longname, _type=str, name='File format', risk=RISK.INFO,\n                       description=description)\n        self.indicators.append(ft)\n        ct = Indicator('container', value=ftype.container, _type=str, name='Container format', risk=RISK.INFO,\n                       description='Container type')\n        self.indicators.append(ct)\n\n        # check if it is actually an OLE file:\n        if self.ftg.container == ftguess.CONTAINER.OLE:\n            # reuse olefile already opened by ftguess\n            self.ole = self.ftg.olefile\n        # oleformat = Indicator('ole_format', True, name='OLE format')\n        # self.indicators.append(oleformat)\n        # if self.ole:\n        #     oleformat.value = True\n        # elif not olefile.isOleFile(self.filename):\n        #     oleformat.value = False\n        #     return self.indicators\n        # else:\n        #     # parse file:\n        #     self.ole = olefile.OleFileIO(self.filename)\n\n        # checks:\n        # TODO: add try/except around each check\n        self.check_properties()\n        self.check_encrypted()\n        self.check_macros()\n        self.check_external_relationships()\n        self.check_object_pool()\n        self.check_flash()\n        if self.ole is not None:\n            self.ole.close()\n        return self.indicators\n"}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = list()\n    excluded_list = excluded_files.split(\",\")\n    for target in targets:\n        if os.path.isdir(target):\n            for root, _, files in os.walk(target):\n                for file in files:\n                    if file.endswith('.py') and file not in excluded_list:\n                        fullpath = os.path.join(root, file)\n                        included_files.append(fullpath)\n                        log.debug('Discovered file: %s', fullpath)\n                if not recursive:\n                    break\n        else:\n            if target not in excluded_list:\n                included_files.append(target)\n                log.debug('Discovered file: %s', target)\n    return included_files\n"}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    if not notes.is_valid_note(note):\n        return False\n\n    # [shorthand, interval function up, interval function down]\n    shorthand_lookup = [\n        [\"1\", major_unison, major_unison],\n        [\"2\", major_second, minor_seventh],\n        [\"3\", major_third, minor_sixth],\n        [\"4\", major_fourth, major_fifth],\n        [\"5\", major_fifth, major_fourth],\n        [\"6\", major_sixth, minor_third],\n        [\"7\", major_seventh, minor_second],\n    ]\n\n    # Looking up last character in interval in shorthand_lookup and calling that\n    # function.\n    val = False\n    for shorthand in shorthand_lookup:\n        if shorthand[0] == interval[-1]:\n            if up:\n                val = shorthand[1](note)\n            else:\n                val = shorthand[2](note)\n\n    # warning Last character in interval should be 1-7\n    if val == False:\n        return False\n\n    # Collect accidentals\n    for x in interval:\n        if x == \"#\":\n            if up:\n                val = notes.augment(val)\n            else:\n                val = notes.diminish(val)\n        elif x == \"b\":\n            if up:\n                val = notes.diminish(val)\n            else:\n                val = notes.augment(val)\n        else:\n            return val\n"}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\"ext.NpNDArray\", pep574_loads(bs, payload.data, indices))\n\n        return pickle.loads(payload.data)\n"}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        from mingus.core import intervals\n        (old, o_octave) = (self.name, self.octave)\n        self.name = intervals.from_shorthand(self.name, interval, up)\n        if up:\n            if self < Note(old, o_octave):\n                self.octave += 1\n        else:\n            if self > Note(old, o_octave):\n                self.octave -= 1\n"}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    router = app._router\n\n    inspect_function = _supported_routers.get(type(router))\n    if inspect_function is None:\n        raise TypeError(\n            'Unsupported router class {}. Use \"register_router\" '\n            'to register a function that can inspect the router '\n            'used by the provided application'.format(type(router))\n        )\n    return inspect_function(router)\n"}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if self.accepts and hasattr(request, 'accept'):\n            views = []\n            for offer, _ in request.accept.acceptable_offers(self.accepts):\n                views.extend(self.media_views[offer])\n            views.extend(self.views)\n            return views\n        return self.views\n"}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError(\"invalid offset {0}, need SEEK_* constant\"\n                             .format(offset))\n\n        # now get to that position, doing reads and resets as necessary\n        if new_pos < 0:\n            # print('ZipSubFile: Error: seek to {}'.format(new_pos))\n            raise IOError('Seek beyond start of file not allowed')\n        elif new_pos == self.pos:\n            # print('ZipSubFile: nothing to do')\n            pass\n        elif new_pos == 0:\n            # print('ZipSubFile: seek to start')\n            self.reset()\n        elif new_pos < self.pos:\n            # print('ZipSubFile: seek back')\n            self.reset()\n            self._seek_skip(new_pos)             # --> read --> update self.pos\n        elif new_pos < self.size:\n            # print('ZipSubFile: seek forward')\n            self._seek_skip(new_pos - self.pos)  # --> read --> update self.pos\n        else:   # new_pos >= self.size\n            # print('ZipSubFile: seek to end')\n            self.pos = new_pos    # fake being at the end; remember pos >= size\n"}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_settings_path = \"Library/Application Support/Copy Agent/config.db\"\n    copy_home = None\n\n    copy_settings = os.path.join(os.environ[\"HOME\"], copy_settings_path)\n\n    if os.path.isfile(copy_settings):\n        database = sqlite3.connect(copy_settings)\n        if database:\n            cur = database.cursor()\n            query = \"SELECT value \" \"FROM config2 \" \"WHERE option = 'csmRootPath';\"\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            cur.close()\n\n    if not copy_home:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Copy install\"))\n\n    return copy_home\n"}
{"namespace": "falcon.request.Request.forwarded", "completion": "        from falcon.forwarded import _parse_forwarded_header\n        if self._cached_forwarded is None:\n            forwarded = self.get_header('Forwarded')\n            if forwarded is None:\n                return None\n\n            self._cached_forwarded = _parse_forwarded_header(forwarded)\n\n        return self._cached_forwarded\n"}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n"}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        item_data = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            attributes_to_get=attributes,\n            consistent_read=consistent\n        )\n        if 'Item' not in item_data:\n            raise exceptions.ItemNotFound(\"Item %s couldn't be found.\" % kwargs)\n        item = Item(self)\n        item.load(item_data)\n        return item\n"}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    lower_name = user_provided_svc_name.lower()\n\n    if user_provided_svc_name != lower_name:\n        logger.warning(\n            \"Converting %s to lowercase: %s.\", user_provided_svc_name, lower_name\n        )\n\n    # Service name must be a valid Tag name; create a dummy tag to use its validation\n    Tag(lower_name)\n    return lower_name\n"}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    from .vulnerability_helper import Sanitiser\n    sanitisers = list()\n    for sink in sinks_in_file:\n        sanitisers.extend(sink.sanitisers)\n\n    sanitisers_in_file = list()\n    for sanitiser in sanitisers:\n        for cfg_node in cfg.nodes:\n            if sanitiser in cfg_node.label:\n                sanitisers_in_file.append(Sanitiser(sanitiser, cfg_node))\n\n    sanitiser_node_dict = dict()\n    for sanitiser in sanitisers:\n        sanitiser_node_dict[sanitiser] = list(find_sanitiser_nodes(\n            sanitiser,\n            sanitisers_in_file\n        ))\n    return sanitiser_node_dict\n"}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones\n"}
{"namespace": "hl7.version.get_version", "completion": "    main_version = \"%s.%s.%s\" % VERSION[0:3]\n\n    if len(VERSION) < 4:\n        return main_version\n\n    version_type = VERSION[3]\n    if not version_type or version_type == \"final\":\n        return main_version\n    elif version_type == \"dev\":\n        return \"%s.dev\" % main_version\n    else:\n        return \"%s%s\" % (main_version, version_type)\n"}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    theme_colors = THEMES[theme_name].Color\n    failure_text = []\n    if color_depth == 16:\n        for color in theme_colors:\n            color_16code = color.value.split()[0]\n            if color_16code not in valid_16_color_codes:\n                invalid_16_color_code = str(color.name)\n                failure_text.append(f\"- {invalid_16_color_code} = {color_16code}\")\n        if failure_text == []:\n            return\n        else:\n            text = \"\\n\".join(\n                [f\"Invalid 16-color codes in theme '{theme_name}':\"] + failure_text\n            )\n            raise InvalidThemeColorCode(text)\n"}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)\n"}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    from .listener import BUCKET_QUOTA_OBJECT_ID\n    for bucket in paginated(storage, resource_name=\"bucket\", parent_id=\"\", sorting=[OLDEST_FIRST]):\n        bucket_id = bucket[\"id\"]\n        bucket_path = f\"/buckets/{bucket['id']}\"\n        bucket_collection_count = 0\n        bucket_record_count = 0\n        bucket_storage_size = record_size(bucket)\n\n        for collection in paginated(\n            storage, resource_name=\"collection\", parent_id=bucket_path, sorting=[OLDEST_FIRST]\n        ):\n            collection_info = rebuild_quotas_collection(storage, bucket_id, collection, dry_run)\n            (collection_record_count, collection_storage_size) = collection_info\n            bucket_collection_count += 1\n            bucket_record_count += collection_record_count\n            bucket_storage_size += collection_storage_size\n\n        bucket_record = {\n            \"record_count\": bucket_record_count,\n            \"storage_size\": bucket_storage_size,\n            \"collection_count\": bucket_collection_count,\n        }\n        if not dry_run:\n            storage.update(\n                resource_name=\"quota\",\n                parent_id=bucket_path,\n                object_id=BUCKET_QUOTA_OBJECT_ID,\n                obj=bucket_record,\n            )\n\n        logger.info(\n            f\"Bucket {bucket_id}. Final size: {bucket_collection_count} collections, {bucket_record_count} records, {bucket_storage_size} bytes.\"\n        )\n"}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "    from litecli.encodingutils import text_type\n    word_before_cursor = last_word(text_before_cursor, include=\"many_punctuations\")\n\n    identifier = None\n\n    # here should be removed once sqlparse has been fixed\n    try:\n        # If we've partially typed a word then word_before_cursor won't be an empty\n        # string. In that case we want to remove the partially typed string before\n        # sending it to the sqlparser. Otherwise the last token will always be the\n        # partially typed string which renders the smart completion useless because\n        # it will always return the list of keywords as completion.\n        if word_before_cursor:\n            if word_before_cursor.endswith(\"(\") or word_before_cursor.startswith(\"\\\\\"):\n                parsed = sqlparse.parse(text_before_cursor)\n            else:\n                parsed = sqlparse.parse(text_before_cursor[: -len(word_before_cursor)])\n\n                # word_before_cursor may include a schema qualification, like\n                # \"schema_name.partial_name\" or \"schema_name.\", so parse it\n                # separately\n                p = sqlparse.parse(word_before_cursor)[0]\n\n                if p.tokens and isinstance(p.tokens[0], Identifier):\n                    identifier = p.tokens[0]\n        else:\n            parsed = sqlparse.parse(text_before_cursor)\n    except (TypeError, AttributeError):\n        return [{\"type\": \"keyword\"}]\n\n    if len(parsed) > 1:\n        # Multiple statements being edited -- isolate the current one by\n        # cumulatively summing statement lengths to find the one that bounds the\n        # current position\n        current_pos = len(text_before_cursor)\n        stmt_start, stmt_end = 0, 0\n\n        for statement in parsed:\n            stmt_len = len(text_type(statement))\n            stmt_start, stmt_end = stmt_end, stmt_end + stmt_len\n\n            if stmt_end >= current_pos:\n                text_before_cursor = full_text[stmt_start:current_pos]\n                full_text = full_text[stmt_start:]\n                break\n\n    elif parsed:\n        # A single statement\n        statement = parsed[0]\n    else:\n        # The empty string\n        statement = None\n\n    # Check for special commands and handle those separately\n    if statement:\n        # Be careful here because trivial whitespace is parsed as a statement,\n        # but the statement won't have a first token\n        tok1 = statement.token_first()\n        if tok1 and tok1.value.startswith(\".\"):\n            return suggest_special(text_before_cursor)\n        elif tok1 and tok1.value.startswith(\"\\\\\"):\n            return suggest_special(text_before_cursor)\n        elif tok1 and tok1.value.startswith(\"source\"):\n            return suggest_special(text_before_cursor)\n        elif text_before_cursor and text_before_cursor.startswith(\".open \"):\n            return suggest_special(text_before_cursor)\n\n    last_token = statement and statement.token_prev(len(statement.tokens))[1] or \"\"\n\n    return suggest_based_on_last_token(\n        last_token, text_before_cursor, full_text, identifier\n    )\n"}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        scope_uris = [\n            scope_uri.to_payload() for scope_uri in self.capabilities.values()\n        ]\n        return {\"scope\": \" \".join(scope_uris)}\n"}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        store_path = self.find_or_save(filename, self.resource_dir)\n        self.run_entry[\"resources\"].append([filename, str(store_path)])\n        self.save_json(self.run_entry, \"run.json\")\n"}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        sources_map = {\n            \"baidu\": \"baidu\",\n            # \"flac\": \"flac\",\n            \"kugou\": \"kugou\",\n            \"netease\": \"netease\",\n            \"163\": \"netease\",\n            \"qq\": \"qq\",\n            \"migu\": \"migu\",\n            # \"xiami\": \"xiami\",\n        }\n        thread_pool = []\n        ret_songs_list = []\n        ret_errors = []\n\n        click.echo(\"\")\n        click.echo(\n            _(\"Searching {keyword} from ...\").format(\n                keyword=colorize(config.get(\"keyword\"), \"highlight\")\n            ),\n            nl=False,\n        )\n\n        for source_key in sources_list:\n            if not source_key in sources_map:\n                raise ParameterError(\"Invalid music source.\")\n\n            t = threading.Thread(\n                target=self.search_thread,\n                args=(sources_map.get(source_key), keyword, ret_songs_list, ret_errors),\n            )\n            thread_pool.append(t)\n            t.start()\n\n        for t in thread_pool:\n            t.join()\n\n        click.echo(\"\")\n        # \u8f93\u51fa\u9519\u8bef\u4fe1\u606f\n        for err in ret_errors:\n            self.logger.debug(_(\"\u97f3\u4e50\u5217\u8868 {error} \u83b7\u53d6\u5931\u8d25.\").format(error=err[0].upper()))\n            self.logger.debug(err[1])\n\n        # \u5bf9\u641c\u7d22\u7ed3\u679c\u6392\u5e8f\u548c\u53bb\u91cd\n        if not config.get(\"nomerge\"):\n            ret_songs_list.sort(\n                key=lambda song: (song.singer, song.title, song.size), reverse=True\n            )\n            tmp_list = []\n            for i in range(len(ret_songs_list)):\n                # \u5982\u679c\u540d\u79f0\u3001\u6b4c\u624b\u90fd\u4e00\u81f4\u7684\u8bdd\u5c31\u53bb\u91cd\uff0c\u4fdd\u7559\u6700\u5927\u7684\u6587\u4ef6\n                if (\n                    i > 0\n                    and ret_songs_list[i].size <= ret_songs_list[i - 1].size\n                    and ret_songs_list[i].title == ret_songs_list[i - 1].title\n                    and ret_songs_list[i].singer == ret_songs_list[i - 1].singer\n                ):\n                    continue\n                tmp_list.append(ret_songs_list[i])\n            ret_songs_list = tmp_list\n\n        return ret_songs_list\n"}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)\n"}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "        from zulipterminal.config.ui_mappings import STREAM_ACCESS_TYPE\n        stream_marker = INVALID_MARKER\n        color = \"general_bar\"\n        if self.model.is_valid_stream(new_text):\n            stream_id = self.model.stream_id_from_name(new_text)\n            stream_access_type = self.model.stream_access_type(stream_id)\n            stream_marker = STREAM_ACCESS_TYPE[stream_access_type][\"icon\"]\n            stream = self.model.stream_dict[stream_id]\n            color = stream[\"color\"]\n        self.header_write_box[self.FOCUS_HEADER_PREFIX_STREAM].set_text(\n            (color, stream_marker)\n        )\n"}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if schema is None:\n        schema = grouping\n    else:\n        validate_grouping(grouping, schema)\n\n    if isinstance(schema, (tuple, list)):\n        return [\n            g\n            for group_el, schema_el in zip(grouping, schema)\n            for g in flatten_grouping(group_el, schema_el)\n        ]\n\n    if isinstance(schema, dict):\n        return [g for k in schema for g in flatten_grouping(grouping[k], schema[k])]\n\n    return [grouping]\n"}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        self.capabilities[\"incoming\"] = ScopeURI(\n            \"client\", \"incoming\", {\"clientName\": client_name}\n        )\n"}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n\n        self.close()\n\n        if self.filename == ':memory:':\n            return\n\n        logger.info(\"deleting %s\" % self.filename)\n        try:\n            if os.path.isfile(self.filename):\n                os.remove(self.filename)\n        except (OSError, IOError):\n            logger.exception(\"failed to delete %s\" % (self.filename))\n"}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    frt = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, frt, 4)\n"}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = self._find_hadoop_bin()\n        return self._hadoop_bin\n"}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.isdir(path):\n            os.makedirs(path)\n"}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_value = self.arg_value\n        modifiers.variation_name = self.variation\n        return modifiers\n"}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    from sslyze.plugins.certificate_info._certificate_utils import get_common_names\n    common_names = get_common_names(name_field)\n    if common_names:\n        # We don't support certs with multiple CNs\n        return common_names[0]\n    else:\n        # Otherwise show the whole field\n        return name_field.rfc4514_string()\n"}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n"}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()\n"}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    from exodus_bundler import root_logger\n    log_level = logging.WARN\n    if quiet and not verbose:\n        log_level = logging.ERROR\n    elif verbose and not quiet:\n        log_level = logging.INFO\n    root_logger.setLevel(log_level)\n\n    class StderrFilter(logging.Filter):\n        def filter(self, record):\n            return record.levelno in (logging.WARN, logging.ERROR)\n\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_formatter = logging.Formatter('%(levelname)s: %(message)s')\n    stderr_handler.setFormatter(stderr_formatter)\n    stderr_handler.addFilter(StderrFilter())\n    root_logger.addHandler(stderr_handler)\n\n    # We won't even configure/add the stdout handler if this is specified.\n    if suppress_stdout:\n        return\n\n    class StdoutFilter(logging.Filter):\n        def filter(self, record):\n            return record.levelno in (logging.DEBUG, logging.INFO)\n\n    stdout_formatter = logging.Formatter('%(message)s')\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setFormatter(stdout_formatter)\n    stdout_handler.addFilter(StdoutFilter())\n    root_logger.addHandler(stdout_handler)\n"}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n\n    assert p is not None\n    assert len(a) == len(p)\n    assert len(a) >= length, \"You can't request more unique samples than elements in the dataset.\"\n\n    choices = []\n    items = list(a)\n    probabilities = list(p)\n    for i in range(length):\n        cdf = tuple(cumsum(probabilities))\n        normal = cdf[-1]\n        cdf2 = [i / normal for i in cdf]\n        uniform_sample = random_sample(random=random)\n        idx = bisect.bisect_right(cdf2, uniform_sample)\n        item = items[idx]\n        choices.append(item)\n        probabilities.pop(idx)\n        items.pop(idx)\n    return choices\n"}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)\n"}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        content_words = self._filter_out_stop_words(all_words)\n        normalized_content_words = self._normalize_words(content_words)\n        return normalized_content_words\n"}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if val < 0:\n        raise ValueError(\"Must be a positive integer\")\n\n    int_bytes = bytes_from_int(val)\n\n    if len(int_bytes) == 0:\n        int_bytes = b\"\\x00\"\n\n    return base64url_encode(int_bytes)\n"}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        if receiver:\n            name = name or receiver.__name__\n        if not name:\n            raise ValueError('a receiver or a name must be provided')\n\n        key = (name, sender)\n        if key not in self._receivers:\n            raise ValueError('receiver named %s for sender=%s not found.' %\n                             (name, sender or 'any'))\n\n        self._receivers.remove(key)\n        self._receiver_list = [(n, r, s) for n, r, s in self._receiver_list\n                               if (n, s) != key]\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be set to None when \"\n                \"use_start_end_tokens is set to True\"\n            )\n\n    likelihoods = []\n    sess = session.copy()\n    if use_start_end_tokens and end_token:\n        sess += [str(end_token)]\n    end = len(sess) - window_len\n    for i in range(end + 1):\n        window = sess[i : i + window_len]  # noqa: E203\n\n        if i == 0:\n            use_start = use_start_end_tokens\n        else:\n            use_start = False\n\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=use_start,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            k = window_len\n            lik = lik ** (1 / k)\n\n        likelihoods.append(lik)\n\n    return likelihoods\n"}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        self.names.remove(name)\n        del self.name2val[name]\n        after = self.name2after.pop(name, [])\n        if after:\n            self.req_after.remove(name)\n            for u in after:\n                self.order.remove((u, name))\n        before = self.name2before.pop(name, [])\n        if before:\n            self.req_before.remove(name)\n            for u in before:\n                self.order.remove((name, u))\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    pars = params_with_vals.copy()\n    if isinstance(pars, set):\n        pars = dict.fromkeys(pars)\n    if len(pars) == 0:\n        return 1.0\n    ref_cmd = param_cond_cmd_probs[cmd]\n    lik: float = 1\n    num = 0\n    for param, prob in ref_cmd.items():\n        if param in pars:\n            lik *= prob\n            if param in modellable_params:\n                num += 1\n                val = pars[param]\n                lik *= value_cond_param_probs[param][val]\n        else:\n            lik *= 1 - prob\n    if use_geo_mean:\n        k = len(ref_cmd) + num\n        if k > 0:\n            lik = lik ** (1 / k)\n\n    return lik\n"}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n\n    payload = _session.generate_payload()\n    output_payload_to_file(payload)\n    return upload_payload(payload, service_endpoint_uri, separate_process)\n"}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            table_name=index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )\n"}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        id_, branch_label = self._resolve_revision_number(check_against)\n\n        shares = []\n        if branch_label:\n            shares.append(branch_label)\n        if id_:\n            shares.extend(id_)\n\n        return tuple(\n            tg\n            for tg in targets\n            if self._shares_lineage(\n                tg, shares, include_dependencies=include_dependencies\n            )\n        )\n"}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for n in self.notes:\n            n.transpose(interval, up)\n        return self\n"}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        for order, view, phash in self.get_views(request):\n            if not hasattr(view, '__predicated__'):\n                return view\n            if view.__predicated__(context, request):\n                return view\n        raise PredicateMismatch(self.name)\n"}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            kwonly_pairs = None\n            formatters = {}\n            if self.kwonlyargs:\n                kwonly_pairs = dict((arg, arg)\n                                    for arg in self.kwonlyargs)\n                formatters['formatvalue'] = lambda value: '=' + value\n\n            sig = inspect_formatargspec(self.args,\n                                        self.varargs,\n                                        self.varkw,\n                                        [],\n                                        kwonly_pairs,\n                                        kwonly_pairs,\n                                        {},\n                                        **formatters)\n            sig = self._KWONLY_MARKER.sub('', sig)\n            return sig[1:-1]\n"}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if version is None:\n        raise TypeError\n\n    if variable in _JOBCONF_MAP:\n        return map_version(version, _JOBCONF_MAP[variable])\n    else:\n        return variable\n"}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if iterable is not None:\n            if callable(getattr(iterable, 'iteritems', None)):\n                for key, count in iterable.iteritems():\n                    for i in xrange(count):\n                        self.add(key)\n            else:\n                for key in iterable:\n                    self.add(key)\n        if kwargs:\n            self.update(kwargs)\n"}
{"namespace": "datasette.utils.is_url", "completion": "    if not isinstance(value, str):\n        return False\n    if not value.startswith(\"http://\") and not value.startswith(\"https://\"):\n        return False\n    # Any whitespace at all is invalid\n    if whitespace_re.search(value):\n        return False\n    return True\n"}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return urljoin('file:', pathname2url(abspath(path_or_uri)))\n"}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    data = self._render([], 0, indent, pretty, xhtml)\n    return u''.join(data)\n"}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        info(\"Installing OpenSSL ...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])\n"}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        assert self.source != field_name, (\n            \"It is redundant to specify `source='%s'` on field '%s' in \"\n            \"serializer '%s', because it is the same as the field name. \"\n            \"Remove the `source` keyword argument.\" %\n            (field_name, self.__class__.__name__, parent.__class__.__name__)\n        )\n\n        self.field_name = field_name\n        self.parent = parent\n\n        # `self.label` should default to being based on the field name.\n        if self.label is None:\n            self.label = field_name.replace('_', ' ').capitalize()\n\n        # self.source should default to being the same as the field name.\n        if self.source is None:\n            self.source = field_name\n\n        # self.source_attrs is a list of attributes that need to be looked up\n        # when serializing the instance, or populating the validated data.\n        if self.source == '*':\n            self.source_attrs = []\n        else:\n            self.source_attrs = self.source.split('.')\n"}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {'access_key': self.access_key,\n                'secret_key': self.secret_key,\n                'session_token': self.session_token,\n                'expiration': self.expiration,\n                'request_id': self.request_id}\n"}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = list()\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            trigger_nodes.extend(iter(label_contains(node, trigger_words)))\n    return trigger_nodes\n"}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    target_type = None\n    # Unicode type aliased to str is code-smell for Boltons in Python 3 env.\n    is_py3 = (unicode == builtins.str)\n    if is_py3 and isinstance(text, (bytes, bytearray)):\n        target_type = type(text)\n        text = text.decode('utf-8')\n\n    cleaned = ANSI_SEQUENCES.sub('', text)\n\n    # Transform back the result to the same bytearray type provided by the user.\n    if target_type and target_type != type(cleaned):\n        cleaned = target_type(cleaned, 'utf-8')\n\n    return cleaned\n"}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        if isinstance(batch, t.Generator):  # Generators can't be pickled\n            batch = list(t.cast(t.Generator[t.Any, t.Any, t.Any], batch))\n\n        data = pickle.dumps(batch)\n\n        if isinstance(batch, list):\n            batch_size = len(t.cast(t.List[t.Any], batch))\n        else:\n            batch_size = 1\n\n        return cls.create_payload(data=data, batch_size=batch_size)\n"}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    crack_times_seconds = {\n        'online_throttling_100_per_hour': Decimal(guesses) / float_to_decimal(100.0 / 3600.0),\n        'online_no_throttling_10_per_second': Decimal(guesses) / float_to_decimal(10.0),\n        'offline_slow_hashing_1e4_per_second': Decimal(guesses) / float_to_decimal(1e4),\n        'offline_fast_hashing_1e10_per_second': Decimal(guesses) / float_to_decimal(1e10),\n    }\n\n    crack_times_display = {}\n    for scenario, seconds in crack_times_seconds.items():\n        crack_times_display[scenario] = display_time(seconds)\n\n    return {\n        'crack_times_seconds': crack_times_seconds,\n        'crack_times_display': crack_times_display,\n        'score': guesses_to_score(guesses),\n    }\n"}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n        args = dict(self.get_querystring_pairs())\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            # TODO: does this query break if inner sql produces value or count columns?\n            facet_sql = \"\"\"\n                select date({col}) as value, count(*) as count from (\n                    {sql}\n                )\n                where date({col}) is not null\n                group by date({col}) order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": path_with_removed_args(\n                            self.request, {\"_facet_date\": column}\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                for row in facet_rows:\n                    selected = str(args.get(f\"{column}__date\")) == str(row[\"value\"])\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}__date\": str(row[\"value\"])}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}__date\": row[\"value\"]}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": row[\"value\"],\n                            \"label\": row[\"value\"],\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, toggle_path\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out\n"}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions=('The NDK API is a minimum supported API number and must be lower '\n                          'than the target Android API'))\n\n    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)\n"}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])\n"}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        return self.connection.attach_network_interface(\n            self.id,\n            instance_id,\n            device_index,\n            dry_run=dry_run\n        )\n"}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)\n"}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        try:\n            return s.decode('utf_8')\n        except UnicodeDecodeError:\n            return s.decode('latin_1')\n    elif isinstance(s, string_types):  # e.g. is unicode\n        return s\n    else:\n        raise TypeError\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n        self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n        self.compute_geomean_lik_of_sessions()\n        self.compute_rarest_windows(\n            window_len=2, use_geo_mean=False, use_start_end_tokens=use_start_end_tokens\n        )\n        self.compute_rarest_windows(\n            window_len=3, use_geo_mean=False, use_start_end_tokens=use_start_end_tokens\n        )\n"}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        discretized_df = dataframe.copy()\n        all_columns = dataframe.columns\n        num_columns = self._get_numerical_columns(dataframe)\n        for column in num_columns:\n            discretized_df.loc[:, column] = self._discretize_column(\n                discretized_df[column]\n            )\n\n        discretized_df = discretized_df[all_columns]\n        return (\n            discretized_df.reset_index(drop=True)\n            if self.reset_index\n            else discretized_df\n        )\n"}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg)\n    [check_uri(a, msg) for a in arg]\n"}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        from rest_framework.compat import parse_header_parameters\n        try:\n            return parser_context['kwargs']['filename']\n        except KeyError:\n            pass\n\n        try:\n            meta = parser_context['request'].META\n            disposition, params = parse_header_parameters(meta['HTTP_CONTENT_DISPOSITION'])\n            if 'filename*' in params:\n                return params['filename*']\n            else:\n                return params['filename']\n        except (AttributeError, KeyError, ValueError):\n            pass\n"}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        if time.localtime().tm_isdst and time.daylight:\n            offset = time.altzone\n        else:\n            offset = time.timezone\n        return cls(-offset // 60)\n"}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be set to None when \"\n                \"use_start_end_tokens is set to True\"\n            )\n\n    likelihoods = []\n    sess = session.copy()\n    if use_start_end_tokens and end_token:\n        sess += [Cmd(name=str(end_token), params={})]\n    end = len(sess) - window_len\n\n    for i in range(end + 1):\n        window = sess[i : i + window_len]  # noqa E203\n        if i == 0:\n            use_start = use_start_end_tokens\n        else:\n            use_start = False\n\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n\n        if use_geo_mean:\n            k = window_len\n            lik = lik ** (1 / k)\n        likelihoods.append(lik)\n\n    return likelihoods\n"}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        try:\n            fd = open(binary.filename, 'rb')\n            signature = fd.read(4)\n            fd.close()\n        except:\n            raise Exception(\"Error loading file.\")\n\n        if signature[:4] == b\"\\x7f\\x45\\x4c\\x46\":\n            self._load_binary_elf(binary.filename)\n        elif signature[:2] == b\"\\x4d\\x5a\":\n            self._load_binary_pe(binary.filename)\n        else:\n            raise Exception(\"Unknown file format.\")\n"}
{"namespace": "boltons.urlutils.URL.path", "completion": "        self.path_parts = tuple([unquote(p) if '%' in p else p\n                                 for p in to_unicode(path_text).split(u'/')])\n        return\n"}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        self._stdin = stdin or BytesIO()\n        self._stdout = stdout or BytesIO()\n        self._stderr = stderr or BytesIO()\n\n        return self\n"}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        if self._opts['bootstrap_mrjob'] is None:\n            return True\n        else:\n            return bool(self._opts['bootstrap_mrjob'])\n"}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    linker_dirname, linker_basename = os.path.split(linker)\n    full_linker = 'true' if full_linker else 'false'\n    return render_template_file('launcher.sh', linker_basename=linker_basename,\n                                linker_dirname=linker_dirname, library_path=library_path,\n                                executable=executable, full_linker=full_linker)\n"}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    item = d\n    i = 0\n    j = len(keys)\n    while i < j:\n        key = keys[i]\n        if i < (j - 1):\n            item = _get_or_new_item_value(item, key, keys[i + 1])\n            i += 1\n            continue\n        _set_item_value(item, key, value)\n        break\n"}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    from pythonforandroid.bootstrap import Bootstrap\n    from pythonforandroid.logger import info\n    names = set(names)\n    if bs is not None and bs.recipe_depends:\n        names = names.union(set(bs.recipe_depends))\n    names = fix_deplist([\n        ([name] if not isinstance(name, (list, tuple)) else name)\n        for name in names\n    ])\n    if blacklist is None:\n        blacklist = set()\n    blacklist = {bitem.lower() for bitem in blacklist}\n\n    # Remove all values that are in the blacklist:\n    names_before_blacklist = list(names)\n    names = []\n    for name in names_before_blacklist:\n        cleaned_up_tuple = tuple([\n            item for item in name if item not in blacklist\n        ])\n        if cleaned_up_tuple:\n            names.append(cleaned_up_tuple)\n\n    # Do check for obvious conflicts (that would trigger in any order, and\n    # without comitting to any specific choice in a multi-choice tuple of\n    # dependencies):\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n    # If we get here, no obvious conflicts!\n\n    # get all possible order graphs, as names may include tuples/lists\n    # of alternative dependencies\n    possible_orders = []\n    for name_set in product(*names):\n        new_possible_orders = [RecipeOrder(ctx)]\n        for name in name_set:\n            new_possible_orders = recursively_collect_orders(\n                name, ctx, name_set, orders=new_possible_orders,\n                blacklist=blacklist\n            )\n        possible_orders.extend(new_possible_orders)\n\n    # turn each order graph into a linear list if possible\n    orders = []\n    for possible_order in possible_orders:\n        try:\n            order = find_order(possible_order)\n        except ValueError:  # a circular dependency was found\n            info('Circular dependency found in graph {}, skipping it.'.format(\n                possible_order))\n            continue\n        orders.append(list(order))\n\n    # prefer python3 and SDL2 if available\n    orders = sorted(orders,\n                    key=lambda order: -('python3' in order) - ('sdl2' in order))\n\n    if not orders:\n        raise BuildInterruptingException(\n            'Didn\\'t find any valid dependency graphs. '\n            'This means that some of your '\n            'requirements pull in conflicting dependencies.')\n\n    # It would be better to check against possible orders other\n    # than the first one, but in practice clashes will be rare,\n    # and can be resolved by specifying more parameters\n    chosen_order = orders[0]\n    if len(orders) > 1:\n        info('Found multiple valid dependency orders:')\n        for order in orders:\n            info('    {}'.format(order))\n        info('Using the first of these: {}'.format(chosen_order))\n    else:\n        info('Found a single valid recipe set: {}'.format(chosen_order))\n\n    if bs is None:\n        bs = Bootstrap.get_bootstrap_from_recipes(chosen_order, ctx)\n        if bs is None:\n            # Note: don't remove this without thought, causes infinite loop\n            raise BuildInterruptingException(\n                \"Could not find any compatible bootstrap!\"\n            )\n        recipes, python_modules, bs = get_recipe_order_and_bootstrap(\n            ctx, chosen_order, bs=bs, blacklist=blacklist\n        )\n    else:\n        # check if each requirement has a recipe\n        recipes = []\n        python_modules = []\n        for name in chosen_order:\n            try:\n                recipe = Recipe.get_recipe(name, ctx)\n                python_modules += recipe.python_depends\n            except ValueError:\n                python_modules.append(name)\n            else:\n                recipes.append(name)\n\n    python_modules = list(set(python_modules))\n    return recipes, python_modules, bs\n"}
{"namespace": "fs.path.splitext", "completion": "    parent_path, pathname = split(path)\n    if pathname.startswith(\".\") and pathname.count(\".\") == 1:\n        return path, \"\"\n    if \".\" not in pathname:\n        return path, \"\"\n    pathname, ext = pathname.rsplit(\".\", 1)\n    path = join(parent_path, pathname)\n    return path, \".\" + ext\n"}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(\n            self._tree_conf,\n            key=key  # Hack to compare and order\n        )\n        i = bisect.bisect_left(self.entries, entry)\n        if i != len(self.entries) and self.entries[i] == entry:\n            return i\n        raise ValueError('No entry for key {}'.format(key))\n"}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        conf_file = os.path.join(os.path.dirname(__file__), \"ext.conf\")\n        return config_lib.read(conf_file)\n"}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        new_color = self.original_color if text_color is None else text_color\n\n        self.count = count\n        if count == 0:\n            count_text = \"\"\n        else:\n            count_text = str(count)\n\n        self.update_widget((self.count_style, count_text), new_color)\n"}
{"namespace": "discord.utils.resolve_invite", "completion": "    from .invite import Invite  # circular import\n\n    if isinstance(invite, Invite):\n        return ResolvedInvite(invite.code, invite.scheduled_event_id)\n    else:\n        rx = r'(?:https?\\:\\/\\/)?discord(?:\\.gg|(?:app)?\\.com\\/invite)\\/[^/]+'\n        m = re.match(rx, invite)\n\n        if m:\n            url = yarl.URL(invite)\n            code = url.parts[-1]\n            event_id = url.query.get('event')\n\n            return ResolvedInvite(code, int(event_id) if event_id else None)\n    return ResolvedInvite(invite, None)\n"}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    from boto.regioninfo import connect\n    return connect('autoscaling', region_name,\n                   connection_cls=AutoScaleConnection, **kw_params)\n"}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if application_id:\n                path = self.fs.join(log_dir, 'userlogs', application_id)\n            else:\n                path = self.fs.join(log_dir, 'userlogs')\n\n            if _logs_exist(self.fs, path):\n                log.info('Looking for task syslogs in %s...' % path)\n                yield [path]\n"}
{"namespace": "boltons.iterutils.get_path", "completion": "    if isinstance(path, basestring):\n        path = path.split('.')\n    cur = root\n    try:\n        for seg in path:\n            try:\n                cur = cur[seg]\n            except (KeyError, IndexError) as exc:\n                raise PathAccessError(exc, seg, path)\n            except TypeError as exc:\n                # either string index in a list, or a parent that\n                # doesn't support indexing\n                try:\n                    seg = int(seg)\n                    cur = cur[seg]\n                except (ValueError, KeyError, IndexError, TypeError):\n                    if not is_iterable(cur):\n                        exc = TypeError('%r object is not indexable'\n                                        % type(cur).__name__)\n                    raise PathAccessError(exc, seg, path)\n    except PathAccessError:\n        if default is _UNSET:\n            raise\n        return default\n    return cur\n"}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n"}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        return self.containers[0](\n            sequence=data,\n            esc=self.esc,\n            separators=self.separators,\n            factory=self.factory,\n        )\n"}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        inner_text = []\n        for rule in self:\n            inner_text.append(rule.to_xml())\n        return tag('RoutingRules', '\\n'.join(inner_text))\n"}
{"namespace": "mingus.core.keys.get_key", "completion": "    from mingus.core.mt_exceptions import RangeError\n    if accidentals not in range(-7, 8):\n        raise RangeError(\"integer not in range (-7)-(+7).\")\n    return keys[accidentals + 7]\n"}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    from chatette.adapters.jsonl import JsonListAdapter\n    from chatette.adapters.rasa import RasaAdapter\n    from chatette.adapters.rasa_md import RasaMdAdapter\n    if adapter_name is None:\n        return None\n    adapter_name = adapter_name.lower()\n    if adapter_name == 'rasa':\n        return RasaAdapter(base_filepath)\n    elif adapter_name in ('rasa-md', 'rasamd'):\n        return RasaMdAdapter(base_filepath)\n    elif adapter_name == 'jsonl':\n        return JsonListAdapter(base_filepath)\n    raise ValueError(\"Unknown adapter was selected.\")\n"}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        if payload.meta[\"with_buffer\"]:\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return pep574_loads(bs, payload.data, indices)\n        else:\n            return pep574_loads(payload.data, b\"\", [])\n"}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    magic_len = len(RTF_MAGIC)\n    if isinstance(arg, UNICODE_TYPE):\n        with open(arg, 'rb') as reader:\n            return reader.read(len(RTF_MAGIC)) == RTF_MAGIC\n    if isinstance(arg, bytes) and not isinstance(arg, str):  # only in PY3\n        return arg[:magic_len] == RTF_MAGIC\n    if isinstance(arg, bytearray):\n        return arg[:magic_len] == RTF_MAGIC\n    if isinstance(arg, str):      # could be bytes, but we assume file name\n        if treat_str_as_data:\n            try:\n                return arg[:magic_len].encode('ascii', errors='strict')\\\n                    == RTF_MAGIC\n            except UnicodeError:\n                return False\n        else:\n            with open(arg, 'rb') as reader:\n                return reader.read(len(RTF_MAGIC)) == RTF_MAGIC\n    if hasattr(arg, 'read'):      # a stream (i.e. file-like object)\n        return arg.read(len(RTF_MAGIC)) == RTF_MAGIC\n    if isinstance(arg, (list, tuple)):\n        iter_arg = iter(arg)\n    else:\n        iter_arg = arg\n\n    # check iterable\n    for magic_byte in zip(RTF_MAGIC):\n        try:\n            if next(iter_arg) not in magic_byte:\n                return False\n        except StopIteration:\n            return False\n\n    return True  # checked the complete magic without returning False --> match\n"}
{"namespace": "boto.utils.pythonize_name", "completion": "    s1 = _first_cap_regex.sub(r'\\1_\\2', name)\n    s2 = _number_cap_regex.sub(r'\\1_\\2', s1)\n    return _end_cap_regex.sub(r'\\1_\\2', s2).lower()\n"}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        if path not in self._path_to_name:\n            # use unhide so that input files won't be hidden from Hadoop,\n            # see #1200\n            name = name_uniquely(\n                path, names_taken=self._names_taken, unhide=True)\n            self._names_taken.add(name)\n            self._path_to_name[path] = name\n\n        return self.uri(path)\n"}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        from pyramid.util import object_description\n        return getattr(\n            self.func,\n            '__text__',\n            'custom predicate: %s' % object_description(self.func),\n        )\n"}
{"namespace": "fs.info.Info.suffix", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return \"\"\n        basename, dot, ext = name.rpartition(\".\")\n        return \".\" + ext if dot else \"\"\n"}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            file_type = output_file.split(\".\")[-1]\n\n            if file_type == \"html\":\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, output_format=\"html\", file_info=file_info)\n            elif file_type == \"json\":\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, output_format=\"json\", file_info=file_info)\n            elif file_type == \"gz\":\n                with gzip.open(output_file, \"wt\") as f:\n                    self.generate_report(f, output_format=\"json\", file_info=file_info)\n            else:\n                raise Exception(\"Only html, json and gz are supported\")\n        else:\n            self.generate_report(output_file, output_format=\"json\", file_info=file_info)\n\n        if isinstance(output_file, str):\n            self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n\n        self.print_messages()\n"}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        root_node = self._mem.get_node(self._root_node_page)\n        assert isinstance(root_node, (LonelyRootNode, RootNode))\n        return root_node\n"}
{"namespace": "pyinfra.api.operation.add_op", "completion": "    from pyinfra.context import ctx_host\n    from pyinfra.context import ctx_state\n    if pyinfra.is_cli:\n        raise PyinfraError(\n            (\"`add_op` should not be called when pyinfra is executing in CLI mode! ({0})\").format(\n                get_call_location(),\n            ),\n        )\n\n    hosts = kwargs.pop(\"host\", state.inventory.iter_active_hosts())\n    if isinstance(hosts, Host):\n        hosts = [hosts]\n\n    with ctx_state.use(state):\n        results = {}\n        for op_host in hosts:\n            with ctx_host.use(op_host):\n                results[op_host] = op_func(*args, **kwargs)\n\n    return results\n"}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            current = self.manager.get()\n            if current['registry'] == self.registry:\n                request = current['request']\n            else:\n                request = None\n        self.manager.push({'registry': self.registry, 'request': request})\n"}
{"namespace": "discord.utils.resolve_annotation", "completion": "    if annotation is None:\n        return type(None)\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n\n    locals = globalns if localns is None else localns\n    if cache is None:\n        cache = {}\n    return evaluate_annotation(annotation, globalns, locals, cache)\n"}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        self._assert_resolved()\n        return '<%s category %r, discriminator %r>' % (\n            self.__class__.__name__,\n            self.category_name,\n            self.discriminator,\n        )\n"}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    m = _HISTORY_LOG_PATH_RE.match(path)\n    if not m:\n        return None\n\n    if not (job_id is None or m.group('job_id') == job_id):\n        return None\n\n    # TODO: couldn't manage to include .jhist in regex; an optional\n    # group has less priority than a non-greedy match, apparently\n    return dict(job_id=m.group('job_id'), yarn='.jhist' in m.group('suffix'))\n"}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        if not kwargs:\n            return self\n        other = super().replace(**kwargs)\n        if hasattr(self, \"_hash\"):\n            object.__delattr__(other, \"_hash\")\n        return self._instances.setdefault(weakref.ref(other), other)\n"}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    from .openmetrics import exposition as openmetrics\n    accept_header = accept_header or ''\n    for accepted in accept_header.split(','):\n        if accepted.split(';')[0].strip() == 'application/openmetrics-text':\n            return (openmetrics.generate_latest,\n                    openmetrics.CONTENT_TYPE_LATEST)\n    return generate_latest, CONTENT_TYPE_LATEST\n"}
{"namespace": "fs.info.Info.created", "completion": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time\n"}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer\n        if ':' in spec:\n            package, relative = spec.split(':', 1)\n            self.renderers[relative] = renderer\n"}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    start = html_bytes.rfind(b'Running Jobs')\n    if start == -1:\n        return None, None\n    end = html_bytes.find(b'Jobs', start + len(b'Running Jobs'))\n    if end == -1:\n        end = None\n\n    html_bytes = html_bytes[start:end]\n\n    # search it for percents\n    matches = _JOB_TRACKER_HTML_RE.findall(html_bytes)\n    if len(matches) >= 2:\n        return float(matches[0]), float(matches[1])\n    else:\n        return None, None\n"}
{"namespace": "falcon.response.Response.delete_header", "completion": "        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to remove cookies')\n\n        self._headers.pop(name, None)\n"}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)\n"}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    data_path.mkdir(exist_ok=True)\n\n    file_path = data_path / file_name\n\n    # If not exists, download and create file\n    if not file_path.exists():\n        response = request.urlopen(url)\n        file_path.write_bytes(response.read())\n\n    return file_path\n"}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n"}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)\n"}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"WSL\":\n        return 1\n\n    return 0\n"}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, \"rb\") as file:\n            return cls(file.read(), tokenizer, url)\n"}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])\n"}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec in self._seen_files:\n            return False\n        self._seen_files.add(spec)\n        return True\n"}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        self._revision_map\n        return self.heads\n"}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    from mingus.core import notes\n    accidentals = get_key_signature(key)\n    res = []\n\n    if accidentals < 0:\n        for i in range(-accidentals):\n            res.append(\"{0}{1}\".format(list(reversed(notes.fifths))[i], \"b\"))\n    elif accidentals > 0:\n        for i in range(accidentals):\n            res.append(\"{0}{1}\".format(notes.fifths[i], \"#\"))\n    return res\n"}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    try:\n        return key in KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)\n"}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return list(KEY_BINDINGS[command][\"keys\"])\n    except KeyError as exception:\n        raise InvalidCommand(command)\n"}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(\n            self.id,\n            instance_id,\n            device,\n            dry_run=dry_run\n        )\n"}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    from mrjob.parse import is_uri\n    if is_uri(path):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/%s/%s' % (getpass.getuser(), path)\n"}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        ret = []\n        if tb is None:\n            tb = sys.exc_info()[2]\n            if tb is None:\n                raise ValueError('no tb set and no exception being handled')\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        return cls(ret)\n"}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        result = [(key, self._mapping[key]) for key in list(self._queue)]\n        result.reverse()\n        return result\n"}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        cursor = self._connection.execute(\n            \"INSERT OR IGNORE INTO tokens VALUES (?, ?, datetime('now'))\",\n            (self.key, refresh_token),\n        )\n        self._connection.commit()\n        return cursor.rowcount == 1\n"}
